# Neuromatch Academy Deep Learning (NMA-DL) syllabus

The content should primarily be accessed from our new ebook: https://deeplearning.neuromatch.io/ [under continuous development]

----

*July 11-29, 2022*

**Objectives**: Gain hands-on, code-first experience with deep learning theories, models, and skills that are useful for applications and for advancing science. We focus on how to decide which problems can be tackled with deep learning, how to determine what model is best, how to best implement a model, how to visualize / justify findings, and how neuroscience can inspire deep learning. And throughout we emphasize the ethical use of DL. 

Please check out [expected prerequisites here](https://github.com/NeuromatchAcademy/precourse-dl)!

**Confirmed speakers**: 

* [Amita Kapoor (U Delhi)](https://www.dramitakapoor.com/)
* [Anima Anandkumar (Caltech)](http://tensorlab.cms.caltech.edu/users/anima/)
* [Aude Oliva (MIT)](http://olivalab.mit.edu/audeoliva.html)
* [Chelsea Finn (Stanford)](https://ai.stanford.edu/~cbfinn/)
* [Emily Denton (Google)](https://cephaloponderer.com/)
* [Geoffrey Hinton (U Toronto)](http://www.cs.toronto.edu/~hinton/)
* [Joao Sedoc (NYU)](https://scholar.google.com/citations?user=vv355NgAAAAJ&hl=en)
* [Kyunghyun Cho (NYU)](https://kyunghyuncho.me/)
* [Melanie Mitchell (Santa Fe Institute)](https://melaniemitchell.me/)
* [Yann LeCun (Facebook)](http://yann.lecun.com/)
* [Yoshua Bengio (MILA)](https://yoshuabengio.org/)

# Course materials

----

Coming soon... stay tuned...

# Course outline

----

## Week 1

----

### Day 1:  Intro to DL academy

*coordinated by [Konrad Kording (U Penn)](http://koerding.com/)*

**Description** Welcome, introduction to Google Colab, meet and greet, a bit of DL history, DL basics and introduction to Pytorch

----

### Day 2: Linear DL

*coordinated by [Andrew Saxe (Oxford)](https://www.saxelab.org/)*

**Description** Gradients, AutoGrad, linear regression, concept of optimization, loss functions, designing deep linear systems and how to train them

----

### Day 3: Multi-layer Perceptrons (MLPs)

*coordinated by [Surya Ganguli (Stanford)](https://ganguli-gang.stanford.edu/)*

**Description** From neuroscience inspiration, to solving the XOR problem, to function approximation, cross-validation, training, and trade-offs

----

### Day 4:  Projects day 

*coordinated by Project TAs*

----

### Day 5: Optimization

*coordinated by [Ioannis Mitliagkas (MILA)](http://mitliagkas.github.io/)*

**Description** Why optimization is hard and all the tricks to get it to work

----

## Week 2

----

### Day 1: Regularization

*coordinated by [Lyle Ungar (U Penn)](https://www.cis.upenn.edu/~ungar/)*

**Description** The problem of overfitting and different ways to solve it

----

### Day 2: Parameter sharing: Convnets and DL Thinking (part 1)

*coordinated by [Alona Fyshe (U Alberta)](https://webdocs.cs.ualberta.ca/~alona/)*

**Description** How the number of parameters affects generalization, and what Convolutional Neural Networks (Convnets) and Recurrent Neural Networks (RNNs) can do for you to help

----

### Day 3: Modern Convnets 

*coordinated by [Alexander Ecker (U Goettingen)](https://eckerlab.org/)*

**Description** Modern Convolutional Neural Nets and how to use them for Transfer Learning

----

### Day 4: Generative Models (VAEs & GANs)

*coordinated by [Vikash Gilja (UCSD)](https://scholar.google.com/citations?user=EO3cAGQAAAAJ&hl=en) and [Akash Srivastava (MIT-IBM)](http://akashgit.github.io/)*

**Description** Variational Auto-Encoders (VAEs) and Generative Adversarial Networks (GANs) as methods for representing latent data statistics

----

### Day 5: Intro to NLP

*coordinated by [Lyle Ungar (U Penn)](https://www.cis.upenn.edu/~ungar/)*

**Description** Memory, time series, recurrence, vanishing gradients and embeddings 

----

## Week 3: Advanced methods

----

### Day 1: Attention and Transformers

*coordinated by [He He (NYU)](https://hhexiy.github.io/)*

**Description** How attention helps classification, encoding and decoding

----

### Day 2: DL Thinking (part 2)

*coordinated by [Konrad Kording (U Penn)](http://koerding.com/)* and *coordinated by [Lyle Ungar (U Penn)](https://www.cis.upenn.edu/~ungar/)*

**Description** TBD

----

### Day 3:  Unsupervised and Self-supervised Learning 

*coordinated by [Blake Richards (McGill)](https://sites.google.com/mila.quebec/linc-lab/home) and [Tim Lillicrap (Google DeepMind)](https://contrastiveconvergence.net/)*

**Description** Learning without direct supervision

----

### Day 4: Basic Reinforcement Learning (RL) ideas 

*coordinated by [Jane Wang (Google DeepMind)](http://www.janexwang.com/) and [Feryal Behbahani (Google DeepMind)](https://feryal.github.io/)*

**Description** How RL can help solve DL problems

----

### TDay 5: RL for games

*coordinated by [Tim Lillicrap (Google DeepMind)](https://contrastiveconvergence.net/) and [Blake Richards (McGill)](https://sites.google.com/mila.quebec/linc-lab/home)*

**Description** Get to learn how RL solved the game of Go

**Description** After the tutorials the day is dedicated to group projects and celebrating course completion

----

# Licensing

[![CC BY 4.0][cc-by-image]][cc-by]

[![CC BY 4.0][cc-by-shield]][cc-by] [![BSD-3][bsd-3-shield]][bsd-3]

The contents of this repository are shared under under a [Creative Commons Attribution 4.0 International License][cc-by].

Software elements are additionally licensed under the [BSD (3-Clause) License][bsd-3].

Derivative works may use the license that is more appropriate to the relevant context.

[cc-by]: http://creativecommons.org/licenses/by/4.0/
[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png
[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg

[bsd-3]: https://opensource.org/licenses/BSD-3-Clause
[bsd-3-shield]: https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667
