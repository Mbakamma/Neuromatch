{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W2D3_Tutorial1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DyxK538kFO6k",
        "8O7D6y_HXDP_",
        "Jiy11EjTFO6p",
        "2e_V1tGDpJNX",
        "HqNJ5jWgpSCB",
        "fP0QFihrsrGS",
        "kuoVYFKBtvU6",
        "-WkoBS0LuAK9",
        "7ECoxlc0uY0Y",
        "30R9lGTapUt3",
        "OsNBxEe5zYCz",
        "oI0xy9E8zYC3",
        "EYUqaNE8zYC5",
        "g4p1DgypCXjO",
        "o72iuoypppbq",
        "c92VC8DYvPY1",
        "UjAyx16xvccJ"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/w2d3_t1_initial/W2D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_FNWVNHwEUc"
      },
      "source": [
        "# Neuromatch Academy: Week 2, Day 3, Tutorial 1\n",
        "# Modern RNNs\n",
        "\n",
        "__Content creators:__ James Evans, Bhargav Srinivasa Desikan, Anis Zahedifard\n",
        "\n",
        "__Content reviewers:__ XXXXX, XXXXXX\n",
        "\n",
        "__Content editors:__ XXXXX\n",
        "\n",
        "__Production editors:__ Roberto Guidotti, Spiros Chavlis, XXXXXX\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e4vDKlzJ7lX"
      },
      "source": [
        "----\n",
        "# Tutorial objectives\n",
        "\n",
        "Before we begin with exploring how RNNs excel at modelling sequences, we will explore some of the other ways we can model sequences, encode text, and make meaningful measurements using such encodings and embeddings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjV6D0zVv-ve"
      },
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "F1G3jNFcOaIP"
      },
      "source": [
        "# @title Imports\n",
        "# imports\n",
        " \n",
        "!pip install --upgrade gensim --quiet\n",
        "!pip install torchtext==0.4.0 --quiet\n",
        "!pip install unidecode --quiet\n",
        "!pip install d2l --quiet\n",
        " \n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import nltk\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import unidecode\n",
        "import collections\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import seaborn as sns\n",
        "\n",
        " \n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import brown\n",
        "from sklearn.manifold import TSNE\n",
        "from torch.autograd import Variable\n",
        "from torchtext import data, datasets\n",
        "from torchtext.vocab import Vectors\n",
        " \n",
        "from IPython.display import Image, YouTubeVideo\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l\n",
        " \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "nltk.download('webtext')\n",
        " \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GvgJq_JGPRk-"
      },
      "source": [
        "# @title Seed\n",
        "# imports\n",
        "\n",
        "seed = 522\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0GPabAFTPtoG"
      },
      "source": [
        "# @title Figure Settings\n",
        "import ipywidgets as widgets\n",
        "%matplotlib inline \n",
        "fig_w, fig_h = (8, 6)\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "SMALL_SIZE = 12\n",
        "\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/\"\n",
        "              \"course-content/master/nma.mplstyle\")\n",
        "\n",
        "# plt.rcParams.update(plt.rcParamsDefault)\n",
        "# plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "# plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "# plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
        "# plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "# plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "# plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "# plt.rc('figure', titlesize=SMALL_SIZE)  # fontsize of the figure title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7fxzUO4ojbh"
      },
      "source": [
        "---\n",
        "# Section 1: Sequences, Markov Chains & HMMs\n",
        "\n",
        "\n",
        "In this notebook we will be exploring the world of sequences - thinking of what kind of data can be thought of as sequences, and how these sequences can be represented as Markov Chains and Hidden Markov Models. These ideas and methods were an important part of natural language processing and language modelling, and serve as a useful way to ground ourselves before we dive into neural network methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te8rL0uQBRq2"
      },
      "source": [
        "## Section 1.1: What data are sequences?\n",
        "\n",
        "Native Sequences:\n",
        "\n",
        "- Temporally occurring events (e.g., history, stock prices)\n",
        "- Temporally processed events (e.g., communication)\n",
        "- Topologically connected components (e.g., polymers, peptides)\n",
        "\n",
        "Synthetic Sequences: \n",
        "\n",
        "- Anything processed as a sequence (e.g., scanned pixels in an image)\n",
        "\n",
        "Sequences can be represented as a Markov Process, since this notion of sequential data is intrinsically linked to RNNs, it is a good place for us to start, and natural language (text!) will be our sequence of choice. \n",
        "\n",
        "We will be using the Brown corpus which comes loaded with NLTK, and using the entire corpus - this requires a lot of RAM for some of the methods, so we recommend using a smaller subset of categories if you do not have enough RAM.\n",
        "\n",
        "We will be using some of the code from this [tutorial](https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html) and this [Jupyter notebook](https://github.com/StrikingLoo/ASOIAF-Markov/blob/master/ASOIAF.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK9nmlOsEzpF"
      },
      "source": [
        "category = ['editorial', 'fiction', 'government', 'news', 'religion']\n",
        "sentences = brown.sents(categories=category) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXkElgkB7KgF"
      },
      "source": [
        "Now that we have our sentences, let us look at some statistics to get an idea of what we are dealing with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI5-01Cc0Rnu"
      },
      "source": [
        "lengths = [len(sentence) for sentence in sentences]\n",
        "lengths = pd.Series(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7hEc3Y30Rnu"
      },
      "source": [
        "lengths.quantile(.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCDxZPjz0Rnv"
      },
      "source": [
        "lengths.describe()\n",
        "# 14228 our of 18k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deZ8o1Yf0Rnv"
      },
      "source": [
        "sentences[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsupO5NG7sFg"
      },
      "source": [
        "Since we will be modelling words as sequences in sentences, let us first collect all the words in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6NmG-VO0kQk"
      },
      "source": [
        "corpus_words = []\n",
        "for sentence in sentences:\n",
        "  for word in sentence:\n",
        "    if \"''\" not in word and \"``\" not in word:\n",
        "      corpus_words.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-E3ahTg0Rny"
      },
      "source": [
        "print(\"Corpus length: \"+ str(len(corpus_words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI_59e8r0Rny"
      },
      "source": [
        "corpus_words[0:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izaBeKVk70FN"
      },
      "source": [
        "We'll now get distinct words and create a matrix to represent all these words - this is necessary because we will be using this matrix to look at the probability of the words in sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Ql1N_J0Rnz"
      },
      "source": [
        "distinct_words = list(set(corpus_words))\n",
        "word_idx_dict = {word: i for i, word in enumerate(distinct_words)}\n",
        "distinct_words_count = len(list(set(corpus_words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmApIkuhOPPY"
      },
      "source": [
        "print(\"Number of distinct words: \" + str(distinct_words_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XKdNPDd0Rnz"
      },
      "source": [
        "next_word_matrix = np.zeros([distinct_words_count, distinct_words_count])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH69uBl40Rnz"
      },
      "source": [
        "word_idx_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7EAYfKQ8WlJ"
      },
      "source": [
        "In the following lines of code we are populating the matrix that tracks the next word in a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbTaB4pu0Rn0"
      },
      "source": [
        "for i, word in enumerate(corpus_words[:-1]):\n",
        "    first_word_idx = word_idx_dict[word]\n",
        "    next_word_idx = word_idx_dict[corpus_words[i+1]]\n",
        "    next_word_matrix[first_word_idx][next_word_idx] +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcPMILfsBNni"
      },
      "source": [
        "## Section 1.2: What is a Markov Chain or Model?\n",
        "\n",
        "A Markov Chain (or Model) is a:\n",
        "- stochastic model describing a sequence of possible events\n",
        "- the probability of each event depends only on the state attained in the previous event.\n",
        "- a countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC) [vs. a continuous-time process or CTMC].\n",
        "- The classic formal language model is a Markov Model\n",
        "\n",
        "*Helpful explanations from [eric mjl's tutorial](https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/#non-autoregressive-homoskedastic-emissions)*!\n",
        "\n",
        "\n",
        "\n",
        "The simplest Markov models assume that we have a _system_ that contains a finite set of states,\n",
        "and that the _system_ transitions between these states with some probability at each time step $t$,\n",
        "thus generating a sequence of states over time.\n",
        "Let's call these states $S$, where\n",
        "\n",
        "$$S = \\{s_1, s_2, ..., s_n\\}$$\n",
        "\n",
        "To keep things simple, let's start with three states:\n",
        "\n",
        "$$S = \\{s_1, s_2, s_3\\}$$\n",
        "\n",
        "A Markov model generates a sequence of states, with one possible realization being:\n",
        "\n",
        "$$\\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\}$$\n",
        "\n",
        "And generically, we represent it as a sequence of states $x_t, x_{t+1}... x_{t+n}$. (We have chosen a different symbol to not confuse the \"generic\" state with the specific realization. Graphically, a plain and simple Markov model looks like the following:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bycv4JzB_8mS"
      },
      "source": [
        "![Screenshot 2021-06-16 at 10.24.31 PM.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z3BfitC8fcX"
      },
      "source": [
        "### Modelling transitions between states\n",
        "\n",
        "To know how a system transitions between states, we now need a **transition matrix**.\n",
        "The transition matrix describes the probability of transitioning from one state to another.\n",
        "(The probability of staying in the same state is semantically equivalent to transitioning to the same state.) \n",
        "\n",
        "By convention, transition matrix rows correspond to the state at time $t$,\n",
        "while columns correspond to state at time $t+1$.\n",
        "Hence, row probabilities sum to one,\n",
        "because the probability of transitioning to the next state depends on only the current state,\n",
        "and all possible states are known and enumerated.\n",
        "\n",
        "Let's call the transition matrix $P_{transition}$.\n",
        "The symbol etymology, which usually gets swept under the rug in mathematically-oriented papers, are as follows: \n",
        "\n",
        "- $transition$ doesn't refer to time but simply indicates that it is for transitioning states, \n",
        "- $P$ is used because it is a probability matrix.\n",
        "\n",
        "$$ P_{transition} = \n",
        "\\begin{pmatrix}\n",
        "    p_{11} & p_{12} & p_{13}\\\\\n",
        "    p_{21} & p_{22} & p_{23}\\\\\n",
        "    p_{31} & p_{32} & p_{33}\\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Using the transition matrix, we can express that the system likes to stay in the state that it enters into,\n",
        "by assigning larger probability mass to the diagonals.\n",
        "Alternatively, we can express that the system likes to transition out of states that it enters into,\n",
        "by assigning larger probability mass to the off-diagonal.\n",
        "\n",
        "In our case, this matrix is created by seeing which words appear after each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5q54u0h0Rn0"
      },
      "source": [
        "def most_likely_word_after(aWord):\n",
        "    most_likely = next_word_matrix[word_idx_dict[aWord]].argmax()\n",
        "    return distinct_words[most_likely]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3T52jWT9oE3"
      },
      "source": [
        "Using our most likely word function, we can begin to create chains of words and create sequences. In the code below we create a naive chain that simply choses the most likely word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pw_huLz0Rn0"
      },
      "source": [
        "def naive_chain(seed, length=15):\n",
        "    current_word = seed\n",
        "    sentence = seed\n",
        "\n",
        "    for _ in range(length):\n",
        "        sentence+=' '\n",
        "        next_word = most_likely_word_after(current_word)\n",
        "        sentence+=next_word\n",
        "        current_word = next_word\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW7ZAwc30Rn0"
      },
      "source": [
        "print(naive_chain('the'))\n",
        "print(naive_chain('I'))\n",
        "print(naive_chain('he'))\n",
        "print(naive_chain('she'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-exbnFP90ks"
      },
      "source": [
        "We can now be a little more sophisticated, and return words in a sequence using a weighted choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_DJxFMu0Rn1"
      },
      "source": [
        "from random import random \n",
        "\n",
        "def weighted_choice(objects, weights):\n",
        "    \"\"\" returns randomly an element from the sequence of 'objects', \n",
        "        the likelihood of the objects is weighted according \n",
        "        to the sequence of 'weights', i.e. percentages.\"\"\"\n",
        "\n",
        "    weights = np.array(weights, dtype=np.float64)\n",
        "    sum_of_weights = weights.sum()\n",
        "    # standardization:\n",
        "    np.multiply(weights, 1 / sum_of_weights, weights)\n",
        "    weights = weights.cumsum()\n",
        "    x = random()\n",
        "    for i in range(len(weights)):\n",
        "        if x < weights[i]:\n",
        "            return objects[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrqyiRGY0Rn1"
      },
      "source": [
        "from numpy.random import choice\n",
        "\n",
        "def sample_next_word_after(word, alpha=0):\n",
        "    next_word_vector = next_word_matrix[word_idx_dict[word]] + alpha\n",
        "    likelihoods = next_word_vector/next_word_vector.sum()\n",
        "    return weighted_choice(distinct_words, likelihoods)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy9zIyGz0Rn1"
      },
      "source": [
        "sample_next_word_after('the')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSo9hL7g0Rn1"
      },
      "source": [
        "def stochastic_chain(seed, length=15):\n",
        "    current_word = seed\n",
        "    sentence = seed\n",
        "\n",
        "    for _ in range(length):\n",
        "        sentence+=' '\n",
        "        next_word = sample_next_word_after(current_word)\n",
        "        sentence+=next_word\n",
        "        current_word = next_word\n",
        "        \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltx98OIx0Rn2"
      },
      "source": [
        "stochastic_chain('Hospital')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oM1MYI--D8f"
      },
      "source": [
        "Neat - we can create stochastic chains for a single word. For a more effective language model, we would want to model sets of words - in the following cells, we create sets of words to predict a chain after a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCl_PRXW0Rn2"
      },
      "source": [
        "k = 3\n",
        "sets_of_k_words = [ ' '.join(corpus_words[i:i+k]) for i, _ in enumerate(corpus_words[:-k]) ]\n",
        "\n",
        "print([len(list(set(sets_of_k_words))),\n",
        "       len(sets_of_k_words)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRXeUmaE0Rn3"
      },
      "source": [
        "from scipy.sparse import dok_matrix\n",
        "\n",
        "sets_count = len(list(set(sets_of_k_words)))\n",
        "next_after_k_words_matrix = dok_matrix((sets_count, len(distinct_words)))\n",
        "print(next_after_k_words_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPXQW3Mn0Rn3",
        "scrolled": true
      },
      "source": [
        "distinct_sets_of_k_words = list(set(sets_of_k_words))\n",
        "k_words_idx_dict = {word: i for i, word in enumerate(distinct_sets_of_k_words)}\n",
        "distinct_k_words_count = len(list(set(sets_of_k_words)))\n",
        "print(len(sets_of_k_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki8VdGW2vJ-S"
      },
      "source": [
        "for i, word in enumerate(sets_of_k_words[:-k]):\n",
        "    if i % 50000 == 0:\n",
        "        print(i)\n",
        "    word_sequence_idx = k_words_idx_dict[word]\n",
        "    next_word_idx = word_idx_dict[corpus_words[i+k]]\n",
        "    next_after_k_words_matrix[word_sequence_idx, next_word_idx] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gntXYPbr0Rn4"
      },
      "source": [
        "def stochastic_chain(seed, chain_length=15, seed_length=2):\n",
        "    current_words = seed.split(' ')\n",
        "    if len(current_words) != seed_length:\n",
        "        raise ValueError(f'wrong number of words, expected {seed_length}')\n",
        "    sentence = seed\n",
        "\n",
        "    for _ in range(chain_length):\n",
        "        sentence+=' '\n",
        "        next_word = sample_next_word_after_sequence(' '.join(current_words))\n",
        "        sentence+=next_word\n",
        "        current_words = current_words[1:]+[next_word]\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHdt14dA0Rn4"
      },
      "source": [
        "from numpy.random import choice\n",
        "\n",
        "def sample_next_word_after_sequence(word_sequence, alpha = 0):\n",
        "    next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]] + alpha\n",
        "    likelihoods = next_word_vector/next_word_vector.sum()\n",
        "    return weighted_choice(distinct_words, likelihoods.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z8iehgv5JvZ"
      },
      "source": [
        "k_words_idx_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL6uYlpx0Rn4"
      },
      "source": [
        "stochastic_chain('in The Story', seed_length=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIxSLObO0Rn7"
      },
      "source": [
        "distinct_sets_of_k_words[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NALLYSfLBObB"
      },
      "source": [
        "\n",
        "\n",
        "## Section 1.3: What is a Hidden Markov Model?\n",
        "\n",
        "A 1960s advance (by Leonard Baum and colleagues): Hidden Markov Models are:\n",
        "- a Markov model in which the system modeled is assumed to be a Markov process/chain with unobservable (\"hidden\") states. \n",
        "- HMM assumes there is another surrogate process whose behavior \"depends\" on the state--you learn about the state by observing the surrogate process. \n",
        "- HMMs have successfully been applied in fields where the goal is to recover a data sequence not immediately observable (but other data that depend on the sequence are).\n",
        "- The first dominant application: Speech and text processing (1970s)\n",
        "\n",
        "In this sub-section we will use the python library [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/tutorial.html#training-hmm-parameters-and-inferring-the-hidden-states), which is part of the *scikit-learn* ecosystem. [nlg-with-hmmlearn](https://github.com/mfilej/nlg-with-hmmlearn) offers useful code snippets to adapt ```hmmlearn``` for text data. Because we are using a package that offers many out of the box implementations for HMMs, we don't have to worry about the states, transition matrices, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYiMpcJ4EWvH"
      },
      "source": [
        "!pip install hmmlearn --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YuGast3BeOc"
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from hmmlearn import hmm\n",
        "from nltk import FreqDist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozcUe65mIQuA"
      },
      "source": [
        "sentences = brown.sents(categories=category)\n",
        "words = [word.lower() for sentence in sentences for word in sentence]\n",
        "lengths = [len(sentence) for sentence in sentences]\n",
        "alphabet = set(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqOc5k56EkNo"
      },
      "source": [
        "le = LabelEncoder()\n",
        "_ = le.fit(list(alphabet))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0rb5eknEmLX"
      },
      "source": [
        "seq = le.transform(words)\n",
        "features = np.fromiter(seq, np.int64)\n",
        "features = np.atleast_2d(features).T\n",
        "fd = FreqDist(seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJP6DGLRKUL4"
      },
      "source": [
        "Now that we have our data setup, we can create our model. We use a multinomial HMM with 8 states, and can either do a random initialisation or use word frequences. We recommend trying both options!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdVqTLq5Fa6v"
      },
      "source": [
        "def get_model(num_states):\n",
        "    print(\"Initial parameter estimation using built-in method\")\n",
        "    model = hmm.MultinomialHMM(n_components=num_states, init_params='ste')\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICfsZBjPFb_3"
      },
      "source": [
        "### Note: It seems that it is not used !\n",
        "def frequencies(num_states):\n",
        "    print(\"Initial parameter estimation using relative frequencies\")\n",
        "\n",
        "    frequencies = np.fromiter((fd.freq(i) for i in range(len(alphabet))), dtype=np.float64)\n",
        "    emission_prob = np.stack([frequencies]*num_states)\n",
        "\n",
        "    model = hmm.MultinomialHMM(n_components=num_states, init_params='st')\n",
        "    model.emissionprob_ = emission_prob\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjjEQjyvGCjQ"
      },
      "source": [
        "num_states = 8\n",
        "model = get_model(num_states)\n",
        "model = model.fit(features, lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2OgUALaI96h"
      },
      "source": [
        "num_lines = 5\n",
        "for _i in range(num_lines):\n",
        "    random_len = 15\n",
        "    seed = seed + 1\n",
        "\n",
        "    symbols, _states = model.sample(random_len)\n",
        "\n",
        "    output = le.inverse_transform(np.squeeze(symbols))\n",
        "    for word in output:\n",
        "        print(word, end=\" \")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM8SgjxaEQJw"
      },
      "source": [
        "### Useful links for Markov Models and HMM:\n",
        "\n",
        "[Markov Chain Text](https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6)\n",
        "\n",
        "[Python QuantEcon: Finite Markov Chains with Finance](https://python.quantecon.org/finite_markov.html)\n",
        "\n",
        "[Markov Models from the ground up, with python](https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/)\n",
        "\n",
        "[GenTex](https://github.com/nareshkumar66675/GenTex)\n",
        "\n",
        "[HMM learn](https://hmmlearn.readthedocs.io/en/latest/tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8l3VgMiCV_f"
      },
      "source": [
        "## Exercise 1 \n",
        "\n",
        "***Note: Don't know where to put it***\n",
        "\n",
        "We have seen how we can use sequences of text to form probability chains, as well as how we can use out of the box models to generate text. In this exercise, you will be using your own data to generate sequences using hmmlearn. Explore the transition probabilities in your corpus and generate sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmOe9Z5q7pKh"
      },
      "source": [
        "---\n",
        "# Section 2: Encoding Text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRH69vgrubl-"
      },
      "source": [
        "## Section 2.1: Text Data - Cleaning, Organising, and Representing\n",
        "\n",
        "This section will cover basic code and key concepts for textual data cleaning, organising and representing. We often describe this part of dealing with data as pre-processing. We also note that it is often encouraged to perform less pre-processing with deep learning methods and more data (e.g., stem with small data and non-neural methods; do not stem with big data and neural methods.) At the limits, with extremely large data and very deep models, we may actually add noise to the data to increase the robustness of our models to errors and outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrJvuTh1D47t",
        "cellView": "form"
      },
      "source": [
        "# @title Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# no critical warnings, so we supress it\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UhXYU_LD47u",
        "cellView": "form"
      },
      "source": [
        "# @title Text related packages\n",
        "import gensim\n",
        "import spacy\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yXxsUbLD47v",
        "cellView": "form"
      },
      "source": [
        "# @title ML and DL packages\n",
        "import sklearn\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXVA5e5rD47w"
      },
      "source": [
        "Let's whip up some trivial texts so that we can illustrate the creation of different representations. We will construct our raw texts such that there are a few overlapping words. Note that there are 4 sentences, and each sentence is the complete document or text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNvCU1_vD47w"
      },
      "source": [
        "raw_texts = [\"Apples taste like onions when we cannot smell.\",\n",
        "             \"I find it easy to compare apples and oranges.\",\n",
        "             \"Chopping onions can make you cry.\",\n",
        "             \"Peeling an orange does not make you cry.\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD3R6sOeD47x"
      },
      "source": [
        "We now load an English [spaCy language model](https://spacy.io/usage/models), which allows us to do a number of basic linguistic and textual analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTrstSthD47x"
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSLE2S4eD47x"
      },
      "source": [
        "Our first use of the language model is to clean the data - when we pass text through the language model, it [tokenises](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) it, and assigns various attributes, such as whether or not it is a [stop word](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html), number, and its part of speech. A stop word is a word we deem irrelevant or will distract from or diminish our analysis - some examples are articles (a, an, the), filler words, and so on. You will notice that here we also used a [lemmatised](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) form of the word, which is a way of extracting the linguistic 'root' of the word. Again, the more plentiful your data and deeper your model (e.g., BERT), the less likely you should be to remove, prune and process text data because the model is designed to do it for you. Even then, however, sometimes augmenting data with these representations can accelerate model training and performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAUaAi7BD47y"
      },
      "source": [
        "texts, article = [], []\n",
        "for text in raw_texts:\n",
        "    doc = nlp(text)\n",
        "    article = []\n",
        "    for w in doc:\n",
        "        # if it's not a stop word or punctuation mark, add it to our article!\n",
        "        if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
        "            # we add the lematized version of the word\n",
        "            article.append(w.lemma_)\n",
        "\n",
        "    texts.append(article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7Rpn3ND47y"
      },
      "source": [
        "Let's see what that did to our texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20aQu4M4D47y"
      },
      "source": [
        "print(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ6Bbg2kD470"
      },
      "source": [
        "We see that from a list of strings we now have a list of lists, and each of these lists is a list of strings (words, or tokens). You can see that punctuation and stop words are removed and it is now closer to machine-readable and analysis-ready. This process of going from raw, punctuated text to individual tokens is called tokenization.\n",
        "\n",
        "Now, we use the gensim dictionary class to map each of these words to an ID and represent them as a bag of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq1b1RloD470"
      },
      "source": [
        "dictionary = Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZI3FaNpD470"
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T2zaYV6D471"
      },
      "source": [
        "We see here that each sentence is represented by a word ID and the number of times it was present in that document. We now look at Term Frequency - Inverse Document Frequency ([TF-IDF](https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html)), one simple way to measure word importance (locally and globally). Such representations begin to build representations that begin to approximate some of the things automatically learned within a deep model. Sometimes these representations can be fed into a deep model as input (augmented data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xdP3B8jD471"
      },
      "source": [
        "from gensim.models import TfidfModel\n",
        "\n",
        "tf_idf_model = TfidfModel(corpus)\n",
        "tf_idf_texts = tf_idf_model[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8plCKrsMD471"
      },
      "source": [
        "tf_idf_texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8n3XeP5D472"
      },
      "source": [
        "tf_idf_texts[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS1Oh7RBD472"
      },
      "source": [
        "We now see each document is a word ID mapped to a number that measures how it distinguishes the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m4MezHID472"
      },
      "source": [
        "### One Hot Encoding and Vectorising with scikit-learn\n",
        "\n",
        "So we saw how we can go from raw, punctuated texts to cleaner texts my manually picking out what we want. The bag of words format tells us which words exist, but the vector length isn't standard, and you can see the same for TF-IDF. There are ways to present this information in what is called a 1-hot vector, where the length of the vector is the whole vocabulary. \n",
        "\n",
        "It can be useful to use gensim and spaCy for pre-processing because it is more clear at each step what my data structure looks like. However, for an abstracted approach to text pre-processing, scikit-learn works well, especially when you want to feed it into a traditional machine learning algorithm. Gensim, for example, would convert bag of words or tf-idf into a similar data structure under the hood to keep the lengths of the vector the same. \n",
        "\n",
        "See below for code to use the scikit-learn vectoriser, which creates both one hot encoding vectors as well as tf-idf vectors which are **sparse representations**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLNc9yoCD473"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMD1O3cRD473"
      },
      "source": [
        " vectorizer = CountVectorizer()\n",
        " X = vectorizer.fit_transform(raw_texts).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQrM0DAts77i"
      },
      "source": [
        "vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1om_8mJctHQZ"
      },
      "source": [
        "raw_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCiDvSWBtJgR"
      },
      "source": [
        "len(X[0]), len(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUKjXpS0tOuk"
      },
      "source": [
        "You can see that the length of each vector is the length of the full vocavulary. Below, for each document, a 1 is a presence of that word_id (which you can confirm using the dictionary above), and a 0 is the absence of a word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay1WJ2EjD474"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYIMrPHntg4A"
      },
      "source": [
        "Below this we create a similar sparse representation, but using TF-IDF, and remove stop-words using the built-in function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFNLq6PJD475"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(raw_texts).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-KYGG4KD476"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR2bSIhGt46V"
      },
      "source": [
        "We will now be moving on from sparse representations to dense representations of documents, with a range of methods which can be used to condense information in a body of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNIFBeoOD477"
      },
      "source": [
        "### Low and High Dimensional Representations of Text and Documents\n",
        "\n",
        "One of the issues with using a bag of words or TF-IDF to represent documents is that even though you’ve reduced the size of the document from the full vocabulary, for documents with a lot of words, they remain very high dimensional objects (e.g., where each dimension is defined by a word in the vocabulary, a long document might have 20,000 dimensions or unique words). LSA stands for latent semantic analysis (sometimes also referred to as LSI, or Latent Semantic Indexing), and is a way to represent text documents by reducing the dimensionality of the word (or meaning)-space represented by a TF-IDF or bag of words. The idea is this: if words in documents are rows and the documents are columns, we want to keep the same distance between columns (documents) while reducing the number of rows (reduced word-features). By representing documents with a reduced number of features instead of using all the words, we now represent each with “latent” features rooted in the idea of distributional semantics--that words appearing together are likely to reference similar meanings.\n",
        "\n",
        "Let us use `TruncatedSVD` to perform a latent semantic analysis on our data. SVD -- Singular Value Decomposition -- is a form of matrix factorization very similar to Principle Components Analysis (PCA), which we saw last week, but it can be computed on rectangular (not square) datasets. The truncation comes as we limit the number of Singular Values (dimensions) that we consider by selecting the most informative and distinguishing $n$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBoOB7KbD477"
      },
      "source": [
        "svd = TruncatedSVD(2)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "X = lsa.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEkR5_e1D477"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj_5pGbzD478"
      },
      "source": [
        "Each document here is represented by 2 features ($n=2$)!\n",
        "\n",
        "Now we'll take a quick peak at topic models, word and document embedding methods, and other low dimensional methods that can be used to represent document in meaning space. \n",
        "\n",
        "We're going to use a bigger dataset, because these methods are only useful when we have substantial data. Models based on very large datasets take time to train, so we'll use a medium sized corpus that comes pre-loaded with gensim to illustare the concepts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOFZSMfjD478"
      },
      "source": [
        "import os\n",
        "\n",
        "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
        "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
        "text = open(lee_train_file).read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE-DLu6bD479"
      },
      "source": [
        "doc = nlp(text.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SBFBQJkD479"
      },
      "source": [
        "my_stop_words = [u'say', u'mr', u'said', u'saying', u'says']\n",
        "for stopword in my_stop_words:\n",
        "    lexeme = nlp.vocab[stopword]\n",
        "    lexeme.is_stop = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHIztrL5D479"
      },
      "source": [
        "# we add some words to the stop word list\n",
        "texts, article = [], []\n",
        "for w in doc:\n",
        "    # if it's not a stop word or punctuation mark, add it to our article!\n",
        "    if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and w.text != 'I':\n",
        "        # we add the lematized version of the word\n",
        "        article.append(w.lemma_)\n",
        "    # if it's a new line, it means we're onto our next document\n",
        "    if w.text == '\\n':\n",
        "        texts.append(article)\n",
        "        article = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgVr8ooED479"
      },
      "source": [
        "len(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9ZXJ2ZvD47-"
      },
      "source": [
        "texts[0][0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWmig-VoD47-"
      },
      "source": [
        "### Bi-grams and n-grams\n",
        "Here we identify statistically significant bigrams (two sequential word units of meaning) that may go into a model as single units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQYYgqa7D47_"
      },
      "source": [
        "bigram = gensim.models.Phrases(texts)\n",
        "texts = [bigram[line] for line in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0q_4zmkD47_"
      },
      "source": [
        "texts[0][0:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMP9k8uVD47_"
      },
      "source": [
        "### Topic Models\n",
        "Topic models are a two-level clustering technique in which topics are identified as distributions over words, such that documents may be modeled as distributions over topics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm1U_lGCD47_"
      },
      "source": [
        "dictionary = Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "ldamodel = LdaModel(corpus=corpus, num_topics=5, id2word=dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0At5jt-D48A"
      },
      "source": [
        "ldamodel.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW2Y5TfYD48A"
      },
      "source": [
        "lda_doc_5 = ldamodel[dictionary.doc2bow(texts[5])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP7pyqEKD48A"
      },
      "source": [
        "lda_doc_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkPZgkM8D48A"
      },
      "source": [
        "texts[5][0:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waR1RXDfD48B"
      },
      "source": [
        "The document on Australian government and asylum is recognized as being composed of a topic with the top words of \"Australia\", \"government\", and \"people\". Now we've seen how both LDA and LSI, in two different packages, and a variety of ways to represent textual documents. Some representations of textual documents can be especially useful for visualising documents - let's try a few of these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU6cj3KXlvAs"
      },
      "source": [
        "## Section 2.2: Shallow Neural Embeddings\n",
        "\n",
        "Word2vec, much like methods like LSA which we saw earlier, is based on theories of distributional semantics - words that appear around each other are more likely to mean similar things than words that do not appear around each other. Keeping this in mind, our job is to create a high dimensional space where these semantic relations are preserved. The innovation in word2vec is the realisation that we can use unlabelled, running text in sentences as inputs for a supervised learning algorithm--as a self-supervision task. It is supervised because we use the words in a sentence to serve as positive and negative examples. Let’s break this down:\n",
        "\n",
        "... \"use the kitchen knife to chop the vegetables\"…\n",
        "\n",
        "**C1   C2   C3   T   C4   C5   C6   C7**\n",
        "\n",
        "Here, the target word is knife, and the context words are the ones in its immediate (6-word) window. \n",
        "The first word2vec method we’ll see is called skipgram, where the task is to assign a probability for how likely it is that the context window appears around the target word. In the training process, positive examples are samples of words and their context words, and negative examples are created by sampling from pairs of words that do not appear nearby one another. \n",
        "\n",
        "This method of implementing word2vec is called skipgram with negative sampling. So while the algorithm tries to better learn which context words are likely to appear around a target word, it ends up pushing the embedded representations for every word so that they are located optimally (e.g., with minimal semantic distortion). In this process of adjusting embedding values, the algorithm brings semantically similar words close together in the resulting high dimensional space, and dissimilar words far away. \n",
        "\n",
        "Another word2vec training method, Continuous Bag of Words (CBOW), works in a similar fashion, and tries to predict the target word, given context. This is converse of skipgram, which tries to predict the context, given the target word. Skip-gram represents rare words and phrases well, often requiring more data for stable representations, while CBOW is several times faster to train than the skip-gram, but with slightly better accuracy for the frequent words in its prediction task. The popular gensim implementation of word2vec has both the methods included. \n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKj3po7ZbD6Z"
      },
      "source": [
        "## Section 2.3: Creating Word Embeddings\n",
        "\n",
        "We will create embeddings for a subset of categories in [Brown corpus](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html).  In order to achieve this task we will use [gensim](https://radimrehurek.com/gensim/) library to create word2vec embeddings. Gensim’s word2vec expects a sequence of sentences as its input. Each sentence is a list of words.\n",
        "Calling Word2Vec(sentences, iter=1) will run two passes over the sentences iterator (or, in general iter+1 passes). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model. \n",
        "Word2vec accepts several parameters that affect both training speed and quality.\n",
        "\n",
        "One of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:\n",
        "\n",
        "`model = Word2Vec(sentences, min_count=10)  # default value is 5`\n",
        "\n",
        "\n",
        "A reasonable value for min_count is between 0-100, depending on the size of your dataset.\n",
        "\n",
        "Another parameter is the size of the NN layers, which correspond to the “degrees” of freedom the training algorithm has:\n",
        "\n",
        "`model = Word2Vec(sentences, size=200)  # default value is 100`\n",
        "\n",
        "\n",
        "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.\n",
        "\n",
        "The last of the major parameters (full list [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) is for training parallelization, to speed up training:\n",
        "\n",
        "`model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69aJ56KQP1EY"
      },
      "source": [
        "category = ['editorial', 'fiction', 'government', 'mystery', 'news', 'religion',\n",
        "            'reviews', 'romance', 'science_fiction']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4aQAUh8P-fK"
      },
      "source": [
        "def create_word2vec_model(category = brown.categories(), size = 50, sg = 1, min_count = 5):\n",
        "  try:\n",
        "    sentences = brown.sents(categories=category) \n",
        "    model = Word2Vec(sentences, vector_size=size, sg=sg, min_count=min_count)\n",
        "\n",
        "  except (AttributeError, TypeError):\n",
        "      raise AssertionError('Input variable \"category\" should be a string or list,' \n",
        "      '\"size\", \"sg\", \"min_count\" should be integers')\n",
        "\n",
        "  return model\n",
        "\n",
        "def model_dictionary(model):\n",
        "  words = list(w2vmodel.wv.key_to_index)\n",
        "  return words \n",
        "\n",
        "def get_embedding(word, model):\n",
        "  if word in w2vmodel.wv.key_to_index:\n",
        "    return model.wv[word]\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbcs4VR2QBIW"
      },
      "source": [
        "w2vmodel = create_word2vec_model(category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLe5CgBEQFVP"
      },
      "source": [
        "print(model_dictionary(w2vmodel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWCTijyEQGIz"
      },
      "source": [
        "print(get_embedding('weather', w2vmodel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ajhPp1hEmOE"
      },
      "source": [
        "We don't always need to train our own models. One of the advantages of word embedding (and many deep learning) models is using pre-trained models, where the training has already been completed on a large and/or representative corpus or document collection. In the following lines of code we load either a [Google News model](https://drive.google.com/file/d/1TW3h1SIx__4Y6zEzLnrPry2Z-6BZgtyh/view?usp=sharing) (3+ GB), or a [model trained on NY Times data](https://drive.google.com/file/d/1xrs1UhTH5LPTBFLcflX-nQ86zwCAoLt5/view?usp=sharing) (30 MB). If you have Colab Pro, you can try the heavier model. I have used the mounting capacity on google drive here. You can also just upload the file if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiv-MIHEiDWG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o7OTP2SiJGi"
      },
      "source": [
        "model_address = \"/content/drive/MyDrive/nytimes_cbow.reduced.txt\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRZM2GU3iJDf"
      },
      "source": [
        "nytimes_w2v_model = gensim.models.KeyedVectors.load_word2vec_format(model_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfUW9xYiiI_u"
      },
      "source": [
        "nytimes_w2v_model.most_similar(\"book\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6WqbsrbiV4-"
      },
      "source": [
        "nytimes_w2v_model.most_similar(\"talk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeBjXC7nmISM"
      },
      "source": [
        "### Word and Context Embeddings\n",
        "\n",
        "The gensim model allows us to access both the target embedding for the word itself, and the context embedding for words. These two embedding are multiplied to recreate the original word space, revealing the way in which neural embeddings perform (and are optimized through) matrix factorization of the original word-by-context matrix. The target embedding is what is most commonly used, though in some cases the context is also used along with the target. We show how to use both.\n",
        "\n",
        "The context vector is not showcased in the documentation - we unearthed it thanks to this helpful [StackOverflow link](https://stackoverflow.com/questions/39406092/how-to-get-both-the-word-embeddings-vector-and-context-vector-of-a-given-word-by)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcA7eNT7LqY3"
      },
      "source": [
        "book_embedding_ny = nytimes_w2v_model[\"book\"]\n",
        "book_embedding = w2v_model[\"book\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD_nIROKjSIR"
      },
      "source": [
        " book_context_embedding = w2vmodel.wv.syn0[w2vmodel.wv.vocab[\"book\"].index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80j5g5_RkP5p"
      },
      "source": [
        " book_context_embedding_ny = nytimes_w2v_model.wv.syn0[nytimes_w2v_model.wv.vocab[\"book\"].index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoP02K9Bm4ZJ"
      },
      "source": [
        "### Exploring Semantics with Embeddings\n",
        "\n",
        "So what do these vectors encapsulate? We've seen before that it is some notion of meaning, as captured by distributional semantics, with some aspect of syntactic information. How do we then use these word embeddings? Because they capture semantic information in a space that allows for semantic decomposition, a common task is to perform semantic arithmetic and analogy tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMUxn0hzT5sS"
      },
      "source": [
        "Find the word that least matches the others within a word set (cosine similarity)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhhpaUcXnBYK"
      },
      "source": [
        "nytimes_w2v_model.doesnt_match(['books', 'books', 'novel', 'memoir', 'truck'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAnRLqu0tn85"
      },
      "source": [
        "nytimes_w2v_model.most_similar(positive=['nature', 'science'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjiYvF1Mtph2"
      },
      "source": [
        "nytimes_w2v_model.most_similar('cat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbwGnVX_T6cZ"
      },
      "source": [
        "We can also perform arithmetic on vectors using the most_similar function we saw earlier. One of the more popular, earlier examples demonstrating the analogous power of word2vec models was vector['king'] - vector['man'] + vector['woman'] = vector['queen'], where the idea is that we are traversing a \"gender\" dimension and looking at either end of it. \n",
        "\n",
        "Let's see how that works on our NY Times model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8f1AjzWT83z"
      },
      "source": [
        "nytimes_w2v_model.most_similar(positive=['king', 'woman'], negative = ['man'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8EJb90ZlcgR"
      },
      "source": [
        "If we shift ***king*** in the direction of ***woman***, we get to ***royal***... and almost to ***queen*** (try adding woman twice!) Interestingly, ***prince*** is a similarly effeminate royal. We can begin to see the utility of these models, even if they do capture all semantic associations as we might imagine. Larger language models capture even more sophisticated relationships. \n",
        "\n",
        "In the rest of this section we will see other ways in which we can use and explore word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE6UxNAwbl7Y"
      },
      "source": [
        "## Section 2.4: Visualizing Word Embedding\n",
        "\n",
        "We can now obtain the word embeddings for any word in the dictionary using word2vec. Let's visualize these embeddings to get an inuition of what these embeddings mean. The word embeddings obtained from word2vec model are in high dimensional space. We will use PCA to pick the 2 features that capture the most variance in the embeddings in order to represent them in a 2D space.\n",
        "\n",
        "\n",
        "For each word in `keys`, we pick the top 10 similar words (using cosine similarity) and plot them.  \n",
        "\n",
        " What should be the arrangement of similar words?\n",
        " What should be arrangement of the key clusters with respect to each other?\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBuK5xWQQIjY"
      },
      "source": [
        "keys = ['voters', 'magic', 'love', 'God', 'evidence', 'administration', 'governments']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ04KEsrQcYL"
      },
      "source": [
        "def get_cluster_embeddings(keys):\n",
        "  embedding_clusters = []\n",
        "  word_clusters = []\n",
        "\n",
        "  # find closest words and add them to cluster\n",
        "  for word in keys:\n",
        "      embeddings = []\n",
        "      words = []\n",
        "      if not word in w2vmodel.wv.key_to_index:\n",
        "        print('The word ', word, 'is not in the dictionary')\n",
        "        continue\n",
        "\n",
        "      for similar_word, _ in w2vmodel.wv.most_similar(word, topn=10):\n",
        "          words.append(similar_word)\n",
        "          embeddings.append(w2vmodel.wv[similar_word])\n",
        "      embedding_clusters.append(embeddings)\n",
        "      word_clusters.append(words)\n",
        "\n",
        "  # get embeddings for the words in clusers\n",
        "  embedding_clusters = np.array(embedding_clusters)\n",
        "  n, m, k = embedding_clusters.shape\n",
        "  tsne_model_en_2d = TSNE(perplexity=10, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
        "  embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
        "  return embeddings_en_2d, word_clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA29G3FfQeiU"
      },
      "source": [
        "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
        "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
        "        x = embeddings[:, 0]\n",
        "        y = embeddings[:, 1]\n",
        "        plt.scatter(x, y, color=color, alpha=a, label=label)\n",
        "        for i, word in enumerate(words):\n",
        "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
        "                         textcoords='offset points', ha='right', va='bottom', size=10)\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    if filename:\n",
        "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS5LjuOnQlrF"
      },
      "source": [
        "embeddings_en_2d, word_clusters = get_cluster_embeddings(keys)\n",
        "tsne_plot_similar_words('Similar words from Brown Corpus', keys, embeddings_en_2d, word_clusters, 0.7,\n",
        "                        'similar_words.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cStJoP3G775u"
      },
      "source": [
        "---\n",
        "# Section 3: Semantic Measurements\n",
        "Lecture\n",
        "Understanding this semantic space: \n",
        "(Can do many of the same measurements on sophisticated, BERT-style “spaces”)\n",
        "Implicit matrix factorization\n",
        "Conceptually (like Latent Semantic Indexing / Analysis)\n",
        "Algorithmically through process of optimization\n",
        "High-D manifold interpretation\n",
        "Semantic dimensions\n",
        "Topic decompositions\n",
        "Alignment \n",
        "Over time\n",
        "Domain\n",
        "Language\n",
        "Extensions:\n",
        "GLoVE (add back some global influence)\n",
        "FastText (consider character not word sequences)\n",
        "Tutorial\n",
        "Semantic dimensions\n",
        "Shift over time\n",
        "Vector decomposition (semantic atoms)\n",
        "Inversion for classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S2FBu-qSGYY"
      },
      "source": [
        "### Section 3.1: Projecting Embeddings on Dimensions of Social Analysis\n",
        "\n",
        "We can see how word2vec does remarkably well in capturing certain aspects of semantics! Word embeddings have a lot of utility outside of just looking at relations between words, though. Because words derive their meanings from various social characteristics and dimensions, it is possible for us to project word vectors onto word-defined dimensions to see where they lie. We can create these dimensions by choosing words which that might anchor the dimension and then project other words onto this dimension. For example, if we were trying to find a gender dimension, we might make a vector between the words he, him, man and she, her, woman, and so on. Then, with a cosine projection, we can get a value of where our word of choice might lie on this dimension, revealing to us how “gendered” the word is. The following snippets of code implements this for various such dimensions. Using such a technique for social scientific analysis as described by Kozlowski et al 2019; it can be very easily extended for many business and analytical solutions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr85WkR2SGYZ"
      },
      "source": [
        "First we can visualize with dimension reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anV88JgpSGYZ"
      },
      "source": [
        "#words to create dimensions\n",
        "tnytTargetWords = ['man','him','he', 'woman', 'her', 'she', \n",
        "                   'black','blacks','African', 'white', 'whites', \n",
        "                   'Caucasian', 'rich', 'richer', 'richest', 'expensive', \n",
        "                   'wealthy', 'poor', 'poorer', 'poorest', 'cheap', 'inexpensive']\n",
        "                   \n",
        "#words we will be mapping\n",
        "tnytTargetWords += [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \n",
        "                    \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \n",
        "                    \"banker\",\"poet\",\"nurse\", \"steak\", \"bacon\", \"croissant\", \n",
        "                    \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \n",
        "                    \"wine\", \"pastry\", \"basketball\", \"baseball\", \"boxing\", \"softball\", \n",
        "                    \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]\n",
        "\n",
        "\n",
        "wordsSubMatrix = []\n",
        "for word in tnytTargetWords:\n",
        "    wordsSubMatrix.append(nytimes_model[word])\n",
        "wordsSubMatrix = np.array(wordsSubMatrix)\n",
        "#wordsSubMatrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "m2D_qnW7SGYZ"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "pcaWordsNYT = PCA(n_components=50).fit(wordsSubMatrix)\n",
        "reducedPCA_dataNYT = pcaWordsNYT.transform(wordsSubMatrix)\n",
        "\n",
        "#T-SNE is theoretically better, but you should experiment\n",
        "tsneWordsNYT = TSNE(n_components=2).fit_transform(reducedPCA_dataNYT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9JLMNGvSGYf"
      },
      "source": [
        "fig = plt.figure(figsize = (20,12))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_frame_on(False)\n",
        "plt.scatter(tsneWordsNYT[:, 0], tsneWordsNYT[:, 1], alpha = 0) #Making the points invisible\n",
        "for i, word in enumerate(tnytTargetWords):\n",
        "    ax.annotate(word, (tsneWordsNYT[:, 0][i],tsneWordsNYT[:, 1][i]), size =  50 * (len(tnytTargetWords) - i) / len(tnytTargetWords))\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keteMCbaSGYg"
      },
      "source": [
        "Define some convenient functions for getting dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9B-vMN93SGYh"
      },
      "source": [
        "def normalize(vector):\n",
        "    normalized_vector = vector / np.linalg.norm(vector)\n",
        "    return normalized_vector\n",
        "\n",
        "def dimension(model, positives, negatives):\n",
        "    diff = sum([normalize(model[x]) for x in positives]) - sum([normalize(model[y]) for y in negatives])\n",
        "    return diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THSdZIuOSGYh"
      },
      "source": [
        "Let's calculate three dimensions: gender, race, and class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GOKUKVmGSGYh"
      },
      "source": [
        "Gender = dimension(nytimes_model, ['man','him','he'], ['woman', 'her', 'she'])\n",
        "Race = dimension(nytimes_model, ['black','blacks','African'], ['white', 'whites', 'Caucasian'])\n",
        "Class = dimension(nytimes_model, ['rich', 'richer', 'richest', 'expensive', 'wealthy'], \n",
        "                                 ['poor', 'poorer', 'poorest', 'cheap', 'inexpensive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLeoeBkBSGYh"
      },
      "source": [
        "Here we have some words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3wQdvi4BSGYi"
      },
      "source": [
        "Occupations = [\"doctor\", \"lawyer\", \"plumber\", \"scientist\", \"hairdresser\", \n",
        "               \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \n",
        "               \"banker\",\"poet\",\"nurse\"]\n",
        "\n",
        "Foods = [\"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \n",
        "         \"vegetables\", \"beer\", \"wine\", \"pastry\"]\n",
        "\n",
        "Sports  = [\"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \n",
        "           \"tennis\", \"golf\", \"hockey\", \"soccer\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22raR1Y9SGYi"
      },
      "source": [
        "Define a function to project words in a word list to each of the three dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oOE2Xf6NSGYj"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def makeDF(model, word_list):\n",
        "    g = []\n",
        "    r = []\n",
        "    c = []\n",
        "    for word in word_list:\n",
        "        g.append(cosine_similarity(nytimes_model[word].reshape(1,-1), Gender.reshape(1,-1))[0][0])\n",
        "        r.append(cosine_similarity(nytimes_model[word].reshape(1,-1), Race.reshape(1,-1))[0][0])\n",
        "        c.append(cosine_similarity(nytimes_model[word].reshape(1,-1), Class.reshape(1,-1))[0][0])\n",
        "    df = pandas.DataFrame({'gender': g, 'race': r, 'class': c}, index = word_list)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocAm5HjjSGYj"
      },
      "source": [
        "Get the projections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UY7x91KZSGYj"
      },
      "source": [
        "OCCdf = makeDF(nytimes_model, Occupations) \n",
        "Fooddf = makeDF(nytimes_model, Foods)\n",
        "Sportsdf = makeDF(nytimes_model, Sports)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNZEgWtSSGYj"
      },
      "source": [
        "Define some useful functions for plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4TAYDcQgSGYk"
      },
      "source": [
        "def Coloring(Series):\n",
        "    x = Series.values\n",
        "    y = x-x.min()\n",
        "    z = y/y.max()\n",
        "    c = list(plt.cm.rainbow(z))\n",
        "    return c\n",
        "\n",
        "def PlotDimension(ax,df, dim):\n",
        "    ax.set_frame_on(False)\n",
        "    ax.set_title(dim, fontsize = 20)\n",
        "    colors = Coloring(df[dim])\n",
        "    for i, word in enumerate(df.index):\n",
        "        ax.annotate(word, (0, df[dim][i]), color = colors[i], alpha = 0.6, fontsize = 12)\n",
        "    MaxY = df[dim].max()\n",
        "    MinY = df[dim].min()\n",
        "    plt.ylim(MinY,MaxY)\n",
        "    plt.yticks(())\n",
        "    plt.xticks(())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrI1stCQSGYk"
      },
      "source": [
        "Plot the occupational words in each of the three dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1BOXgIBSGYk",
        "scrolled": true
      },
      "source": [
        "fig = plt.figure(figsize = (12,4))\n",
        "ax1 = fig.add_subplot(131)\n",
        "PlotDimension(ax1, OCCdf, 'gender')\n",
        "ax2 = fig.add_subplot(132)\n",
        "PlotDimension(ax2, OCCdf, 'race')\n",
        "ax3 = fig.add_subplot(133)\n",
        "PlotDimension(ax3, OCCdf, 'class')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00nQWm0QSGYk"
      },
      "source": [
        "Foods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v8bTfUNSGYk"
      },
      "source": [
        "fig = plt.figure(figsize = (12,4))\n",
        "ax1 = fig.add_subplot(131)\n",
        "PlotDimension(ax1, Fooddf, 'gender')\n",
        "ax2 = fig.add_subplot(132)\n",
        "PlotDimension(ax2, Fooddf, 'race')\n",
        "ax3 = fig.add_subplot(133)\n",
        "PlotDimension(ax3, Fooddf, 'class')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYvqFqESGYl"
      },
      "source": [
        "Sports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdL2ZBKVSGYl"
      },
      "source": [
        "fig = plt.figure(figsize = (12,4))\n",
        "ax1 = fig.add_subplot(131)\n",
        "PlotDimension(ax1, Sportsdf, 'gender')\n",
        "ax2 = fig.add_subplot(132)\n",
        "PlotDimension(ax2, Sportsdf, 'race')\n",
        "ax3 = fig.add_subplot(133)\n",
        "PlotDimension(ax3, Sportsdf, 'class')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn85VN3Ddq4z"
      },
      "source": [
        "import argparse\n",
        "import gzip\n",
        "import math\n",
        "import numpy\n",
        "import re\n",
        "import sys\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "isNumber = re.compile(r'\\d+.*')\n",
        "def norm_word(word):\n",
        "  if isNumber.search(word.lower()):\n",
        "    return '---num---'\n",
        "  elif re.sub(r'\\W+', '', word) == '':\n",
        "    return '---punc---'\n",
        "  else:\n",
        "    return word.lower()\n",
        "\n",
        "''' Read all the word vectors and normalize them '''\n",
        "def read_word_vecs(filename):\n",
        "  wordVectors = {}\n",
        "  if filename.endswith('.gz'): fileObject = gzip.open(filename, 'r')\n",
        "  else: fileObject = open(filename, 'r')\n",
        "  \n",
        "  for line in fileObject:\n",
        "    line = line.strip().lower()\n",
        "    word = line.split()[0]\n",
        "    wordVectors[word] = numpy.zeros(len(line.split())-1, dtype=float)\n",
        "    for index, vecVal in enumerate(line.split()[1:]):\n",
        "      wordVectors[word][index] = float(vecVal)\n",
        "    ''' normalize weight vector '''\n",
        "    wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
        "    \n",
        "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
        "  return wordVectors\n",
        "\n",
        "''' Write word vectors to file '''\n",
        "def print_word_vecs(wordVectors, outFileName):\n",
        "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
        "  outFile = open(outFileName, 'w')  \n",
        "  for word, values in wordVectors.iteritems():\n",
        "    outFile.write(word+' ')\n",
        "    for val in wordVectors[word]:\n",
        "      outFile.write('%.4f' %(val)+' ')\n",
        "    outFile.write('\\n')      \n",
        "  outFile.close()\n",
        "  \n",
        "''' Read the PPDB word relations as a dictionary '''\n",
        "def read_lexicon(filename):\n",
        "  lexicon = {}\n",
        "  for line in open(filename, 'r'):\n",
        "    words = line.lower().strip().split()\n",
        "    lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
        "  return lexicon\n",
        "\n",
        "''' Retrofit word vectors to a lexicon '''\n",
        "def retrofit(wordVecs, lexicon, numIters):\n",
        "  newWordVecs = deepcopy(wordVecs)\n",
        "  wvVocab = set(newWordVecs.keys())\n",
        "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
        "  for it in range(numIters):\n",
        "    # loop through every node also in ontology (else just use data estimate)\n",
        "    for word in loopVocab:\n",
        "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
        "      numNeighbours = len(wordNeighbours)\n",
        "      #no neighbours, pass - use data estimate\n",
        "      if numNeighbours == 0:\n",
        "        continue\n",
        "      # the weight of the data estimate if the number of neighbours\n",
        "      newVec = numNeighbours * wordVecs[word]\n",
        "      # loop over neighbours and add to new vector (currently with weight 1)\n",
        "      for ppWord in wordNeighbours:\n",
        "        newVec += newWordVecs[ppWord]\n",
        "      newWordVecs[word] = newVec/(2*numNeighbours)\n",
        "  return newWordVecs\n",
        "  \n",
        "if __name__=='__main__':\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"-i\", \"--input\", type=str, default=None, help=\"Input word vecs\")\n",
        "  parser.add_argument(\"-l\", \"--lexicon\", type=str, default=None, help=\"Lexicon file name\")\n",
        "  parser.add_argument(\"-o\", \"--output\", type=str, help=\"Output word vecs\")\n",
        "  parser.add_argument(\"-n\", \"--numiter\", type=int, default=10, help=\"Num iterations\")\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  wordVecs = read_word_vecs(args.input)\n",
        "  lexicon = read_lexicon(args.lexicon)\n",
        "  numIter = int(args.numiter)\n",
        "  outFileName = args.output\n",
        "  \n",
        "  ''' Enrich the word vectors using ppdb and print the enriched vectors '''\n",
        "  print_word_vecs(retrofit(wordVecs, lexicon, numIter), outFileName) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mPIfaUbpAIh"
      },
      "source": [
        "### Section 3.2: Aligning Word Embeddings\n",
        "\n",
        "We've explored alignment in some detail in tutorial 2.2, on Data Integration and Alignment. Specifically, we looked at alignment across time and across languages -  in this section, we will revisit these examples.\n",
        "\n",
        "\n",
        "NOTE: the following section has previously seen code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyxK538kFO6k"
      },
      "source": [
        "#### Time based data\n",
        "\n",
        "In order to explore this, let's get some data that follows a time trend. We'll look at conference proceedings from the American Society for Clinical Oncologists. You can download the data [here](https://drive.google.com/file/d/1R9EiThdJQ3vY84xcoPJGUBeqvemu_o8L/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhGzbmRKgxjv"
      },
      "source": [
        "ascoDF = pandas.read_csv(\"/content/ASCO_abstracts.csv\", index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnQwwPcfLK-N"
      },
      "source": [
        "ascoDF['tokenized_sents'] = ascoDF['Body'].apply(lambda x: [lucem_illud.word_tokenize(s) for s in lucem_illud.sent_tokenize(x)])\n",
        "ascoDF['normalized_sents'] = ascoDF['tokenized_sents'].apply(lambda x: [lucem_illud.normalizeTokens(s) for s in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH-koGZuFO6j"
      },
      "source": [
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O7D6y_HXDP_"
      },
      "source": [
        "#### Cross Temporal Alignment\n",
        "\n",
        "Below is code that aligns the dimensions of multiple embeddings arrayed over time or some other dimension and allow identification of semantic change as the word vectors change their loadings for focal words. This code comes from the approach piloted at Stanford by William Hamilton, Daniel Jurafsky and Jure Lescovec [here](https://arxiv.org/pdf/1605.09096.pdf). \n",
        "\n",
        "In this case we train the models ourselves instead of using a pre-trained model, so this might take some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8rKAcNkFO6k"
      },
      "source": [
        "def calc_syn0norm(model):\n",
        "    \"\"\"since syn0norm is now depricated\"\"\"\n",
        "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
        "\n",
        "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
        "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
        "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
        "    (With help from William. Thank you!)\n",
        "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
        "    Then do the alignment on the other_embed model.\n",
        "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
        "    Return other_embed.\n",
        "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
        "    \"\"\"\n",
        "    base_embed = copy.copy(base_embed)\n",
        "    other_embed = copy.copy(other_embed)\n",
        "    # make sure vocabulary and indices are aligned\n",
        "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
        "\n",
        "    # get the embedding matrices\n",
        "    base_vecs = calc_syn0norm(in_base_embed)\n",
        "    other_vecs = calc_syn0norm(in_other_embed)\n",
        "\n",
        "    # just a matrix dot product with numpy\n",
        "    m = other_vecs.T.dot(base_vecs) \n",
        "    # SVD method from numpy\n",
        "    u, _, v = np.linalg.svd(m)\n",
        "    # another matrix operation\n",
        "    ortho = u.dot(v) \n",
        "    # Replace original array with modified one\n",
        "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
        "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)\n",
        "    return other_embed\n",
        "    \n",
        "def intersection_align_gensim(m1,m2, words=None):\n",
        "    \"\"\"\n",
        "    Intersect two gensim word2vec models, m1 and m2.\n",
        "    Only the shared vocabulary between them is kept.\n",
        "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
        "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
        "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
        "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
        "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
        "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the vocab for each model\n",
        "    vocab_m1 = set(m1.wv.vocab.keys())\n",
        "    vocab_m2 = set(m2.wv.vocab.keys())\n",
        "\n",
        "    # Find the common vocabulary\n",
        "    common_vocab = vocab_m1&vocab_m2\n",
        "    if words: common_vocab&=set(words)\n",
        "\n",
        "    # If no alignment necessary because vocab is identical...\n",
        "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
        "        return (m1,m2)\n",
        "\n",
        "    # Otherwise sort by frequency (summed for both)\n",
        "    common_vocab = list(common_vocab)\n",
        "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
        "\n",
        "    # Then for each model...\n",
        "    for m in [m1,m2]:\n",
        "        # Replace old syn0norm array with new one (with common vocab)\n",
        "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
        "        old_arr = calc_syn0norm(m)\n",
        "        new_arr = np.array([old_arr[index] for index in indices])\n",
        "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
        "\n",
        "        # Replace old vocab dictionary with new one (with common vocab)\n",
        "        # and old index2word with new one\n",
        "        m.index2word = common_vocab\n",
        "        old_vocab = m.wv.vocab\n",
        "        new_vocab = {}\n",
        "        for new_index,word in enumerate(common_vocab):\n",
        "            old_vocab_obj=old_vocab[word]\n",
        "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
        "        m.wv.vocab = new_vocab\n",
        "\n",
        "    return (m1,m2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgJogQQFO6l"
      },
      "source": [
        "def compareModels(df, category, sort = True):\n",
        "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
        "    embeddings_raw = {}\n",
        "    cats = sorted(set(df[category]))\n",
        "    for cat in cats:\n",
        "        #This can take a while\n",
        "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
        "        subsetDF = df[df[category] == cat]\n",
        "        #You might want to change the W2V parameters\n",
        "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF['normalized_sents'].sum())\n",
        "    #These are much quicker\n",
        "    embeddings_aligned = {}\n",
        "    for catOuter in cats:\n",
        "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
        "        for catInner in cats:\n",
        "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
        "    return embeddings_raw, embeddings_aligned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv6Bl_KBFO6m"
      },
      "source": [
        "rawEmbeddings, comparedEmbeddings = compareModels(ascoDF, 'Year')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_AQHSlFO6m"
      },
      "source": [
        "This is the key to our aligning - the smart procrustes align method here is the one doing the aligning, and we are adding the alligned embeddings to the dictionary. \n",
        "\n",
        "We need to compare them across all permutions so we will define another function to help, we will be using 1 - cosine similarity as that gives a more intitive range of 0-2 with low values meaning little change and high meaning lots of change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nwc6rpFO6m"
      },
      "source": [
        "def getDivergenceDF(word, embeddingsDict):\n",
        "    dists = []\n",
        "    cats = sorted(set(embeddingsDict.keys()))\n",
        "    dists = {}\n",
        "    for cat in cats:\n",
        "        dists[cat] = []\n",
        "        for embed in embeddingsDict[cat][1:]:\n",
        "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(embeddingsDict[cat][0][word].reshape(1, -1),\n",
        "                                                                             embed[word].reshape(1, -1))[0,0]))\n",
        "    return pandas.DataFrame(dists, index = cats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3izzZMkFO6m"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPuazOr1FO6n"
      },
      "source": [
        "import seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcM_q8YkzW1c"
      },
      "source": [
        "We now check certain words and see their movement in time along these abstracts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBvAHcO4FO6n"
      },
      "source": [
        "targetWord = 'breast'\n",
        "\n",
        "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
        "fig, ax = plt.subplots(figsize = (10, 7))\n",
        "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
        "ax.set_xlabel(\"Starting year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw5fTLxvFO6n"
      },
      "source": [
        "targetWord = 'combination'\n",
        "\n",
        "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
        "fig, ax = plt.subplots(figsize = (10, 7))\n",
        "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
        "ax.set_xlabel(\"Starting year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2OynHQwFO6o"
      },
      "source": [
        "def findDiverence(word, embeddingsDict):\n",
        "    cats = sorted(set(embeddingsDict.keys()))\n",
        "    \n",
        "    dists = []\n",
        "    for embed in embeddingsDict[cats[0]][1:]:\n",
        "        dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(embeddingsDict[cats[0]][0][word].reshape(1, -1), embed[word].reshape(1, -1))[0,0])\n",
        "    return sum(dists)\n",
        "\n",
        "def findMostDivergent(embeddingsDict):\n",
        "    words = []\n",
        "    for embeds in embeddingsDict.values():\n",
        "        for embed in embeds:\n",
        "            words += list(embed.wv.vocab.keys())\n",
        "    words = set(words)\n",
        "    print(\"Found {} words to compare\".format(len(words)))\n",
        "    return sorted([(w, findDiverence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJT9ut0vFO6o"
      },
      "source": [
        "wordDivergences = findMostDivergent(comparedEmbeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrBAiQ8lFO6o"
      },
      "source": [
        "wordDivergences[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91tuJ15JFO6o"
      },
      "source": [
        "wordDivergences[-20:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_54K9DrkFO6p"
      },
      "source": [
        "targetWord = wordDivergences[2][0]\n",
        "\n",
        "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
        "fig, ax = plt.subplots(figsize = (10, 7))\n",
        "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
        "ax.set_xlabel(\"Starting year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m4qOzzDFO6p"
      },
      "source": [
        "targetWord = wordDivergences[-1][0]\n",
        "\n",
        "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
        "fig, ax = plt.subplots(figsize = (10, 7))\n",
        "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
        "ax.set_xlabel(\"Starting year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jiy11EjTFO6p"
      },
      "source": [
        "We've seen here from a small, research based corpus how it is still able to capture some notion of semantic drift. The word cell has moved among the most and the word view has moved the least. We recommend trying similar analyses on different datasets to see how words change their meaning over time based on the company they keep. \n",
        "\n",
        "#### Cross language alignment\n",
        "\n",
        "While in the previous example we saw how we can use a time-stamped textual corpus to create embeddings and then compare between them, in the following example we use pre-trained embeddings for different languages and align them using some powerful packages.\n",
        "\n",
        "Here, we use Fast Text word embeddings downloaded from their [documentation website](https://fasttext.cc/), for Spanish and French embeddings. Fast Text does not take the word as a special unit but rather the character window, allowing it to achieve more subtle contextual information. You must download these embeddings and upload them to be able to complete this part of the exercise. Note that these files are large! If it doesn't work, restart your session, clear your uploads and try this section again.\n",
        "\n",
        "We then perform an SVD and orthogonal transformation on the data to rotate and reflect it so that it best aligns. \n",
        "The code is adapted from:\n",
        "https://github.com/babylonhealth/fastText_multilingual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKSVD6dnFO6p"
      },
      "source": [
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkN8fcy9FO6p"
      },
      "source": [
        "from fasttext import FastVector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYGuEeBSFO6q"
      },
      "source": [
        "fr_dictionary = FastVector(vector_file='/content/wiki.fr.vec')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-tGWNCIFO6q"
      },
      "source": [
        "es_dictionary = FastVector(vector_file='/content/wiki.es.vec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQJigDA2FO6q"
      },
      "source": [
        "fr_vector = fr_dictionary[\"chat\"]\n",
        "es_vector = es_dictionary[\"gata\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ori0by-FO6q"
      },
      "source": [
        "print(FastVector.cosine_similarity(fr_vector, es_vector))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwu0CrAMFO6q"
      },
      "source": [
        "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
        "def normalized(a, axis=-1, order=2):\n",
        "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2==0] = 1\n",
        "    return a / np.expand_dims(l2, axis)\n",
        "\n",
        "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
        "    \"\"\"\n",
        "    Source and target dictionaries are the FastVector objects of\n",
        "    source/target languages. bilingual_dictionary is a list of \n",
        "    translation pair tuples [(source_word, target_word), ...].\n",
        "    \"\"\"\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "\n",
        "    for (source, target) in bilingual_dictionary:\n",
        "        if source in source_dictionary and target in target_dictionary:\n",
        "            source_matrix.append(source_dictionary[source])\n",
        "            target_matrix.append(target_dictionary[target])\n",
        "\n",
        "    # return training matrices\n",
        "    return np.array(source_matrix), np.array(target_matrix)\n",
        "\n",
        "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
        "    \"\"\"\n",
        "    Source and target matrices are numpy arrays, shape\n",
        "    (dictionary_length, embedding_dimension). These contain paired\n",
        "    word vectors from the bilingual dictionary.\n",
        "    \"\"\"\n",
        "    # optionally normalize the training vectors\n",
        "    if normalize_vectors:\n",
        "        source_matrix = normalized(source_matrix)\n",
        "        target_matrix = normalized(target_matrix)\n",
        "\n",
        "    # perform the SVD\n",
        "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
        "    U, s, V = np.linalg.svd(product)\n",
        "\n",
        "    # return orthogonal transformation which aligns source language to the target\n",
        "    return np.matmul(U, V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1z--PkRFO6q"
      },
      "source": [
        "To align the two word embeddings, we must try and find some common words with similar meanings. Lets try this by simply going through some common dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVc_f348FO6q"
      },
      "source": [
        "es_words = set(es_dictionary.word2id.keys())\n",
        "fr_words = set(fr_dictionary.word2id.keys())\n",
        "overlap = list(es_words & fr_words)\n",
        "bilingual_dictionary = [(entry, entry) for entry in overlap]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGpJG0IeFO6r"
      },
      "source": [
        "len(bilingual_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsUxstyJFO6r"
      },
      "source": [
        "# form the training matrices\n",
        "source_matrix, target_matrix = make_training_matrices(fr_dictionary, es_dictionary, bilingual_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EWw8c0AFO6r"
      },
      "source": [
        "# learn and apply the transformation\n",
        "transform = learn_transformation(source_matrix, target_matrix)\n",
        "fr_dictionary.apply_transform(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmH-71zNFO6r"
      },
      "source": [
        "fr_vector = fr_dictionary[\"chat\"]\n",
        "es_vector = es_dictionary[\"gata\"]\n",
        "print(FastVector.cosine_similarity(fr_vector, es_vector))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYuK8AUtFO6r"
      },
      "source": [
        "Here, we see that by simply picking up words spelled the same, we have done a decent job in aligning the two spaces - the Spanish word for cat and French word for cat are closer than they were in the previously unaligned space. Try using an actual Spanish - French dictionary next time and see how well the embeddings end up aligning!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PpFNCZDXLMU"
      },
      "source": [
        "#### Deep Canonical Alignment\n",
        "\n",
        "Aligning word embeddings is a popular task: here is some more material on aligning embeddings in different contexts, as well as a more efficient deep canonical alignment.\n",
        "\n",
        "[Deep Canonical Alignment](http://proceedings.mlr.press/v28/andrew13.html)\n",
        "\n",
        "https://github.com/Michaelvll/DeepCCA\n",
        "\n",
        "https://github.com/mfaruqui/crosslingual-cca\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vonWUutOsPfP"
      },
      "source": [
        "# code for deep CCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Ye7o0oQz-H"
      },
      "source": [
        "The code below implements a version of this.\n",
        "\n",
        "The example input files can be downloaded at: [Dropbox link](\n",
        "https://www.dropbox.com/s/nifi5nj1oj0fu2i/data.zip?dl=0)\n",
        "\n",
        "The input is the word_pair_PMIs, which is stored in csvs (ignore the mat files); you need to put them all in a path and load everything from there (change the 'training head' variable). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a_Ln4VDOz08"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Nov 10 13:10:42 2016\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# main script for time CD\n",
        "# trainfile has lines of the form\n",
        "# tok1,tok2,pmi\n",
        "\n",
        "import numpy as np\n",
        "import util_timeCD as util\n",
        "import pickle as pickle\n",
        "\n",
        "# PARAMETERS\n",
        "\n",
        "nw = 20936 # number of words in vocab (11068100/20936 for ngram/nyt)\n",
        "T = range(1990,2016) # total number of time points (20/range(27) for ngram/nyt)\n",
        "cuda = True\n",
        "\n",
        "trainhead = 'data/wordPairPMI_' # location of training data\n",
        "savehead = 'results/'\n",
        "\n",
        "def print_params(r,lam,tau,gam,emph,ITERS):\n",
        "\n",
        "    print('rank = {}'.format(r))\n",
        "    print('frob  regularizer = {}'.format(lam))\n",
        "    print('time  regularizer = {}'.format(tau))\n",
        "    print('symmetry regularizer = {}'.format(gam))\n",
        "    print('emphasize param   = {}'.format(emph))\n",
        "    print('total iterations = {}'.format(ITERS))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    import sys\n",
        "    ITERS = 5 # total passes over the data\n",
        "    lam = 10 #frob regularizer\n",
        "    gam = 100 # forcing regularizer\n",
        "    tau = 50  # smoothing regularizer\n",
        "    r   = 50  # rank\n",
        "    b = nw # batch size\n",
        "    emph = 1 # emphasize the nonzero\n",
        "\n",
        "    foo = sys.argv\n",
        "    for i in range(1,len(foo)):\n",
        "        if foo[i]=='-r':    r = int(float(foo[i+1]))\n",
        "        if foo[i]=='-iters': ITERS = int(float(foo[i+1]))\n",
        "        if foo[i]=='-lam':    lam = float(foo[i+1])\n",
        "        if foo[i]=='-tau':    tau = float(foo[i+1])\n",
        "        if foo[i]=='-gam':    gam = float(foo[i+1])\n",
        "        if foo[i]=='-b':    b = int(float(foo[i+1]))\n",
        "        if foo[i]=='-emph': emph = float(foo[i+1])\n",
        "        if foo[i]=='-check': erchk=foo[i+1]\n",
        "\n",
        "\n",
        "    savefile = savehead+'L'+str(lam)+'T'+str(tau)+'G'+str(gam)+'A'+str(emph)\n",
        "\n",
        "    print('starting training with following parameters')\n",
        "    print_params(r,lam,tau,gam,emph,ITERS)\n",
        "    print('there are a total of {} words, and {} time points'.format(nw,T))\n",
        "\n",
        "    print('X*X*X*X*X*X*X*X*X')\n",
        "    print('initializing')\n",
        "\n",
        "    #Ulist,Vlist = util.initvars(nw,T,r, trainhead)\n",
        "    Ulist,Vlist = util.import_static_init(T)\n",
        "    print Ulist\n",
        "    print Vlist\n",
        "    asdf\n",
        "    print('getting batch indices')\n",
        "    if b < nw:\n",
        "        b_ind = util.getbatches(nw,b)\n",
        "    else:\n",
        "        b_ind = [range(nw)]\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    # sequential updates\n",
        "    for iteration in xrange(ITERS):\n",
        "        print_params(r,lam,tau,gam,emph,ITERS)\n",
        "        try:\n",
        "            Ulist = pickle.load(open( \"%sngU_iter%d.p\" % (savefile,iteration), \"rb\" ) )\n",
        "            Vlist = pickle.load(open( \"%sngV_iter%d.p\" % (savefile, iteration), \"rb\" ) )\n",
        "            print 'iteration %d loaded succesfully' % iteration\n",
        "            continue\n",
        "        except(IOError):\n",
        "            pass\n",
        "        loss = 0\n",
        "        # shuffle times\n",
        "        if iteration == 0: times = T\n",
        "        else: times = np.random.permutation(T)\n",
        "\n",
        "        for t in xrange(len(times)):   # select a time\n",
        "            print 'iteration %d, time %d' % (iteration, t)\n",
        "            f = trainhead + str(t) + '.csv'\n",
        "            print f\n",
        "\n",
        "            \"\"\"\n",
        "            try:\n",
        "                Ulist = pickle.load( open( \"%sngU_iter%d_time%d_tmp.p\" % (savefile,iteration,t), \"rb\" ) )\n",
        "                Vlist = pickle.load( open( \"%sngV_iter%d_time%d_tmp.p\" % (savefile, iteration,t), \"rb\" ) )\n",
        "                times = pickle.load( open( \"%sngtimes_iter%d_time%d_tmp.p\" % (savefile, iteration,t), \"rb\" ) )\n",
        "                print 'iteration %d time %d loaded succesfully' % (iteration, t)\n",
        "                continue\n",
        "            except(IOError):\n",
        "                pass\n",
        "            \"\"\"\n",
        "\n",
        "            pmi = util.getmat(f,nw,False)\n",
        "            for j in xrange(len(b_ind)): # select a mini batch\n",
        "                print '%d out of %d' % (j,len(b_ind))\n",
        "                ind = b_ind[j]\n",
        "                ## UPDATE V\n",
        "                # get data\n",
        "                pmi_seg = pmi[:,ind].todense()\n",
        "\n",
        "                if t==0:\n",
        "                    vp = np.zeros((len(ind),r))\n",
        "                    up = np.zeros((len(ind),r))\n",
        "                    iflag = True\n",
        "                else:\n",
        "                    vp = Vlist[t-1][ind,:]\n",
        "                    up = Ulist[t-1][ind,:]\n",
        "                    iflag = False\n",
        "\n",
        "                if t==len(T)-1:\n",
        "                    vn = np.zeros((len(ind),r))\n",
        "                    un = np.zeros((len(ind),r))\n",
        "                    iflag = True\n",
        "                else:\n",
        "                    vn = Vlist[t+1][ind,:]\n",
        "                    un = Ulist[t+1][ind,:]\n",
        "                    iflag = False\n",
        "                Vlist[t][ind,:] = util.update(Ulist[t],emph*pmi_seg,vp,vn,lam,tau,gam,ind,iflag)\n",
        "                Ulist[t][ind,:] = util.update(Vlist[t],emph*pmi_seg,up,un,lam,tau,gam,ind,iflag)\n",
        "\n",
        "\n",
        "            #pickle.dump(Ulist, open( \"%sngU_iter%d_time%d_tmp.p\" % (savefile,iteration,t), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n",
        "            #pickle.dump(Vlist, open( \"%sngV_iter%d_time%d_tmp.p\" % (savefile, iteration,t), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n",
        "            #pickle.dump(times, open( \"%sngtimes_iter%d_time%d_tmp.p\" % (savefile, iteration,t), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "            ####  INNER BATCH LOOP END\n",
        "\n",
        "        # save\n",
        "        print 'time elapsed = ', time.time()-start_time\n",
        "\n",
        "\n",
        "        pickle.dump(Ulist, open( \"%sngU_iter%d.p\" % (savefile,iteration), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n",
        "        pickle.dump(Vlist, open( \"%sngV_iter%d.p\" % (savefile, iteration), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhhcvMnAO28m"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Nov 10 10:11:23 2016\n",
        "\n",
        "@author: raon\n",
        "\"\"\"\n",
        "\n",
        "#utility functions for running the CD method\n",
        "#loss: min 1/2 \\sum_t | Yt - UtVt' |^2 + lam/2 \\sum_t(|Ut|^2 + |Vt|^2) +\n",
        "#                                        tau/2 \\sum_t>1(|Vt - Vt-1|^2 + |Ut - Ut-1|^2)\n",
        "#                                        gam/2 \\sum_t (|Ut - Vt|^2)\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import copy\n",
        "def update(U,Y,Vm1,Vp1,lam,tau,gam,ind,iflag):\n",
        "\n",
        "    UtU = np.dot(U.T,U) # rxr\n",
        "    r = UtU.shape[0]\n",
        "    if iflag:   M   = UtU + (lam + 2*tau + gam)*np.eye(r)\n",
        "    else:       M   = UtU + (lam + tau + gam)*np.eye(r)\n",
        "\n",
        "    Uty = np.dot(U.T,Y) # rxb\n",
        "    Ub  = U[ind,:].T   # rxb\n",
        "    A   = Uty + gam*Ub + tau*(Vm1.T+Vp1.T)  # rxb\n",
        "    Vhat = np.linalg.lstsq(M,A) #rxb\n",
        "    return Vhat[0].T #bxr\n",
        "\n",
        "\n",
        "#for the above function, the equations are to update V. So:\n",
        "#Y is n X b (b = batch size)\n",
        "#r = rank\n",
        "#U is n X r\n",
        "#Vm1 and Vp1 are bXr. so they are b rows of V, transposed\n",
        "\n",
        "def import_static_init(T):\n",
        "    emb = sio.loadmat('data/emb_static.mat')['emb']\n",
        "    U = [copy.deepcopy(emb) for t in T]\n",
        "    V = [copy.deepcopy(emb) for t in T]\n",
        "    return U,V\n",
        "def initvars(vocab_size,T,rank):\n",
        "    # dictionary will store the variables U and V. tuple (t,i) indexes time t and word index i\n",
        "\n",
        "    U,V = [],[]\n",
        "    U.append(np.random.randn(vocab_size,rank)/np.sqrt(rank))\n",
        "    V.append(np.random.randn(vocab_size,rank)/np.sqrt(rank))\n",
        "    for t in xrange(1,T):\n",
        "        U.append(U[0].copy())\n",
        "        V.append(V[0].copy())\n",
        "        print t\n",
        "    return U,V\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.sparse as ss\n",
        "def getmat(f,v,rowflag):\n",
        "    data = pd.read_csv(f)\n",
        "    data = data.as_matrix()\n",
        "\n",
        "    X = ss.coo_matrix((data[:,2],(data[:,0],data[:,1])),shape=(v,v))\n",
        "\n",
        "\n",
        "    if rowflag:\n",
        "        X = ss.csr_matrix(X)\n",
        "        #X = X[inds,:]\n",
        "    else:\n",
        "        X = ss.csc_matrix(X)\n",
        "        #X = X[:,inds]\n",
        "\n",
        "    return X#.todense()\n",
        "\n",
        "def getbatches(vocab,b):\n",
        "    batchinds = []\n",
        "    current = 0\n",
        "    while current<vocab:\n",
        "        inds = range(current,min(current+b,vocab))\n",
        "        current = min(current+b,vocab)\n",
        "        batchinds.append(inds)\n",
        "    return batchinds\n",
        "\n",
        "#   THE FOLLOWING FUNCTION TAKES A WORD ID AND RETURNS CLOSEST WORDS BY COSINE DISTANCE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def getclosest(wid,U):\n",
        "    C = []\n",
        "    for t in range(len(U)):\n",
        "        temp = U[t]\n",
        "        K = cosine_similarity(temp[wid,:],temp)\n",
        "        mxinds = np.argsort(-K)\n",
        "        mxinds = mxinds[0:10]\n",
        "        C.append(mxinds)\n",
        "    return C\n",
        "\n",
        "# THE FOLLOWING FUNCTIONS COMPUTES THE REGULARIZER SCORES GIVEN U AND V ENTRIES\n",
        "def compute_symscore(U,V):\n",
        "    return np.linalg.norm(U-V)**2\n",
        "\n",
        "def compute_smoothscore(U,Um1,Up1):\n",
        "    X = np.linalg.norm(U-Up1)**2 + np.linalg.norm(U-Um1)**2\n",
        "    return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e_V1tGDpJNX"
      },
      "source": [
        "## Section 3.3: Adding more context - FastText and GloVE\n",
        "\n",
        "Since the original word2vec paper, there have followed a slew of word embedding related methods which innovate and build on them in many ways. One popular extension is FastText (Bojanowski et al. 2017), which uses sub-words to generate its vectors. Using subwords means that it is powerful in dealing with unknown words and sparse languages that otherwise have a rich morphological structure. These sub-words are incorporated into the previously skipgram and CBOW methods. For example, if the word is “which”, it is represented as the word itself along with a bag of constituent n-grams. If n=3, the representation looks like <wh, whi, hic, ich, ch>, and we learn a representation for each of these constituents, with the word “which” taking on the average value of these constituents. \n",
        "\n",
        "FastText can be used either via Gensim or the official package, and primarily has two functions - word representations, and text classification (Joulin et al. 2017). See below for code using the FastText package for playing with word representations, taken from their word representations tutorial (https://fasttext.cc/docs/en/unsupervised-tutorial.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hXjY0FPpNVq"
      },
      "source": [
        "! pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqNJ5jWgpSCB"
      },
      "source": [
        "### Representations with FastText\n",
        "\n",
        "We will follow the instructions in the FastText tutorial to prepare our data, (Wikipedia). You can either follow the instructions on the page for setting the data, or download/copy it to drive it from this [google drive link](https://drive.google.com/file/d/12T3nNzf0a7tdhm1lVyfz9Ix9XVITFMCP/view?usp=sharing). \n",
        "\n",
        "In this example, we will be training a model - it is also possible to download and use the many pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4iWKbb8s8Na"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_1B3ckRpUP1"
      },
      "source": [
        "import fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t37MtAB4JYUb"
      },
      "source": [
        "file_address = \"/content/drive/MyDrive/fil9\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7YqmoQEI360"
      },
      "source": [
        "model = fasttext.train_unsupervised(file_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVdsL1O-t171"
      },
      "source": [
        "While FastText is running, the progress and estimated time to completion is shown on your screen. Once the training finishes, the model variable contains information on the trained model, which you can use for querying:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2NARfwxI6FL"
      },
      "source": [
        "model.words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7y1Qr95t7if"
      },
      "source": [
        "It returns all words in the vocabulary, sorted by decreasing frequency. We can get the word vector by:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lxZJCAst4n1"
      },
      "source": [
        "model.get_word_vector(\"the\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXjgMWFCt4lE"
      },
      "source": [
        "model.save_model(\"result/fil9.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CznMfOVbt4iG"
      },
      "source": [
        "# when we want to use the model again\n",
        "# model = fasttext.load_model(\"result/fil9.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3H_c9lVq0WQ"
      },
      "source": [
        "Let's now print some vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsAsfHyNt4e4"
      },
      "source": [
        "[model.get_word_vector(x) for x in [\"asparagus\", \"pidgey\", \"yellow\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAXTmBV1q_Xv"
      },
      "source": [
        "A nice feature is that you can also query for words that did not appear in your data! Indeed words are represented by the sum of their substrings. As long as the unknown word is made of known substrings, there is a representation of it!\n",
        "\n",
        "As an example let's try with a misspelled word:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmWu9IZsrCl-"
      },
      "source": [
        "model.get_word_vector(\"enviroment\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP0QFihrsrGS"
      },
      "source": [
        "You still get a word vector for it! But how good it is? Let's find out in the next sections!\n",
        "\n",
        "#### Nearest neighbor queries\n",
        "A simple way to check the quality of a word vector is to look at its nearest neighbors. This gives an intuition of the type of semantic information the vectors are able to capture.\n",
        "\n",
        "This can be achieved with the nearest neighbor (nn) functionality. For example, we can query the 10 nearest neighbors of a word by running the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipxJNvy3sngq"
      },
      "source": [
        "model.get_nearest_neighbors('asparagus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xELqqvJvsxnC"
      },
      "source": [
        "Nice! It seems that vegetable vectors are similar. Note that the nearest neighbor is the word asparagus itself, this means that this word appeared in the dataset. What about Pokemons?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0X6mV5psw53"
      },
      "source": [
        "model.get_nearest_neighbors('pidgey')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4_uK0W1tquU"
      },
      "source": [
        "Different evolution of the same Pokemon have close-by vectors! But what about our misspelled word; is its vector close to anything reasonable? Let's find out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2uhn-2Uttk0"
      },
      "source": [
        "model.get_nearest_neighbors('enviroment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuoVYFKBtvU6"
      },
      "source": [
        "Thanks to the information contained in the word, the vector of our misspelled word matches reasonable words! It is not perfect but the main information has been captured.\n",
        "\n",
        "#### Measure of similarity\n",
        "\n",
        "In order to find nearest neighbors, we need to compute a similarity score between words. Our words are represented by continuous word vectors and we can thus apply simple similarities to them. In particular we use the cosine of the angles between two vectors. This similarity is computed for all words in the vocabulary, and the 10 most similar words are shown. Of course, if the word appears in the vocabulary, it will appear on top, with a similarity of 1.\n",
        "\n",
        "Word analogies\n",
        "In a similar spirit, one can play around with word analogies. For example, we can see if our model can guess what is to France as Berlin is to Germany.\n",
        "\n",
        "This can be done with the analogies functionality. It takes a word triplet (like Germany Berlin France) and outputs the analogy:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7f-t9qbt3eD"
      },
      "source": [
        "model.get_analogies(\"berlin\", \"germany\", \"france\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqf2grslt7on"
      },
      "source": [
        "The answer provided by our model is Paris, which is correct. Let's have a look at a less obvious example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oskukUFvt8Vx"
      },
      "source": [
        "model.get_analogies(\"psx\", \"sony\", \"nintendo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WkoBS0LuAK9"
      },
      "source": [
        "Our model considers that the nintendo analogy of a psx is the gamecube, which seems reasonable. Of course the quality of the analogies depend on the dataset used to train the model; one can only hope to cover domains well-represented in the dataset.\n",
        "\n",
        "#### Importance of character n-grams\n",
        "Using subword-level information is particularly interesting to build vectors for unknown words. For example, the word gearshift does not exist on Wikipedia but we can still query its closest existing words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhHVyXJkt8S7"
      },
      "source": [
        "model.get_nearest_neighbors('gearshift')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0S7iaX3uGMh"
      },
      "source": [
        "Most of the retrieved words share substantial substrings but a few are actually quite different, like cogwheel. You can try other words like sunbathe or grandnieces.\n",
        "\n",
        "Now that we have seen the interest of subword information for unknown words, let's check how it compares to a model that does not use subword information. To train a model without subwords, just run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27llBMXxuMAf"
      },
      "source": [
        "model_without_subwords = fasttext.train_unsupervised('data/fil9', maxn=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUrOrMO8uOdi"
      },
      "source": [
        "To illustrate the difference, let us take an uncommon word in Wikipedia, like accomodation which is a misspelling of accommodation. Here is the nearest neighbors obtained without subwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwCUYmLGuPI4"
      },
      "source": [
        "model_without_subwords.get_nearest_neighbors('accomodation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHxCQ-lDuV8r"
      },
      "source": [
        "The result does not make much sense, most of these words are unrelated. On the other hand, using subword information gives the following list of nearest neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYKitqVNuWoK"
      },
      "source": [
        "model.get_nearest_neighbors('accomodation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ECoxlc0uY0Y"
      },
      "source": [
        "The nearest neighbors captures different variations around the word accommodation. We also get semantically related words such as amenities or catering.\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "In this section, we show how to obtain word vectors from Wikipedia. This can be done for any language and we provide [pre-trained models](https://fasttext.cc/docs/en/pretrained-vectors.html) with the default setting for 294 of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30R9lGTapUt3"
      },
      "source": [
        "### FastText Aligned Embeddings and Vectors\n",
        "\n",
        "The FastText website has an incredible resource, outside of the rich documentation and examples - pre-trained word vectors for 157 different languages! This is great because it allows for comparisons between languages, and an opportunity for us to search for structural similarities across languages. One of the ways we can do this is by aligning embeddings, similar to what we did for the diachronic temporal embeddings. We already saw code for aligning FastText embeddings using an unsupervised method (dictionary), and in the code below we will exploring embeddings that are already aligned. You can find code to perform the alignment in an [unsupervised](https://github.com/facebookresearch/fastText/blob/master/alignment/unsup_align.py) way, as well as [supervised](https://github.com/facebookresearch/fastText/blob/master/alignment/align.py). (Joulin et al, 2018)\n",
        "\n",
        "Remember to first download the two files!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsNBxEe5zYCz"
      },
      "source": [
        "#### Loading word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "X9PdNfSkzYC1"
      },
      "source": [
        "import io\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "nUGL8BlxzYC1"
      },
      "source": [
        "def load_vec(emb_path, nmax=50000):\n",
        "    vectors = []\n",
        "    word2id = {}\n",
        "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
        "        next(f)\n",
        "        for i, line in enumerate(f):\n",
        "            word, vect = line.rstrip().split(' ', 1)\n",
        "            vect = np.fromstring(vect, sep=' ')\n",
        "            assert word not in word2id, 'word found twice'\n",
        "            vectors.append(vect)\n",
        "            word2id[word] = len(word2id)\n",
        "            if len(word2id) == nmax:\n",
        "                break\n",
        "    id2word = {v: k for k, v in word2id.items()}\n",
        "    embeddings = np.vstack(vectors)\n",
        "    return embeddings, id2word, word2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "pafAnr3wzYC2"
      },
      "source": [
        "src_path = '/content/vectors-en.txt'\n",
        "tgt_path = '/content/vectors-es.txt'\n",
        "nmax = 50000  # maximum number of word embeddings to load\n",
        "\n",
        "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
        "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI0xy9E8zYC3"
      },
      "source": [
        "#### Get nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "2NKhbl4NzYC3"
      },
      "source": [
        "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
        "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
        "    word2id = {v: k for k, v in src_id2word.items()}\n",
        "    word_emb = src_emb[word2id[word]]\n",
        "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
        "    k_best = scores.argsort()[-K:][::-1]\n",
        "    for i, idx in enumerate(k_best):\n",
        "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A81G_GDTzYC4"
      },
      "source": [
        "# printing nearest neighbors in the source space\n",
        "src_word = 'cat'\n",
        "get_nn(src_word, src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT6dOnVTzYC4"
      },
      "source": [
        "# printing nearest neighbors in the target space\n",
        "src_word = 'cat'\n",
        "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUqaNE8zYC5"
      },
      "source": [
        "#### Visualize multilingual embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NmwjcH2zYC5"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, whiten=True)  # TSNE(n_components=2, n_iter=3000, verbose=2)\n",
        "pca.fit(np.vstack([src_embeddings, tgt_embeddings]))\n",
        "print('Variance explained: %.2f' % pca.explained_variance_ratio_.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "s1DvaSUEzYC6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_similar_word(src_words, src_word2id, src_emb, tgt_words, tgt_word2id, tgt_emb, pca):\n",
        "\n",
        "    Y = []\n",
        "    word_labels = []\n",
        "    for sw in src_words:\n",
        "        Y.append(src_emb[src_word2id[sw]])\n",
        "        word_labels.append(sw)\n",
        "    for tw in tgt_words:\n",
        "        Y.append(tgt_emb[tgt_word2id[tw]])\n",
        "        word_labels.append(tw)\n",
        "\n",
        "    # find tsne coords for 2 dimensions\n",
        "    Y = pca.transform(Y)\n",
        "    x_coords = Y[:, 0]\n",
        "    y_coords = Y[:, 1]\n",
        "\n",
        "    # display scatter plot\n",
        "    plt.figure(figsize=(10, 8), dpi=80)\n",
        "    plt.scatter(x_coords, y_coords, marker='x')\n",
        "\n",
        "    for k, (label, x, y) in enumerate(zip(word_labels, x_coords, y_coords)):\n",
        "        color = 'blue' if k < len(src_words) else 'red'  # src words in blue / tgt words in red\n",
        "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points', fontsize=19,\n",
        "                     color=color, weight='bold')\n",
        "\n",
        "    plt.xlim(x_coords.min() - 0.2, x_coords.max() + 0.2)\n",
        "    plt.ylim(y_coords.min() - 0.2, y_coords.max() + 0.2)\n",
        "    plt.title('Visualization of the multilingual word embedding space')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKAtRx5ozYC6"
      },
      "source": [
        "# get 5 random input words\n",
        "src_words = ['university', 'love', 'history', 'tennis', 'research', 'conference']\n",
        "tgt_words = ['universidad', 'amor', 'historia', u'tenis',  u'investigación', 'conferencia']\n",
        "\n",
        "# assert words in dictionaries\n",
        "for sw in src_words:\n",
        "    assert sw in src_word2id, '\"%s\" not in source dictionary' % sw\n",
        "for tw in tgt_words:\n",
        "    assert tw in tgt_word2id, '\"%s\" not in target dictionary' % sw\n",
        "\n",
        "plot_similar_word(src_words, src_word2id, src_embeddings, tgt_words, tgt_word2id, tgt_embeddings, pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4p1DgypCXjO"
      },
      "source": [
        "### GloVE\n",
        "\n",
        "While word2vec and FastText focus on the local context of words, there are also methods which also attend to global corpus statistics. The most popular embedding model from this family is GloVe (Pennington et al., 2014), which gets its name from its approach to the embeddings - Global Vectors. It also includes information of ratios of probabilities from the word-word co-occurence matrix, which make it powerful for measuring semantic similarity, while still being able to capture the linear sub-structure (which we construct our dimensions with, and do our algebraic operations on). Below is code for loading GloVe vectors using gensim, and straight into a dictionary. \n",
        "\n",
        "You can download pre-trained GloVe vectors here: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t2r-z2jCagu"
      },
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiLzIrQSk3GX"
      },
      "source": [
        "glove_file = datapath('/content/glove.6B.100d.txt')\n",
        "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQpq8gM_k5uY"
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTA0G2xgk72o"
      },
      "source": [
        "model.most_similar('obama')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-WpO3RIk9aK"
      },
      "source": [
        "model.most_similar('banana')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_75zfoHk-ab"
      },
      "source": [
        "model.most_similar(negative='banana')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmlqKQbkk-A1"
      },
      "source": [
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gpjVrpTlB0W"
      },
      "source": [
        "def analogy(x1, x2, y1):\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
        "    return result[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGt5kNHglBvs"
      },
      "source": [
        "analogy('japan', 'japanese', 'australia')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW8BxHswlF7U"
      },
      "source": [
        "analogy('australia', 'beer', 'france')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6BIqpq4lFzI"
      },
      "source": [
        "analogy('obama', 'clinton', 'reagan')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7shr8-1lFtn"
      },
      "source": [
        "analogy('tall', 'tallest', 'long')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK7taFUtlJt-"
      },
      "source": [
        "analogy('good', 'fantastic', 'bad')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avhxq-5BlJrJ"
      },
      "source": [
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agpXoQy3lL9L"
      },
      "source": [
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
        "        else:\n",
        "            words = [ word for word in model.vocab ]\n",
        "        \n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVYUe6F-lN9r"
      },
      "source": [
        "display_pca_scatterplot(model, \n",
        "                        ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
        "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
        "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
        "                         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
        "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
        "                         'school', 'college', 'university', 'institute'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_olJWqJlOCq"
      },
      "source": [
        "display_pca_scatterplot(model, sample=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o72iuoypppbq"
      },
      "source": [
        "### svd2vec\n",
        "\n",
        "GloVe attempts to use the best of both local and global information (although we note that its performance is comparable to word2vec, which only accounts for local information). Count-based information is built on older methods such as PMI, which stands for Pointwise Mutual Information (Fano 1961) - these measures are built by trying to identify the chances of two events occuring at the same time, and can be refashioned to think about target and context words. There has been further work showing that dense embeddings such as word2vec have a strong relationship with matrix factorisation like processes that use PMI, where word2vec can be seen as implicitly optimizing a shifted version of a PMI matrix (Levy and Goldberg 2015). This work has been taken forward in Levy et al 2015, where they use PMI and SVD to generate vectors comparable to those from word2vec. Below is code for using such methods with the package svd2vec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RceGetroulIh"
      },
      "source": [
        "## Section 3.4: Classification and topic extraction with word embeddings\n",
        "\n",
        "In this section we will explore the use of word embeddings for classification and topic extraction, two popular natural language processing tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqItv98iu7h0"
      },
      "source": [
        "### Inversion of Distributed Representations\n",
        "\n",
        "But this isn’t the only way we can use word and document embeddings. Word embeddings are essentially distributed language models, and such a model can be inverted using bayesian techniques. What this means is that given a document, we can calculate the probability it was generated by a distributed model - by creating multiple word embedding models for each class, we are able to easily whip up a distribution of probabilities for a document belonging to each of these classes. This method of classification by inversion of distributed representations was first introduced by (Taddy 2015), and performed better than a doc2vec based classification on the dataset of Yelp reviews we just used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09moGhQPSGYl"
      },
      "source": [
        "#### The Score Function\n",
        "\n",
        "The score function is a simple calculation developed by [Matt Taddy](https://arxiv.org/pdf/1504.07295.pdf) to calculate the likelihood that a given text would have been generated by a word-embedding model by summing the inner product between each pair of the text's word vectors. This relies on a few data files that are not in the git repo due to their size please download and unzip [this](https://github.com/Computational-Content-Analysis-2018/Upcoming/raw/master/data/supplement.zip) (472MB) file in the data directory.\n",
        "\n",
        "Here, we explore this using a model trained with millions of resumes from the CareerBuilder website (we can't share the private resumes...but we can share a model built with them :-):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "-icQiO70SGYm"
      },
      "source": [
        "resume_model  = gensim.models.word2vec.Word2Vec.load('../data/resumeAll.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg9xeNv3SGYm"
      },
      "source": [
        "We can examine the vacabularies of this model by building a word-index map:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xGwKVj_ZSGYm"
      },
      "source": [
        "vocab = resume_model.wv.index2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHRyacxmSGYm"
      },
      "source": [
        "Let's just load the sample and take a look at it. The sentences in each job description are already tokenized and normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2T3WZf_SGYn"
      },
      "source": [
        "sampleDF = pandas.read_csv('../data/SampleJobAds.csv', index_col = False)\n",
        "#We need to convert the last couple columns from strings to lists\n",
        "sampleDF['tokenized_sents'] = sampleDF['tokenized_sents'].apply(lambda x: eval(x))\n",
        "sampleDF['normalized_sents'] = sampleDF['normalized_sents'].apply(lambda x: eval(x))\n",
        "sampleDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggd2DziJSGYn"
      },
      "source": [
        "Let's define a function to calculate the likelihood of each job description. The idea is borrowed from [Matt Taddy](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb), who shows how a document can be characterized as the inner product of the distance between its words. In other words, this analysis will show which job ads are most likely to find an appropriate pool of workers in the resume bank that generated our word embedding.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9pz9_7gKSGYn"
      },
      "source": [
        "def adprob(ad, model):\n",
        "    sen_scores = model.score(ad, len(ad))\n",
        "    ad_score = sen_scores.mean()\n",
        "    return ad_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpv16DqBSGYn"
      },
      "source": [
        "Let's apply this function to every job description."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uV-ckcBISGYo"
      },
      "source": [
        "sampleDF['likelihood'] = sampleDF['normalized_sents'].apply(lambda x: adprob(x, resume_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLcm0jMhSGYo"
      },
      "source": [
        "Let's take a look at the top 5 job descriptions that have the highest likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7zq8GmJSGYo"
      },
      "source": [
        "for ad in sampleDF.sort_values(by = 'likelihood', ascending = False)['jobDescription'][:5]:\n",
        "    print (ad + '\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxXzv0e1SGYo"
      },
      "source": [
        "Let's take a look at the bottom 5 job descriptions that have the lowest likelihood to be matched by the resumes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBqp_8_RSGYo"
      },
      "source": [
        "for ad in sampleDF.sort_values(by = 'likelihood')['jobDescription'][:5]:\n",
        "    print (ad + '\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfjrkDwSGYp"
      },
      "source": [
        "We can do the same for phrases corresponding to job skills."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUgym9NySGYp"
      },
      "source": [
        "adprob([[\"python\", \"programming\"]], resume_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQnOSDlhSGYp"
      },
      "source": [
        "adprob([[\"julia\", \"programming\"]], resume_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfLzZX7RSGYp"
      },
      "source": [
        "Basic programming appears to be more likely in this pool of resumes than python programming. \n",
        "\n",
        "We can also do some simple statistics. Unfortunately, we don't have a large sample here. Nevertheless, let's first look at the mean likelihood score of each hiring organization. Some organizations will do well to hire on CareerBuilder...while others will not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY_o8NJ6SGYq"
      },
      "source": [
        "sampleDF.groupby(\"hiringOrganization_organizationName\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4HVVcMXSGYq"
      },
      "source": [
        "We can also look at the mean likelihood of each state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6um9dOQSGYq"
      },
      "source": [
        "sampleDF.groupby(\"jobLocation_address_region\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c92VC8DYvPY1"
      },
      "source": [
        "### FastText Classification\n",
        "\n",
        "Earlier in the homework we explored the word embedding method FastText, which is built using character n-grams or subwords. FastText can be very powerful for text classification, especially when we are dealing with many classes, partly due to its using Hierarchical Softmax for determining the probabilities a document belongs to a class, after averaging FastText embeddings of the words in a sentence or document. This is because a hierarchical algorithm follows a tree-like approach to determining the probabilities of the class, allowing for much faster computational times (hence, the fast part of the text). This allows for efficient text classification (performs as well as pure deep learning approaches) while being much faster. The code below, taken from the official documentation for FastText classification, demonstrates how to use the pre-trained vectors for the task.\n",
        "\n",
        "FastText classification: https://fasttext.cc/docs/en/supervised-tutorial.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEKt2jPwvTiO"
      },
      "source": [
        "# code for FastText classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeCZlXXfvUPs"
      },
      "source": [
        "### Discourse Atoms\n",
        "\n",
        "Another method also focuses on treating word embedding models as a generative process are Discourse Atoms, which are very similar to topic models. First described by Princeton NLP researchers ((Arora et al. 2016), (Arora et al. 2018)), the approach uses an idea of gist, or discourse as the process which generates text. These discourse atoms, similar to topic models, are defined as probabilities over words, and make it possible to measure similarities between the discourse vector and individual word vectors. While we won’t delve into the mathematical details of the generative process (a random walk over semantic space), what we do need to know is the steps to generate these discourse atoms for us. This is described well in one of the first social scientific papers to use these atoms for descriptive research (Arseniev-Koehler et al. 2020), and we adapt code used by them to create the atoms. The first major step is training word embeddings on the corpus in question.\n",
        "\n",
        "#### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBHMqKnk_JkC"
      },
      "source": [
        "# from gensim.test.utils import datapath \n",
        "# import re\n",
        "# import string, re\n",
        "# import cython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rhM3yfuALJQ"
      },
      "source": [
        "!pip install ksvd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5y1spof_MCs"
      },
      "source": [
        "from gensim import corpora, models, similarities #calc all similarities at once, from http://radimrehurek.com/gensim/tut3.html\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import KeyedVectors\n",
        "from random import seed, sample\n",
        "from ksvd import ApproximateKSVD #pip or conda install ksvd #this is key!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zietHqDPF5uT"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFYA73V4GH2Y"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgH-NiNgvZMz"
      },
      "source": [
        "nytimes_w2v_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjAyx16xvccJ"
      },
      "source": [
        "#### k-SVD\n",
        "\n",
        "We then perform a K-SVD on the word embedding matrix to learn topics in such a way where each word-vector is represented as a spare linear combination of topics. To generate a good representation of the original word vector space, we want to minimize the difference between our word vectors and the vectors generated as a linear combination of topics. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11xyYrXOvgJw"
      },
      "source": [
        "#### TRAIN MODEL:\n",
        "\n",
        "#n_comp: Number of topics (i.e., atoms, or dictionary elements)\n",
        "#n_nonzeros: Number of nonzero coefficients to target (how many atoms each word can load onto)\n",
        "            \n",
        "def do_aksvd(w2vmodel, n_comp, n_nonzeros, save=False, savelocation='/content/aksvd_models/'): \n",
        "    #https://github.com/nel215/ksvd #takes about 2 min on Alina's laptop for 30 atoms \n",
        "    aksvd_t = ApproximateKSVD(n_components=n_comp, transform_n_nonzero_coefs=n_nonzeros) #also may adjuste n iter which is default at 10, and tolerance for error which is default at  tol=1e-6 #n_components is number of discourse atoms, since vocab size is smallish, keep this fewer. transform_n is the number of atoms (components) that a word can be a linear combo of\n",
        "    dictionary_t = aksvd_t.fit(w2vmodel.wv.vectors).components_ # Dictionary is the matrix of discourse atoms. \n",
        "    alpha_t = aksvd_t.transform(w2vmodel.wv.vectors) #get the alphas, which are the \"weights\" of each word on a discourse atoms\n",
        "\n",
        "    if save==True:\n",
        "        outfile = open(str(savelocation) + '200d_' + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_aksvd_nvdrsdf20','wb')\n",
        "        pickle.dump(aksvd_t,outfile)\n",
        "        outfile.close()\n",
        "        \n",
        "        outfile = open(str(savelocation) + '200d_' +str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_dictionary_nvdrsdf20','wb')\n",
        "        pickle.dump(dictionary_t,outfile)\n",
        "        outfile.close()\n",
        "        \n",
        "        outfile = open(str(savelocation) + '200d_' + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_alpha_nvdrsdf20','wb')\n",
        "        pickle.dump(alpha_t,outfile)\n",
        "        outfile.close()\n",
        "    return(dictionary_t, alpha_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEX-yfOQEDKH"
      },
      "source": [
        "Two quick quality checks. These are useful to choose the number of atoms in the dictionary (i.e., number of topics): $R^2$ and Topic Diversity\n",
        "\n",
        "Useful to look at product of the two since $R^2$ tends to increase with higher # topics, as Topic Diversity decreases. Intuition: more topics can better explain the original semantic space, but also then these topics are less distinct from one another. As a result, we typically want a balance between the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObzUO-JZEEJS"
      },
      "source": [
        "def reconst_qual(w2vmodel, dictionary_mat, alpha_mat):\n",
        "    #reconstruct the word vectors\n",
        "    reconstructed = alpha_mat.dot(dictionary_mat) #reconstruct word vectors and add back in mean(?). but note that reconstructed norm is still around 0-1, not 1, is that an issue?\n",
        "    #e1 = norm(w2vmodel.wv.vectors - reconstructed) #total reconstruction error, larger means MORE error. norm as specified here takes frobenius norm of error matrix.\n",
        "\n",
        "\n",
        "    #total VARIANCE in the data: sum of squares \n",
        "    squares3= w2vmodel.wv.vectors-np.mean(w2vmodel.wv.vectors, axis=1).reshape(-1,1) #https://dziganto.github.io/data%20science/linear%20regression/machine%20learning/python/Linear-Regression-101-Metrics/\n",
        "    #sst3= np.sum([i.dot(i) for i in squares3] ) #same as below\n",
        "\n",
        "    sst3= np.sum(np.square(squares3))\n",
        "\n",
        "\n",
        "    #total sum of squared ERRORS/residuals\n",
        "    e3= [reconstructed[i]-w2vmodel.wv.vectors[i] for i in range(0,len(w2vmodel.wv.vectors))]  #https://dziganto.github.io/data%20science/linear%20regression/machine%20learning/python/Linear-Regression-101-Metrics/\n",
        "    #sse3= np.sum([i.dot(i) for i in e3] ) #same as below\n",
        "    sse3= np.sum(np.square(e3))\n",
        "\n",
        "    #R^2: 1- (SSE / SST )\n",
        "    r2= 1- (sse3 /  sst3) #https://stats.stackexchange.com/questions/184603/in-pca-what-is-the-connection-between-explained-variance-and-squared-error\n",
        "\n",
        "\n",
        "    #compute root mean square error\n",
        "    rmse=  math.sqrt(np.mean(np.square(e3)))\n",
        "\n",
        "\n",
        "\n",
        "    return(sse3, rmse, r2) #https://stats.stackexchange.com/questions/184603/in-pca-what-is-the-connection-between-explained-variance-and-squared-error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipIs45YhvhBM"
      },
      "source": [
        "#### Inferring topics from document\n",
        "\n",
        "We now use a similar approach to what we saw a little earlier, where we inverted our generative model to see which documents belong to which class - we do the same now, but with discourse atoms instead of the whole model. This process tells us the topic most likely to have generated a specific context (document). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGNQXH96vmiD"
      },
      "source": [
        "#topic diversity (% unique words among total closest 25 words to each atom)\n",
        "def topic_diversity(w2vmodel, dictionary_mat, top_n=25):\n",
        "\n",
        "    topwords=[] #list of list, each innter list includes top N words in that topic\n",
        "\n",
        "    for i in range(0, len(dictionary_mat)): #set to number of total topics\n",
        "        topwords.extend([i[0] for i in w2vmodel.wv.similar_by_vector(dictionary_mat[i],topn=top_n)]) #set for top N words \n",
        "        #print(w2vmodel.wv.similar_by_vector(dictionary[i],topn=N))\n",
        "\n",
        "    uniquewords= set(topwords)\n",
        "    diversity = len(uniquewords)/len(topwords)\n",
        "    return(diversity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMuRYYmVFMCF"
      },
      "source": [
        "dictionary, alpha = do_aksvd(nytimes_w2v_model, 150, 5, save=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0njiXVfFzpH"
      },
      "source": [
        "topic_diversity(nytimes_w2v_model, dictionary, top_n=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQGv2X_NFL60"
      },
      "source": [
        "reconst_qual(nytimes_w2v_model, dictionary, alpha)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiCYSBTwFLvK"
      },
      "source": [
        "#loading back in the model pieces if not already in\n",
        "\n",
        "# infile = open('../data/aksvd_models/200d_150comp5nonzeros_dictionary_nvdrsdf20','rb')\n",
        "# dictionary=pickle.load(infile)\n",
        "# infile.close()\n",
        "\n",
        "# infile = open('../data/aksvd_models/200d_150comp5nonzeros_aksvd_nvdrsdf20','rb')\n",
        "# aksvd=pickle.load(infile)\n",
        "# infile.close()\n",
        "\n",
        "# infile = open('../data/aksvd_models/200d_150comp5nonzeros_alpha_nvdrsdf20','rb')\n",
        "# alpha=pickle.load(infile)\n",
        "# infile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VgluAGRGCHi"
      },
      "source": [
        "for i in range(0, len(dictionary)):\n",
        "    print(\"Discourse_Atom \" + str(i))\n",
        "    print([i[0] for i in nytimes_w2v_model.wv.similar_by_vector(dictionary[i],topn=25)]) #what are the most similar words to the Nth\n",
        "    #print([i[0] for i in w2vmodel.wv.similar_by_vector(-dictionary[i],topn=25)]) #what are the most similar words to the Nth dicourse atom?\n",
        "    print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysp5BMTxGEnm"
      },
      "source": [
        "# for a specific atom, e.g., 112th atom look at 25 most similar words:\n",
        "nytimes_w2v_model.wv.similar_by_vector(dictionary[112],topn=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_C7I9ZFGVi9"
      },
      "source": [
        "print(nytimes_w2v_model.wv.vocab.get('the').index, '\\n', alpha[w2vmodel.wv.vocab.get('the').index])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ZtXsVrGo-1"
      },
      "source": [
        "#useful relevant code:\n",
        "#w2vmodel.wv.index2word[3452]\n",
        "#w2vmodel.wv.most_similar('the', topn=15)\n",
        "#np.where(alpha[w2vmodel.wv.vocab.get('the').index] != 0) #get index where the loading of a word onto discourse atoms is not 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf_MDQsKp8Vh"
      },
      "source": [
        "! pip install svd2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arnse03a_N4K"
      },
      "source": [
        "We've seen a few corpora so far - the small hobbies corpus, and the wikipedia corpus. We will use the Wikipedia corpus for training this model, as it is more suited to training a language model. While we could directly feed the raw file to train it for FastText, svd2vec requires it to be similar to the gensim format, which is a list of lists representing the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR7EzV2NyFN3"
      },
      "source": [
        "from svd2vec import svd2vec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQXNWMstJPO_"
      },
      "source": [
        "wiki_texts = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVoqA_9iJqO1"
      },
      "source": [
        "for line in open(\"/content/drive/MyDrive/fil9\"):\n",
        "  # this is an example of generating a corpus - you can use your own!\n",
        "  wiki_texts.append(line)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euohwKC3uB5P"
      },
      "source": [
        "# code to clean files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-Qf2o8080yM"
      },
      "source": [
        "# creating the words representation (can take a while)\n",
        "svd = svd2vec(wiki_texts, window=5, min_count=100, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_KWCr1WACTb"
      },
      "source": [
        "svd.similarity(\"bad\", \"good\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5dpclQ7ACL2"
      },
      "source": [
        "svd.similarity(\"monday\", \"friday\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AuKGCgVACAD"
      },
      "source": [
        "svd.distance(\"apollo\", \"moon\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoOPEefPAh0U"
      },
      "source": [
        "svd.most_similar(positive=[\"january\"], topn=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw0PTIP5Ahws"
      },
      "source": [
        "svd.analogy(\"paris\", \"france\", \"berlin\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpbRADc0Ali2"
      },
      "source": [
        "svd.analogy(\"road\", \"cars\", \"rail\", topn=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0vlhqhpB4x9"
      },
      "source": [
        "This sort of matrix decomposition can be as powerful as word2vec - please follow this [notebook tutorial](https://valentinp72.github.io/svd2vec/gensim_comparison.html) for a demonstration comparing it to the gensim word2vec algorithm on the same corpus."
      ]
    }
  ]
}