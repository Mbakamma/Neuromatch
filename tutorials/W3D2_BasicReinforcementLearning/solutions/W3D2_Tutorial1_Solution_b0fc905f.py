"""
For TAs:

Encourage students to consider different examples of RL in the real
world that have safety issues that don't necessarily come up with supervised
learning, due to the fact that in RL, agents must *interact* with the world,
and explore.

Self-driving cars is a really salient example, but also consider using RL in
the health-care setting, where exploration would be unethical. What can be done
in this case? (Consider offline RL, using simulations, safeguards, etc.)

Interpretability can help with the safe deployment of deep RL algorithms
because we can have higher expectation that our agents will not perform
unpredictable actions, or if they do, we can at least understand why they did,
and work to prevent such occurrences in the future.

This should be a very open-ended discussion. Try to have everyone say at least
one thing. They can either take these 3 questions in turn, with 3-4 minutes
allotted to each, or address them all at once, and allow for a more natural
conversation.

"""