{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Neuromatch Academy: Week 3, Day 1, Tutorial 1\n",
    "# Unsupervised and self-supervised learning: Representations\n",
    "\n",
    "__Content creators:__ Arna Ghosh, Colleen Gillon, Blake Richards, Tim Lillicrap\n",
    "\n",
    "__Content reviewers:__ Atnafu Lambebo, Hadi Vafaei, Khalid Almubarak, Melvin Selim Atay\n",
    "\n",
    "__Content editors:__ Anoop Kulkarni, Spiros Chalvis\n",
    "\n",
    "__Production editors:__ Deepak Raya, Spiros Chalvis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Tutorial Objectives\n",
    "\n",
    "In this tutorial, you will learn about the importance of learning good representations of data.\n",
    "\n",
    "Specific objectives for this tutorial:\n",
    "*   Train logistic regressions (A) directly on input data and (B) on representations learned from the data.\n",
    "*   Compare the classification performances achieved by the different networks.\n",
    "*   Compare the representations learned by the different networks\n",
    "*   Identify the advantages of self-supervised learning over supervised or traditional unsupervised methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "outputId": "d1d06857-32b8-424d-abfa-6b3c8c39db56"
   },
   "outputs": [],
   "source": [
    "#@markdown Tutorial slides\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://docs.google.com/presentation/d/1Wa9-SfwfM0r2jFwZego892K_u_Z7dgr0RQI00i5Nzpc/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "1d0229c4-bddc-4258-c020-3c3022a9945f"
   },
   "outputs": [],
   "source": [
    "#@title Video 0: Introduction\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"lnC-wzzF_rE\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dd4d44ad-5ae5-4630-8a66-fe015ff7bb9c"
   },
   "outputs": [],
   "source": [
    "#@title Download dataset, modules, and files needed for the tutorial from GitHub.\n",
    "\n",
    "# The code is below temporary. It enables our data and code currently hosted on\n",
    "# a PRIVATE Github repo to be downloaded to the colab.\n",
    "# It uses an exposed READ-ONLY deploy ssh key.\n",
    "# This is safe, as they key gives ONLY read access and ONLY to the neuromatch\n",
    "# repo.\n",
    "# Once we make the repo public, we will replace this with a simple git clone.\n",
    "\n",
    "import os, sys, importlib\n",
    "\n",
    "REPO_PATH = \"neuromatch_ssl_tutorial\"\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    # download and place deploy key\n",
    "    !mkdir -p /root/.ssh\n",
    "    !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1DIL4zYLqhpEJjpsB4NQI29atFxsnvVd7' -O /root/.ssh/id_ed25519 --quiet\n",
    "\n",
    "    # configure .ssh folder\n",
    "    !touch /root/.ssh/known_hosts\n",
    "    !ssh-keyscan github.com >> /root/.ssh/known_hosts 2> /dev/null # completely silenced\n",
    "    !chmod 700 /root/.ssh\n",
    "    !chmod 600 /root/.ssh/id_ed25519\n",
    "    !chmod 644 /root/.ssh/known_hosts\n",
    "\n",
    "download_str = \"Downloading\"\n",
    "if os.path.exists(REPO_PATH):\n",
    "    download_str = \"Redownloading\"\n",
    "    !\\rm -rf $REPO_PATH\n",
    "\n",
    "# download from github repo\n",
    "print(f\"{download_str} data, modules, and files from GitHub to {REPO_PATH}...\")\n",
    "!ssh -Tq git@github.com 2> /dev/null # completely silenced\n",
    "!git clone git@github.com:colleenjg/neuromatch_ssl_tutorial.git --quiet\n",
    "\n",
    "from neuromatch_ssl_tutorial.modules import data, load, models, plot_util\n",
    "importlib.reload(data);\n",
    "importlib.reload(load);\n",
    "importlib.reload(models)\n",
    "importlib.reload(plot_util);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import modules designed for use in these tutorials\n",
    "from neuromatch_ssl_tutorial.modules import data, load, models, plot_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", message=\"This figure includes Axes\", category=UserWarning\n",
    "    )\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", message=\"Calculating RSM\", category=UserWarning\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Plotting functions\n",
    "\n",
    "# @markdown Function to plot a histogram of RSM values: `plot_rsm_histogram(rsms, colors)`\n",
    "def plot_rsm_histogram(rsms, colors, labels=None, nbins=100):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.set_title(\"Histogram of RSM values\", y=1.05)\n",
    "\n",
    "    min_val = np.min([np.nanmin(rsm) for rsm in rsms])\n",
    "    max_val = np.max([np.nanmax(rsm) for rsm in rsms])\n",
    "\n",
    "    bins = np.linspace(min_val, max_val, nbins+1)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [labels] * len(rsms)\n",
    "    elif len(labels) != len(rsms):\n",
    "        raise ValueError(\"If providing labels, must provide as many as RSMs.\")\n",
    "\n",
    "    if len(rsms) != len(colors):\n",
    "        raise ValueError(\"Must provide as may colors as RSMs.\")\n",
    "\n",
    "    for r, rsm in enumerate(rsms):\n",
    "        ax.hist(\n",
    "            rsm.reshape(-1), bins, density=True, alpha=0.6,\n",
    "            color=colors[r], label=labels[r]\n",
    "            )\n",
    "    ax.axvline(x=0, ls=\"dashed\", alpha=0.6, color=\"k\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_xlabel(\"Similarity values\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "# @markdown Function to set test custom torch RSM function: `test_custom_torch_RSM_fct()`\n",
    "def test_custom_torch_RSM_fct(custom_torch_RSM_fct):\n",
    "  rand_feats = torch.rand(100, 1000)\n",
    "  RSM_custom = custom_torch_RSM_fct(rand_feats)\n",
    "  RSM_ground_truth = data.calculate_torch_RSM(rand_feats)\n",
    "\n",
    "  if torch.allclose(RSM_custom, RSM_ground_truth, equal_nan=True):\n",
    "    print(\"custom_torch_RSM_fct() is correctly implemented.\")\n",
    "  else:\n",
    "    print(\"custom_torch_RSM_fct() is NOT correctly implemented.\")\n",
    "\n",
    "\n",
    "# @markdown Function to set test custom contrastive loss function: `test_custom_contrastive_loss_fct()`\n",
    "def test_custom_contrastive_loss_fct(custom_simclr_contrastive_loss):\n",
    "  rand_proj_feat1 = torch.rand(100, 1000)\n",
    "  rand_proj_feat2 = torch.rand(100, 1000)\n",
    "  loss_custom = custom_simclr_contrastive_loss(rand_proj_feat1, rand_proj_feat2)\n",
    "  loss_ground_truth = models.contrastiveLoss(rand_proj_feat1,rand_proj_feat2)\n",
    "\n",
    "  if torch.allclose(loss_custom, loss_ground_truth):\n",
    "    print(\"custom_simclr_contrastive_loss() is correctly implemented.\")\n",
    "  else:\n",
    "    print(\"custom_simclr_contrastive_loss() is NOT correctly implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Set random seed.\n",
    "\n",
    "#@markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "be51d0cd-ad5e-42b3-d5d6-e1df784ffa63"
   },
   "outputs": [],
   "source": [
    "#@title Set device (GPU or CPU). Execute `set_device()`\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "524d6011-68ef-4c87-c9b5-147c0fcc77de"
   },
   "outputs": [],
   "source": [
    "# Set global variables\n",
    "SEED = 2021\n",
    "DEVICE = set_device()\n",
    "REPO_PATH = \"neuromatch_ssl_tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Representations are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "c8c2df1f-31aa-4797-ad1f-0f7054780634"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Why do representations matter?\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"ZK4-Exik4-s\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Introducing the dSprites dataset\n",
    "\n",
    "In this tutorial, we will be using a subset of the openly available **dSprites dataset** to investigate the importance of learning good representations.  \n",
    "\n",
    "_N.B.: For convenience, we will be using a subset of the original, full dataset which is available [here](https://github.com/deepmind/dsprites-dataset/), on GitHub._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSprites = data.dSpritesDataset(\n",
    "    os.path.join(REPO_PATH, \"dsprites\", \"dsprites_subset.npz\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo 1.1.1: Exploring the dSprites dataset\n",
    "\n",
    "In this first demo, we will get to know the **dSprites dataset**. This dataset is made up of black and white images (20,000 images total in the subset we are using).  \n",
    "\n",
    "The images in the dataset were can be described using different combinations of **latent dimension values**, sampled from:\n",
    "- **Shapes (3):** square (1.0), oval (2.0) or heart (3.0)\n",
    "- **Scales (6):** 0.5 to 1.0\n",
    "- **Orientations (40):** 0 to 2$\\pi$\n",
    "- **Positions in X (32):** 0 to 1 (left to right)\n",
    "- **Positions in Y (32):** 0 to 1 (top to bottom)\n",
    "\n",
    "As a result, **each image carries 5 labels.** One for each of the latent dimension.  \n",
    "\n",
    "We will first load the dataset into the `dSprites` object, which is an instance of the `data.dSpritesDataset` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `dSpritesDataset` class method `show_images()` to plot a few images from the dataset, with their latent dimension values printed below.    \n",
    "\n",
    "**Interactive Demo:** View a different set of randomly sampled images by passing the random state argument `randst` any integer or the value `None`. (The original setting is `randst=SEED`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "outputId": "8842b53a-89b2-4a70-dfb0-cf529b953e4b"
   },
   "outputs": [],
   "source": [
    "# DEMO: to view different images, set randst to any integer value.\n",
    "dSprites.show_images(num_images=10, randst=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Training a classifier with and without representations  \n",
    "\n",
    "Now, we will investigate how 2 different types of classifiers perform when trained to decode the shape latent dimension of images in the **dSprites dataset**.  \n",
    "\n",
    "Specifically, we will train **one classifier directly on the images**, and **another on the output of an encoder network**.\n",
    "\n",
    "The **encoder network** we will use here and throughout the tutorial is the multi-layer convolutional network, pictured below. It comprises 2 consecutive convolutional layers, followed by 3 fully connected layers, and uses average pooling and batch normalization between layers, as well as rectified linear units as non-linearities. \n",
    "\n",
    "The **classifier layer** then takes the encoder features as input, predicting, for example, the shape latent dimension of encoded input images.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1ZlKeDK7cD-3aNV8EbC0XOQr8yuQvBuGd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo 1.2.1: Training a logistic regression classifier directly on images.\n",
    "\n",
    "The following code:\n",
    "*    seeds modules that will use random processes, to ensure the results are consistently reproducible, using the `seed_processes()` function,\n",
    "*    collects the dSprites dataset into a torch dataset using the `data.dSpritesTorchDataset` class, \n",
    "*    initializes a training and test sampler to keep the two datasets separate using the `data.train_test_splix_idx()` function.\n",
    "\n",
    "_(cont'd below)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b2808cec-b4d9-4ebc-9670-aeedce850f36"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# initialize a torch dataset, specifying the target latent dimension for\n",
    "# the classifier\n",
    "dSprites_torchdataset = data.dSpritesTorchDataset(\n",
    "  dSprites,\n",
    "  target_latent=\"shape\"\n",
    "  )\n",
    "\n",
    "# initialize a train_sampler and a test_sampler to keep the two sets\n",
    "# consistently separate\n",
    "train_sampler, test_sampler = data.train_test_split_idx(\n",
    "  dSprites_torchdataset,\n",
    "  fraction_train=0.8,  # 80:20 data split\n",
    "  randst=SEED\n",
    "  )\n",
    "\n",
    "print(f\"Dataset size: {len(train_sampler)} training, \"\n",
    "      f\"{len(test_sampler)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo 1.2.1: Training a logistic regression classifier directly on images (cont'd).\n",
    "\n",
    "The following code:\n",
    "*    trains a logistic regression directly on the training set images to classify their shape, and assesses its performance on the test set images using the `models.train_classifier()` function.\n",
    "\n",
    "**Interactive Demo:** Try a few different `num_epochs` settings to see whether performance improves with more training, e.g., between 1 and 50 epochs. (The original setting is `num_epochs=25`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151,
     "referenced_widgets": [
      "878c6ffa65684036ab2a338eed8a10e0",
      "97270c4620b14223859d5982294082b1",
      "532e3e55197c42159b98c54634c327f5",
      "4b065009ef2a4ab282e18e95a144d2f8",
      "ba118dfd96c84581a3610620bb1abb93",
      "6801fd1875f54fe781ec0dfb66a355b7",
      "6f4c21bc953d440fb5ce2b29f1703f54",
      "8c9dc88976dc4d1e8f7bec4f7c21c4b1"
     ]
    },
    "outputId": "74693653-2d19-49f6-945b-f4f475abb4ec"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "num_epochs = 25 # DEMO: Try different numbers of training epochs\n",
    "\n",
    "# train a classifier directly on the images\n",
    "print(\"Training a classifier directly on the images...\")\n",
    "_ = models.train_classifier(\n",
    "  encoder=None,\n",
    "  dataset=dSprites_torchdataset,\n",
    "  train_sampler=train_sampler,\n",
    "  test_sampler=test_sampler,\n",
    "  freeze_features=True, # there is no feature encoder to train here, anyway\n",
    "  num_epochs=num_epochs,\n",
    "  verbose=True # print results\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the classifier trained directly on the images performs only a bit above chance (39.55%) on the test set, after 25 training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 1.2.1: Training a logistic regression classifier along with an encoder.\n",
    "\n",
    "The following code:\n",
    "*    uses the same dSprites torch dataset (`dSprites_torchdataset`) initialized above, as well as the training and test samplers (`train_sampler`, `test_sampler`),\n",
    "*    again, seeds modules that will use random processes, to ensure the results are consistently reproducible,\n",
    "*    initializes an encoder network to use in the supervised network using the `models.EncoderCore` class,\n",
    "*    sets a proposed number of epochs to use when training the classifier and encoder (`num_epochs=10`). \n",
    "\n",
    "**Exercise:** Train a classifier, along with the encoder, to classify the input images according to shape, using `models.train_classifier()`. How does it perform?\n",
    "\n",
    "**Hint**: `models.train_classifier()` is introduced in **Interactive Demo 1.2.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_encoder(num_epochs, seed):\n",
    "  # call this before any dataset/network initializing or training,\n",
    "  # to ensure reproducibility\n",
    "  set_seed(seed)\n",
    "\n",
    "  # initialize a core encoder network on which the classifier will be added\n",
    "  supervised_encoder = models.EncoderCore()\n",
    "\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Train a supervised encoder and classifier.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Train an encoder and classifier on the images, using models.train_classifier()\n",
    "  print(\"Training a supervised encoder and classifier...\")\n",
    "  _ = models.train_classifier(\n",
    "      encoder=...,\n",
    "      dataset=...,\n",
    "      train_sampler=...,\n",
    "      test_sampler=...,\n",
    "      freeze_features=..., # we train the encoder, along with the classifier layer\n",
    "      num_epochs=...,\n",
    "      verbose=... # print results\n",
    "      )\n",
    "\n",
    "  return supervised_encoder\n",
    "\n",
    "\n",
    "num_epochs = 10 # Proposed number of training epochs\n",
    "## Uncomment below to test your function\n",
    "# supervised_encoder = train_supervised_encoder(num_epochs=num_epochs, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151,
     "referenced_widgets": [
      "e4db3b79170140178ee7b0c54131281b",
      "8d253ae795d04d00bb95da9ecf67f3b1",
      "ab7235ea88ee4a89a27ac5f22d348de1",
      "6e265b1c325f45ba8e2c1952fde6d970",
      "d404c5f4e9434c0eaf09bf95e523b3d5",
      "0ba99051af2c40dc9fd4398f92aca365",
      "aad5b77ad4b34879babdd952b75a7977",
      "d54ab003db324c2ab8165901a7653cb4"
     ]
    },
    "colab_type": "text",
    "outputId": "ca1ed130-8a47-423f-e95e-1d5cb0f9f53e"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_737fdb93.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Training a supervised encoder and classifier...\n",
    "100%                                    10/10 [00:12<00:00, 1.23s/it]\n",
    "\n",
    "Network performance after 10 encoder and classifier training epochs (chance: 33.33%):\n",
    "    Training accuracy: 100.00%\n",
    "    Testing accuracy: 98.72%\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the classifier is trained with an encoder network, however, it achieves very high classification accuracy (98.40%) on the test set, after only 10 training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Supervised learning induces invariant representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "b53bfb39-1e10-4c07-e23e-59c506ac0882"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Supervised Learning and Invariance\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"E8nMPniyG0c\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: Examining Representational Similarity Matrices (RSMs)\n",
    "\n",
    "To examine the representations learned by the encoder network, we use **Representational Similarity Matrices (RSMs)**. In these matrices, the similarity between the encoder's representations of each possible pair of images is plotted to reveal overall structure in representation space.  \n",
    "\n",
    "_N.B.: Here, we use cosine similarity as a measure of representational similarity._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 2.1.1: Complete a function that calculates RSMs.\n",
    "\n",
    "The following code:\n",
    "*    lays out the skeleton of a function `custom_torch_fct()` which calculates an RSM from features,\n",
    "*    tests the custom function against the solution implementation.\n",
    "\n",
    "**Exercise:** Complete the `custom_torch_fct()` implementation.\n",
    "\n",
    "**Hint**: `custom_torch_fct()` takes 1 input argument: \n",
    "- features (2D torch Tensor): feature matrix (items x features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_torch_RSM_fct(features):\n",
    "  \"\"\"\n",
    "  custom_torch_RSM_fct(features)\n",
    "\n",
    "  Custom function to calculates representational similarity matrix (RSM) of a feature\n",
    "  matrix using pairwise cosine similarity.\n",
    "\n",
    "  Complete the function below given the specific guidelines.\n",
    "  Uses torch.nn.functional.cosine_similarity()\n",
    "\n",
    "  Required args:\n",
    "  - features (2D torch Tensor): feature matrix (items x features)\n",
    "\n",
    "  Returns:\n",
    "  - rsm (2D torch Tensor): similarity matrix\n",
    "      (nbr features items x nbr features items)\n",
    "  \"\"\"\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your function\n",
    "  raise NotImplementedError(\"Exercise: Implement RSM calculation.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Implement RSM calculation\n",
    "  rsm = ...\n",
    "\n",
    "  if not rsm.shape == (len(features), len(features)):\n",
    "    raise ValueError(\n",
    "        f\"RSM should be of shape ({len(features)}, {len(features)})\"\n",
    "        )\n",
    "\n",
    "  return rsm\n",
    "\n",
    "\n",
    "# Test implementation by comparing output to solution implementation\n",
    "# test_custom_torch_RSM_fct(custom_torch_RSM_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "711c1fd2-2cb2-43ee-9dfe-946d8e1fc9d7"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_e6f4595f.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo 2.1.1: Plotting the supervised network encoder RSM along different latent dimensions.\n",
    "\n",
    "In this demo, we calculate an RSM for representations of the test set images generated by the supervised network encoder. \n",
    "\n",
    "The following code:\n",
    "*    calculates and plots the RSM for the test set, with rows and columns sorted by whichever latent dimension is specified (e.g., `sorting_latent=\"shape\"`) using `models.plot_model_RSMs()`.\n",
    "\n",
    "**Interactive Demo:** In the current example, the rows and columns of the RSM are organized by along the `shape` latent dimension. Try organizing them along one of the other latent dimensions (`\"posX\"`, `\"posY\"`, `\"orientation\"` or `\"scale\"`) to see whether different patterns emerge. (The original setting is `sorting_latent=\"shape\"`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "outputId": "2ab7a36b-2f99-437c-e45b-18fcd7a7bda6"
   },
   "outputs": [],
   "source": [
    "sorting_latent = \"shape\" # DEMO: Try sorting by different latent dimensions\n",
    "\n",
    "print(\"Plotting RSMs...\")\n",
    "_ = models.plot_model_RSMs(\n",
    "    encoders=[supervised_encoder], # we pass the trained supervised_encoder\n",
    "    dataset=dSprites_torchdataset,\n",
    "    sampler=test_sampler, # we want to see the representations on the held out test set\n",
    "    titles=[\"Supervised network encoder RSM\"], # plot title\n",
    "    sorting_latent=sorting_latent,\n",
    "    RSM_fct=custom_torch_RSM_fct\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2.1.1: What patterns do the RSMs reveal about how the encoder represents different images? \n",
    "**A.** What does the yellow (maximal similarity color) diagonal, going from the top left to the bottom right, correspond to?  \n",
    "**B.** What pattern can be observed when comparing RSM values for pairs of images that share a similar latent value (e.g., 2 heart images) vs pairs of images that do not (e.g., a heart and a square image)?   \n",
    "**C.** Do some shapes appear to be encoded more similarly than others?  \n",
    "**D.** Do some latent dimensions show clearer RSM patterns than others? Why might that be so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses to 2.1.1\n",
    "**A.** The yellow diagonal corresponds to the similarity between each encoded image and itself. Since each encoded image is, of course, identical to itself, the similarity is of 1 at each point on the diagonal.  \n",
    "**B.** The pattern we observe is that there are square sections of the RSM that have higher similarity values than the rest, and these sections lie along the yellow diagonal. These sections correspond to the similarities between **encoded images of the same shape** (e.g., 2 hearts), which are generally **higher than the similarities between encoded images of different shapes** (e.g., a heart and a square), when using this trained, supervised encoder.  \n",
    "**C.** It is a bit subtle, but it looks like the **hearts and squares** might be encoded more similarly to one another than the **hearts and ovals**, in general. This is based on the fact that the RSM values for hearts x squares (bottom left and top right) appear to be lighter (more yellow) than the the RSM values for hearts x ovals (bottom middle and middle right), which are a bit darker (more blue).  \n",
    "**D.**  If we sort by different latent dimensions (e.g., orientation, scale, posX or posY), we do not see as much structure in the RSMs. This is because the supervised encoder is specifically trained on a shape classification task, which forces it to encode images of the same shape more similarly, and images of different shapes more differently. It is not trained to distinguish orientations, scales or positions. If it **were** trained to predict orientation, scale or position, we could expect to see similar RSM patterns, with high similarity along the diagonal for the predicted dimension. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Random projections don’t work as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "aed4f9d5-d586-4833-d790-52b8658d443c"
   },
   "outputs": [],
   "source": [
    "#@title Video 3: Random Representations\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"px-DEXijrbo\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Examining RSMs of an untrained encoder\n",
    "\n",
    "To determine whether the patterns observed in the RSMs of the supervised network encoder are trivial, we investigate whether they also emerge from the **random projections of an untrained encoder**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 3.1.1: Plotting an untrained network encoder RSM along different latent dimensions.\n",
    "\n",
    "In this exercise, we repeat the same analysis as in **Section 2.1**, but with an untrained encoder.  \n",
    "\n",
    "The following code:\n",
    "*    initializes an encoder network to use in the untrained network using the `models.EncoderCore` class,\n",
    "*    proposes a latent dimension along which to sort the rows and columns (`sorting_latent = \"shape\"`)\n",
    "\n",
    "**Exercise:** \n",
    "*    Visualize the RSMs for the supervised and untrained network encoders, using `models.plot_model_RSMs()`. \n",
    "*    Visualize the RSMs, organized along different latent dimensions (`\"posX\"`, `\"posY\"`, `\"orientation\"` or `\"scale\"`), and compare the patterns observed for the supervised versus the untrained encoder network.\n",
    "\n",
    "**Hint**: `models.plot_model_RSMs()` is introduced in **Interactive Demo 2.1.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rsms(seed):\n",
    "  # call this before any dataset/network initializing or training,\n",
    "  # to ensure reproducibility\n",
    "  set_seed(seed)\n",
    "\n",
    "  # initialize a core encoder network that will not get trained\n",
    "  untrained_encoder = models.EncoderCore()\n",
    "\n",
    "  # EXERCISE: Try sorting by different latent dimensions\n",
    "  sorting_latent = \"shape\"\n",
    "\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Plot RSMs.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Plot RSMs\n",
    "  print(\"Plotting RSMs...\")\n",
    "  _ = models.plot_model_RSMs(\n",
    "      encoders=[..., ...],  # we pass both encoders\n",
    "      dataset=...,\n",
    "      sampler=...,  # we want to see the representations on the held out test set\n",
    "      titles=[\"Supervised network encoder RSM\",\n",
    "              \"Untrained network encoder RSM\"],  # plot titles\n",
    "      sorting_latent=...,\n",
    "      )\n",
    "\n",
    "  return untrained_encoder\n",
    "\n",
    "\n",
    "## Uncomment below to test your function\n",
    "# untrained_encoder = plot_rsms(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "text",
    "outputId": "37f43a2d-5bb0-40c8-d9f7-2e84b5049542"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_52c82c06.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=751 height=376 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_52c82c06_2.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3.1.1: What does comparing these RSMs reveal about the potential value of trained versus untrained encoder representations? \n",
    "\n",
    "**A.** What patterns, if any, are visible in the untrained network encoder RSM?   \n",
    "**B.** Which encoder network is most likely to produce meaningful representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses to 3.1.1\n",
    "**A.** Only the yellow diagnonal identity line is visible. No other patterns emerge, as most images are encoded with near 0 similarity to one another, using the untrained encoder.  \n",
    "**B.** The trained, supervised network produces more meaningful representations, as the similarity between different encoded images actually **captures certain meaningful conceptual similarities between the different images**, specifically shape. In other words, the image representations obtained with the trained, supervised encoding reflect the fact that two hearts are more conceptually similar to each other in terms of shape than a heart and a square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 3.1.2: Evaluating the classification performance of a logistic regression trained on the representations produced by an untrained network encoder.\n",
    "\n",
    "In this exercise, we repeat a similar analysis to **Section 1.2.2**, but with the untrained encoder network. Importantly, this time, the encoder parameters must stay frozen during training by setting `freeze_features=True`. Instead of being provided ahead of time a suggestion for a reasonable number of training epochs, we use the training loss array to select a good value.  \n",
    "\n",
    "\n",
    "The following code:\n",
    "*    trains a logistic regression on top of the untrained encoder network to classify images based on shape, and assesses its performance on the test set images using `models.train_classifier()` with `freeze_features=True` to ensure that the encoder is **not** trained, and only the classifier is.\n",
    "\n",
    "**Exercise:** \n",
    "*     Set a number of epochs for which  to train classifier.  \n",
    "*     Plot the training loss array (`untrained_loss_array`) returned when training the model. _(N.B.: 'untrained' is in the name here simply because the classifier is trained on the ***untrained*** network encoder.)_ \n",
    "*     Rerun the classifier if more training epochs are needed based on the progression of the training loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(num_epochs, seed):\n",
    "  # call this before any dataset/network initializing or training,\n",
    "  # to ensure reproducibility\n",
    "  set_seed(seed)\n",
    "\n",
    "  # train classifier on the randomly encoded images\n",
    "  print(\"Training a classifier on the untrained encoder representations...\")\n",
    "  _, untrained_loss_array, _, _ = models.train_classifier(\n",
    "      encoder=untrained_encoder,\n",
    "      dataset=dSprites_torchdataset,\n",
    "      train_sampler=train_sampler,\n",
    "      test_sampler=test_sampler,\n",
    "      freeze_features=True,  # keep the encoder frozen while training the classifier\n",
    "      num_epochs=num_epochs,\n",
    "      verbose=True  # print results\n",
    "      )\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Plot loss array.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Plot the loss array\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(...)\n",
    "  ax.set_title(...)\n",
    "  ax.set_xlabel(...)\n",
    "  ax.set_ylabel(...)\n",
    "\n",
    "  return untrained_loss_array\n",
    "\n",
    "# EXERCISE: Set a reasonable number of training epochs\n",
    "num_epochs = 25\n",
    "## Uncomment below to test your plot\n",
    "# untrained_loss_array = plot_loss(num_epochs=num_epochs, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564,
     "referenced_widgets": [
      "53ebe639e6aa457fad8b3c439159db2a",
      "771acbc4d12f45c3bbfa7a758dcc2cb5",
      "eb0e8c5cc9a84cd19c7287b042b8de9c",
      "f30c0c80dc27419d994783baf7eeaf25",
      "7c4642e95dd943f9bd066b6e740690b0",
      "3699bf87e5174a17ae08ff1f6cf4a5ef",
      "eb0b8ebd8bc246b5a74932606a4d2f16",
      "2d426c4834304df990fae026d267a8aa"
     ]
    },
    "colab_type": "text",
    "outputId": "3cf9ac47-f738-4fa3-b0f2-eaa3fbae7c9b"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_a453e17d.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=555 height=413 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_a453e17d_3.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network loss training start is fairly stable by 25 epochs, at which point the classifier performs at 44.67% accuracy on the test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3.1.2: What can we conclude about the potential consequences of using random projections with a dataset like dSprites? \n",
    "\n",
    "**A.** How does the classifier performance compare to (A) the classifier trained directly on the images and (B) the classifier trained along with the encoder?   \n",
    "**B.** What explains these different performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses 3.1.2\n",
    "**A.**  \n",
    "**B.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Generative approaches to representation learning can fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "185b912f-a8c7-41fb-8d9a-5660bec25d99"
   },
   "outputs": [],
   "source": [
    "#@title Video 4: Generative models\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"znihs6t4OHg\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.1: Examining the RSMs of a Variational Autoencoder\n",
    "\n",
    "We next ask what kind of representations a network can learn in the absence of labelled data. We first look at a **generative model**, namely the **Variational Autoencoder (VAE)**.  \n",
    "\n",
    "Given that generative models typically require more training than supervised models, instead of pre-training a network here, we will load one that was **pre-trained for 300 epochs**. Importantly, the **encoder shares the same architecture** as the one used for the supervised and untrained examples above.\n",
    "\n",
    "The following code:\n",
    "*    loads the parameters of a full VAE network (encoder and decoder) pre-trained on the generative task of reconstructing the input images, under the Kullback–Leibler divergence (KLD) minimization constraint over the latent space that characterizes VAEs, using `load.load_encoder()` and `load.load_decoder()`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ca98a696-050b-4552-b517-4bc39a304081"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load VAE encoder and decoder pre-trained on the reconstruction and KLD tasks\n",
    "vae_encoder = load.load_encoder(REPO_PATH, model_type=\"vae\")\n",
    "vae_decoder = load.load_vae_decoder(REPO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo 4.1.1: Plotting example reconstructions using the pre-trained VAE encoder and decoder.\n",
    "\n",
    "In this demo, we sample images from the test set, and take a look at the quality of the reconstructions using `models.plot_vae_reconstructions()`.  \n",
    "\n",
    "**Interactive Demo:** Try plotting different images from the test dataset by selecting different `test_sampler.indices` values. (Original setting is `indices=test_sampler.indices[:10]`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    },
    "outputId": "560c9432-be88-47de-f239-560d100eec20"
   },
   "outputs": [],
   "source": [
    "models.plot_vae_reconstructions(\n",
    "    vae_encoder, # pre-trained encoder\n",
    "    vae_decoder, # pre-trained decoder\n",
    "    dataset=dSprites_torchdataset,\n",
    "    indices=test_sampler.indices[:10], # DEMO: select different indices to plot from the test set\n",
    "    title=\"VAE test set image reconstructions\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 4.1.1: How does the VAE perform on the reconstruction task? \n",
    "*    Which latent features does the network appear to encode well, and which does it encode less well?\n",
    "\n",
    "_N.B.: This VAE network uses a basic VAE loss with a convolutional encoder (our core encoder network), and a deconvolutional decoder. This can lead to some blurriness in the reconstructed shapes which a more sophisticated VAE could overcome._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 4.1.1: Evaluating the RSMs produced by the VAE encoder network.\n",
    "\n",
    "We will now compare the pre-trained VAE encoder network RSM to the previously generated encoder RSMs. \n",
    "\n",
    "**Exercise:** \n",
    "*    Visualize the RSMs for the supervised, untrained and VAE network encoders, using `models.plot_model_RSMs()`. \n",
    "*    Visualize the RSMs, organized along different latent dimensions (`\"posX\"`, `\"posY\"`, `\"orientation\"` or `\"scale\"`), and compare the patterns observed for the different encoder networks.\n",
    "\n",
    "**Hint**: `models.plot_model_RSMs()` is introduced in **Interactive Demo 2.1.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rsms_all():\n",
    "  sorting_latent = \"shape\"\n",
    "  print(\"Plotting RSMs...\")\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Plot RSMs.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Visualize RSMs for the supervised, untrained and VAE network encoders.\n",
    "  _ = models.plot_model_RSMs(\n",
    "      encoders=[..., ..., ...], # we pass all three encoders\n",
    "      dataset=...,\n",
    "      sampler=..., # we want to see the representations on the held out test set\n",
    "      titles=[\"Supervised network encoder RSM\", \"Untrained network encoder RSM\",\n",
    "              \"VAE network encoder RSM\"], # plot titles\n",
    "      sorting_latent=...,\n",
    "      )\n",
    "\n",
    "\n",
    "## Uncomment below to test your function\n",
    "# plot_rsms_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "colab_type": "text",
    "outputId": "d9b08cb3-4f33-4c72-b95b-b0f2408f12dd"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_191beb54.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=1107 height=376 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_191beb54_2.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 4.1.2: What can we conclude about the the ability of generative models like VAEs to construct a meaningful representation space?\n",
    "\n",
    "**A.** How do the pre-trained VAE encoder RSMs (sorted along different latent dimensions) compare to the supervised and untrained encoder network RSMs?  \n",
    "**B.** What explains these different RSMs?  \n",
    "**C.** How well will the pre-trained VAE encoder likely perform on the shape classification task, as compared to the other encoder networks?  \n",
    "**D.** Might the pre-trained VAE encoder be better suited to predicting a different latent dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses 4.1.2\n",
    "**A.**  \n",
    "**B.**  \n",
    "**C.**  \n",
    "**D.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 4.1.2: Evaluating the classification performance of a logistic regression trained on the representations produced by the pre-trained VAE network encoder.\n",
    "\n",
    "For the pre-trained VAE encoder, as the encoder parameters have already been trained, they should be kept frozen while the classifier is trained by setting `freeze_features=True`.   \n",
    "\n",
    "**Exercise:** \n",
    "*     Set a number of epochs for which  to train classifier.  \n",
    "*     Train a classifier, along with the encoder, to classify the input images according to shape, using `models.train_classifier()`.\n",
    "*     Plot the loss array returned when training the model, and update the number of training epochs, if needed. \n",
    "\n",
    "**Hint**: `models.train_classifier()` is introduced in **Interactive Demo 1.2.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_train_loss(num_epochs, seed):\n",
    "  # call this before any dataset/network initializing or training,\n",
    "  # to ensure reproducibility\n",
    "  set_seed(seed)\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Train a classifer on the pre-trained VAE encoder representations.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Train an encoder and classifier on the images, using models.train_classifier()\n",
    "  print(\"Training a classifier on the pre-trained VAE encoder representations...\")\n",
    "  _, vae_loss_array, _, _ = models.train_classifier(\n",
    "      encoder=...,\n",
    "      dataset=...,\n",
    "      train_sampler=...,\n",
    "      test_sampler=...,\n",
    "      freeze_features=..., # keep the encoder frozen while training the classifier\n",
    "      num_epochs=...,\n",
    "      verbose=... # print results\n",
    "      )\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Plot the VAE classifier training loss.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Plot the VAE classifier training loss.\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(...)\n",
    "  ax.set_title(...)\n",
    "  ax.set_xlabel(...)\n",
    "  ax.set_ylabel(...)\n",
    "\n",
    "  return vae_loss_array\n",
    "\n",
    "# Set a reasonable number of training epochs\n",
    "num_epochs = 25\n",
    "## Uncomment below to test your function\n",
    "# vae_loss_array = vae_train_loss(num_epochs=num_epochs, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564,
     "referenced_widgets": [
      "46c383202a484e71b46e422e02c851b6",
      "b51707097a004dc1bda7640d873395d2",
      "5df0a5e086e34c5c96f9d351f8b1e8c1",
      "8dbca4b03006446c8c6c73e24ba1d895",
      "b32791cd26d547d6b03dd6b9e85ede82",
      "362732b36d4f442ba0956777ae684dc4",
      "51492aa7a10a4c909c59f8b8070a63bd",
      "689f293efc054a6dbdf55fd56e46b57d"
     ]
    },
    "colab_type": "text",
    "outputId": "8cade323-838e-4b63-8632-615e40cd4014"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_db815e8e.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=558 height=413 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_db815e8e_3.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network loss training is fairly stable by 25 epochs, at which point the classifier performs at 45.75% accuracy on the test dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: The modern approach to self-supervised training for invariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "89f5976b-429e-4df5-f012-aa69ac1dbac0"
   },
   "outputs": [],
   "source": [
    "#@title Video 5: Modern Self-supervised Learning\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"S519cbZNDqU\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5.1: Examining different options for learning invariant representations.\n",
    "\n",
    "We now take a look at a few options for learning invariant shape representations for a dataset such as dSprites.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo 5.1.1: Visualizing a few different image transformations available that could be used to learn invariance.\n",
    "\n",
    "The following code:\n",
    "*    initializes a set of transforms called `invariance_transforms` using the `torchvision.transforms.RandomAffine` class,\n",
    "*    collects the dSprites dataset into a torch dataset `dSprites_invariance_torchdataset` which takes the `invariance_transforms` as input and deploys the transforms when it is called, \n",
    "*    shows a few examples of images and their transformed versions using the `data.dSpritesTorchDataset` `show_images()` method. \n",
    "\n",
    "The `torchvision.transforms.RandomAffine` class enables us to predetermine which types and ranges of transforms will be sampled from when transforming the images, by setting the following arguments:\n",
    "*    `degrees`: absolute maximum number of degrees to rotate\n",
    "*    `translate`: absolute maximum proportion of width to shift in x, and of height to shift in y\n",
    "*   `scale`: minimum to maximum scaling factor\n",
    "\n",
    "**Interactive Demo:** Try out a few combinations of the transformation parameters, and visualize the resulting effects on the original images. (The original settings are `degrees=90`, `translate=(0.2, 0.2)`, `scale=(0.8, 1.2)`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "outputId": "81753a88-ad4f-4f83-9106-4c854fb0bdb9"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# DEMO: Try some random affine data augmentations combinations to apply to the images\n",
    "invariance_transforms = torchvision.transforms.RandomAffine(\n",
    "    degrees=90,\n",
    "    translate=(0.2, 0.2),  # (in x, in y)\n",
    "    scale=(0.8, 1.2)   # min to max scaling\n",
    "    )\n",
    "\n",
    "# initialize a simclr-specific torch dataset\n",
    "dSprites_invariance_torchdataset = data.dSpritesTorchDataset(\n",
    "    dSprites,\n",
    "    target_latent=\"shape\",\n",
    "    simclr=True,\n",
    "    simclr_transforms=invariance_transforms\n",
    "    )\n",
    "\n",
    "# show a few example images are their augmentations\n",
    "_ = dSprites_invariance_torchdataset.show_images(randst=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: How to train for invariance to transformations with a target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "5fe7d554-79f3-4080-f909-25f32c852326"
   },
   "outputs": [],
   "source": [
    "#@title Video 6: Data Transformations\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"WX_PRtohPDQ\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.1: Using image transformations to learn feature invariant representations in an SSL network.\n",
    "\n",
    "We will now investigate the effects of selecting certain transformations compared to others on the invariance learned by an encoder network trained with a **specific type of SSL algorithm, namely SimCLR**. Specifically, we will observe how pre-training an encoder network with SimCLR affects the performance of a classifier trained on the representations the network has learned.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 6.1.1: Complete a SimCLR loss function.\n",
    "\n",
    "The following code:\n",
    "*    lays out the skeleton of a function `custom_simclr_contrastive_loss()` which calculates the contrastive loss for a SimCLR network,\n",
    "*    tests the custom function against the solution implementation,\n",
    "*    trains SimCLR for a few epochs.\n",
    "\n",
    "**Exercise:** \n",
    "*    Complete the `custom_simclr_contrastive_loss()` implementation,\n",
    "*    Plot the loss after training SimCLR with the custom loss function for a few epochs.\n",
    "\n",
    "**Hint**: `custom_simclr_contrastive_loss()` takes 2 input arguments: \n",
    "- proj_feat1 (2D torch Tensor): first set of projected features (batch_size x feat_size)  \n",
    "- proj_feat2 (2D torch Tensor): second set of projected features (batch_size x feat_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_simclr_contrastive_loss(proj_feat1, proj_feat2, temperature=0.5,\n",
    "                                   seed=SEED):\n",
    "  \"\"\"\n",
    "  custom_simclr_contrastive_loss(proj_feat1, proj_feat2)\n",
    "  Returns contrastive loss, given sets of projected features, with positive\n",
    "  pairs matched along the batch dimension.\n",
    "  Required args:\n",
    "  - proj_feat1 (2D torch Tensor): first set of projected features\n",
    "      (batch_size x feat_size)\n",
    "  - proj_feat2 (2D torch Tensor): second set of projected features\n",
    "      (batch_size x feat_size)\n",
    "\n",
    "  Optional args:\n",
    "  - temperature (float): relaxation temperature. (default: 0.5)\n",
    "  Returns:\n",
    "  - loss (float): mean contrastive loss\n",
    "  \"\"\"\n",
    "  # call this before any dataset/network initializing or training,\n",
    "  # to ensure reproducibility\n",
    "  set_seed(seed)\n",
    "  device = proj_feat1.device\n",
    "\n",
    "  if len(proj_feat1) != len(proj_feat2):\n",
    "    raise ValueError(f\"Batch dimension of proj_feat1 ({len(proj_feat1)}) \"\n",
    "                     f\"and proj_feat2 ({len(proj_feat2)}) should be same\")\n",
    "\n",
    "  batch_size = len(proj_feat1) # N\n",
    "  z1 = torch.nn.functional.normalize(proj_feat1, dim=1)\n",
    "  z2 = torch.nn.functional.normalize(proj_feat2, dim=1)\n",
    "\n",
    "  proj_features = torch.cat([z1, z2], dim=0) # 2N x projected feature dimension\n",
    "  similarity_mat = torch.nn.functional.cosine_similarity(\n",
    "      proj_features.unsqueeze(1), proj_features.unsqueeze(0), dim=2\n",
    "      ) # dim: 2N x 2N\n",
    "\n",
    "  # initialize arrays to identify sets of positive and negative examples\n",
    "  pos_sample_indicators = torch.roll(torch.eye(2 * batch_size), batch_size, 1)\n",
    "  neg_sample_indicators = torch.ones(2 * batch_size) - torch.eye(2 * batch_size)\n",
    "\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your function\n",
    "  raise NotImplementedError(\"Exercise: Implement SimCLR loss.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Implement the SimClr loss calculation\n",
    "  # Calculate the numerator of the Loss expression by selecting the appropriate elements from similarity_mat\n",
    "  # Use the pos_sample_indicators tensor\n",
    "  numerator = ...\n",
    "  # Calculate the denominator of the Loss expression by selecting the appropriate elements from similarity_mat\n",
    "  # Use the neg_sample_indicators tensor\n",
    "  denominator = ...\n",
    "\n",
    "  if (denominator < 1e-8).any(): # clamp to avoid division by 0\n",
    "    denominator = torch.clamp(denominator, 1e-8)\n",
    "\n",
    "  loss = torch.mean(-torch.log(numerator / denominator))\n",
    "\n",
    "  return loss\n",
    "\n",
    "\n",
    "## Uncomment below to test your function\n",
    "# test_custom_contrastive_loss_fct(custom_simclr_contrastive_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "0702f81b-bb67-41fa-ce2b-9bdb3baf6b25"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_8dde8bad.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Random seed 2021 has been set.\n",
    "custom_simclr_contrastive_loss() is correctly implemented.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "dc5382c06b88471b82fb22be3652350d",
      "5c892ed7f591479d9d7f68ff35bb3732",
      "9ec00b31654243e0b196efe85a08b1b3",
      "9ee532526e0542f3aac771c32e39c0a5",
      "813da374c1f94e518237eea42aae0672",
      "c1fb86fdc38942a2937674cd6792f86e",
      "fb01cd90552d462d90b0c01b2c9976e8",
      "92a74091e8724f6a96c2a540d290947a"
     ]
    },
    "outputId": "7f786d4b-a782-43bc-f117-87811e202066"
   },
   "outputs": [],
   "source": [
    "# Train SimCLR for a few epochs\n",
    "_, test_simclr_loss_array = models.train_simclr(\n",
    "    encoder=models.EncoderCore(),\n",
    "    dataset=dSprites_invariance_torchdataset,\n",
    "    train_sampler=train_sampler,\n",
    "    num_epochs=5,\n",
    "    loss_fct=custom_simclr_contrastive_loss\n",
    "    )\n",
    "\n",
    "# Plot SimCLR loss over a few epochs.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_simclr_loss_array)\n",
    "ax.set_title(\"SimCLR network loss.\")\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "_ = ax.set_ylabel(\"Training loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given that self-supervised models typically require more training than supervised models, instead of fully pre-training a network here, we will load one that was **pre-trained for 50 epochs**. Again, the **encoder shares the same architecture** as the one used for the supervised, untrained and VAE examples above.\n",
    "\n",
    "The following code:\n",
    "*    load the parameters of a SimCLR network pre-trained on the task SimCLR contrastive task using `load.load_encoder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7925b570-d641-4116-9d63-8b1addffff34"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load SimCLR encoder pre-trained on the contrastive loss\n",
    "simclr_encoder = load.load_encoder(REPO_PATH, model_type=\"simclr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 6.1.2: Evaluating the classification performance of a logistic regression trained on the representations produced by a SimCLR encoder that was pre-trained using different image transformations.\n",
    "\n",
    "For the pre-trained SimCLR encoder, as with the VAE encoder, as the encoder parameters have already been trained, they should be kept frozen while the classifier is trained by setting `freeze_features=True`.   \n",
    "\n",
    "**Exercise:** \n",
    "*     Set a number of epochs for which  to train classifier.  \n",
    "*     Train a classifier, along with the encoder, to classify the input images according to shape, using `models.train_classifier()`.\n",
    "*     Plot the loss array returned when training the model, and update the number of training epochs, if needed. \n",
    "\n",
    "**Hint**: `models.train_classifier()` is introduced in **Interactive Demo 1.2.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simclr(num_epochs, seed):\n",
    "  # call this before any dataset/network initializing or training,\n",
    "  # to ensure reproducibility\n",
    "  set_seed(seed)\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Train a classifer on the pre-trained SimCLR encoder representations.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Train an encoder and classifier on the images, using models.train_classifier()\n",
    "  print(\"Training a classifier on the pre-trained SimCLR encoder representations...\")\n",
    "  _, simclr_loss_array, _, _ = models.train_classifier(\n",
    "      encoder=...,\n",
    "      dataset=...,\n",
    "      train_sampler=...,\n",
    "      test_sampler=...,\n",
    "      freeze_features=..., # keep the encoder frozen while training the classifier\n",
    "      num_epochs=...,\n",
    "      verbose=...\n",
    "      )\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Plot the loss array\")\n",
    "  #################################################\n",
    "  # EXERCISE: Plot the loss array\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(...)\n",
    "  ax.set_title(...)\n",
    "  ax.set_xlabel(...)\n",
    "  ax.set_ylabel(...)\n",
    "\n",
    "\n",
    "# Set a reasonable number of training epochs\n",
    "num_epochs = 25\n",
    "## Uncomment below to test your function\n",
    "# plot_simclr(num_epochs=num_epochs, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564,
     "referenced_widgets": [
      "c76bef0f018d4da9884ab44425cb68e9",
      "a8b663ece30b4697995d095ad6a0f1ec",
      "fdf33f9ce8514cacbe31110bb7e1f6df",
      "2b00589aa7ba488789c1ca6c61176381",
      "f3df6aeeb5e1446fb4b1924deda771e2",
      "1aff5cc234b6435ca1af6a9ade9ae596",
      "a463013d26584ad0a14fdf17b3768d57",
      "e498aabd13ce4c5397f2e255dcfe64e9"
     ]
    },
    "colab_type": "text",
    "outputId": "da0f1d35-0115-4d15-dfa9-cb9b8df7fd2f"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_e26c27bd.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=558 height=413 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_e26c27bd_3.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network (using the transforms proposed above) performs at 92.60% accuracy on the test dataset, after 25 classifier training epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 7: Self-supervised representations learn invariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "47a6dc10-40fa-4f9f-c49d-66c97d6015ab"
   },
   "outputs": [],
   "source": [
    "#@title Video 7: Invariant Representations\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"qtpVm3QCHCk\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.1: The effects of using data transformations on invariance in SimCLR network representations.\n",
    "\n",
    "We now observe the effects of adding our data transformations on the invariance learned by a pre-trained SimCLR network encoder.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 7.1.1: Visualizing the network encoder RSMs, organized along different latent dimensions.\n",
    "\n",
    "We will now compare the pre-trained SimCLR encoder network RSM to some of the previously generated encoder RSMs. \n",
    "\n",
    "**Exercise:** \n",
    "*    Visualize the RSMs for the supervised, VAE and SimCLR network encoders, using `models.plot_model_RSMs()`. \n",
    "*    Visualize the RSMs, organized along different latent dimensions (`\"posX\"`, `\"posY\"`, `\"orientation\"` or `\"scale\"`), and compare the patterns observed for the different encoder networks.\n",
    "\n",
    "**Hint**: `models.plot_model_RSMs()` is introduced in **Interactive Demo 2.1.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rsms2():\n",
    "  sorting_latent = \"shape\"\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Plot RSMs.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Visualize RSMs for the supervised, VAE and SimCLR network encoders.\n",
    "  print(\"Plotting RSMs...\")\n",
    "  _ = models.plot_model_RSMs(\n",
    "      encoders=[..., ..., ...],\n",
    "      dataset=...,\n",
    "      sampler=..., # we want to see the representations on the held out test set\n",
    "      titles=[\"Supervised network encoder RSM\", \"VAE network encoder RSM\",\n",
    "              \"SimCLR network encoder RSM\"], # plot titles\n",
    "      sorting_latent=...\n",
    "      )\n",
    "\n",
    "\n",
    "## Uncomment below to test your code\n",
    "# plot_rsms2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "colab_type": "text",
    "outputId": "a5e004f4-b806-48ae-a581-7796431ab9ea"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_d347f6c3.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=1107 height=376 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_d347f6c3_2.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 7.1.1: What can we conclude about the ability of contrastive models like SimCLR to construct a meaningful representation space?\n",
    "\n",
    "**A.** How do the pre-trained SimCLR encoder RSMs (sorted along different latent dimensions) compare to the supervised and pre-trained VAE encoder network RSMs?  \n",
    "**B.**  What explains these different RSMs?  \n",
    "**C.**  What advantages might some encoders have other others?  \n",
    "**D.**  How might one modify the SimCLR encoder pre-training, for example, if the downstream task were to predict orientation instead of shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses 7.1.1\n",
    "\n",
    "**A.**  \n",
    "**B.**  \n",
    "**C.**  \n",
    "**D.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 8: Avoiding representational collapse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "a09ae942-cb86-4381-c9a4-89cd12348350"
   },
   "outputs": [],
   "source": [
    "#@title Video 8: Avoiding Represntational Collapse\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"OjcFJb6SQ8Q\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.1: The effects of reducing the number of negative examples used in the SimCLR contrastive loss.\n",
    "\n",
    "Typically, a trick used to train neural networks on contrastive losses is to use large batch sizes (here, we use 1,000 examples per batch), and to use the representations of different images in a batch as **each other's negative examples**. So with a batch size of 1,000, each image has one positive paired image (its augmentation), and 1,998 negative paired images (all the other images and their augmentations). This enables the contrastive loss to obtain a good estimate of the full representational distribution.  \n",
    "\n",
    "To observe the consequences of sampling using fewer negative examples in the contrastive loss, we use a pre-trained SimCLR network again. However, this one was pre-trained with a parameter called `neg_pairs` set to `2`. Under the hood, this parameter affects only the contrastive loss calculation, allowing it to use **only 2 of the total available negative pairs in a batch, for each image.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code:\n",
    "*    loads the parameters of a SimCLR network pre-trained on the task SimCLR contrastive task, but with only 2 negative pairs used per image in the loss calculation, using `load.load_encoder()`,\n",
    "*    plots the RSMs of a few network encoders for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8329fedc-9def-4aac-cdee-7dae874e619b"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load SimCLR encoder pre-trained on the contrastive loss\n",
    "simclr_encoder_neg_pairs = load.load_encoder(\n",
    "    REPO_PATH, model_type=\"simclr\", neg_pairs=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 8.1.1: Visualizing the network encoder RSMs, organized along different latent dimensions.\n",
    "\n",
    "We will now compare the RSM for the pre-trained SimCLR encoder  trained with **only 2 negative pairs** to the normal pre-trained SimCLR network encoder and the untrained network encoder. To help us compare the representations learned by the normal and modified SimCLR encoders, we will plot a histogram of the values that makes up both RSMs.\n",
    "\n",
    "**Exercise:** \n",
    "*    Visualize the RSMs for the normal SimCLR, 2-neg-pair SimCLR and untrained network encoders, using `models.plot_model_RSMs()`. \n",
    "*    Visualize the RSMs, organized along the `shape` latent dimension, and compare the patterns observed for the different encoder networks.\n",
    "*    Plot a histogram of RSM values for the normal and 2-neg-pair SimCLR network encoders.  \n",
    "\n",
    "**Hint**: \n",
    "*    `models.plot_model_RSMs()` is introduced in **Interactive Demo 2.1.1**.\n",
    "*    `models.plot_model_RSMs()` returns the **data matrices** calculated for each encoder's RSM, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_plot():\n",
    "  sorting_latent = \"shape\" # Exercise: Try sorting by different latent dimensions\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Plot RSMs.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Visualize RSMs for the normal SimCLR, 2-neg-pair SimCLR and untrained network encoders.\n",
    "  print(\"Plotting RSMs...\")\n",
    "  simclr_rsm, simclr_neg_pairs_rsm, untrained_rsm = models.plot_model_RSMs(\n",
    "      encoders=[..., ..., ...],\n",
    "      dataset=...,\n",
    "      sampler=..., # we want to see the representations on the held out test set\n",
    "      titles=[\"SimCLR network encoder RSM\",\n",
    "              f\"SimCLR network encoder RSM\\n(2 negative pairs per image used in loss calc.)\",\n",
    "              \"Untrained network encoder RSM\"], # plot titles\n",
    "      sorting_latent=...\n",
    "      )\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Plot histogram.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Plot a histogram of RSM values for both encoders.\n",
    "  plot_rsm_histogram(\n",
    "      [..., ...],\n",
    "      colors=[...],\n",
    "      labels=[..., ...],\n",
    "      nbins=100\n",
    "      )\n",
    "\n",
    "\n",
    "## Uncommnet below to test your code\n",
    "# sorting_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858
    },
    "colab_type": "text",
    "outputId": "c7c9efe9-755e-45d8-d025-4e260bc45f3c"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_47bcb021.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=1125 height=376 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_47bcb021_2.png>\n",
    "\n",
    "<img alt='Solution hint' align='left' width=558 height=414 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_47bcb021_3.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 8.1.1: What can we conclude about the importance of negative pairs in computing the contrastive loss for models like SimCLR?\n",
    "\n",
    "**A.**  How does changing the number of negative pairs affect the networks' RSMs?  \n",
    "**B.**  How is the shape classifier likely to perform when the encoder is pre-trained with very few negative pairs?  \n",
    "**C.**  What, intuitively, is the role of negative pairs in shaping the representational space that a contrastive model learns, and how does this role relate to the role of positive pairs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses 8.1.1\n",
    "**A.**  \n",
    "**B.**  \n",
    "**C.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 8.1.2: Evaluating the classification performance of a logistic regression trained on the representations produced by the SimCLR network encoder pre-trained with only a few negative pairs.\n",
    "\n",
    "For the 2-neg-pair SimCLR encoder, as the encoder parameters have already been trained, they should again be kept frozen while the classifier is trained by setting `freeze_features=True`.   \n",
    "\n",
    "**Exercise:** \n",
    "*     Set a number of epochs for which  to train classifier.  \n",
    "*     Train a classifier, along with the encoder, to classify the input images according to shape, using `models.train_classifier()`.\n",
    "*     Plot the loss array returned when training the model, and update the number of training epochs, if needed. \n",
    "\n",
    "**Hint**: `models.train_classifier()` is introduced in **Interactive Demo 1.2.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_simclr(num_epochs, seed):\n",
    "  # call this before any dataset/network initializing or training,\n",
    "  # to ensure reproducibility\n",
    "  set_seed(seed)\n",
    "  print(\"Training a classifier on the representations learned by the SimCLR \"\n",
    "        \"network encoder pre-trained\\nusing only 2 negative pairs per image \"\n",
    "        \"for the loss calculation...\")\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your implementation\n",
    "  raise NotImplementedError(\"Exercise: Train a classifer on the pre-trained few negative pairs SimCLR encoder representations.\")\n",
    "  #################################################\n",
    "  # EXERCISE: Train an encoder and classifier on the images, using models.train_classifier()\n",
    "  _, simclr_neg_pairs_loss_array, _, _ = models.train_classifier(\n",
    "    encoder=simclr_encoder_neg_pairs,\n",
    "    dataset=dSprites_torchdataset,\n",
    "    train_sampler=train_sampler,\n",
    "    test_sampler=test_sampler,\n",
    "    freeze_features=True, # keep the encoder frozen while training the classifier\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "  # Plot the loss array\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(simclr_neg_pairs_loss_array)\n",
    "  ax.set_title((\"Loss of classifier trained on a SimCLR encoder\\n\"\n",
    "  \"trained with 2 negative pairs only.\"))\n",
    "  ax.set_xlabel(\"Epoch number\")\n",
    "  _ = ax.set_ylabel(\"Training loss\")\n",
    "\n",
    "  return simclr_neg_pairs_loss_array\n",
    "\n",
    "\n",
    "# Set a reasonable number of training epochs\n",
    "num_epochs = 50\n",
    "## Uncomment below to test your code\n",
    "# _ = train_plot_simclr(num_epochs=num_epochs, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581,
     "referenced_widgets": [
      "ddc5a822bb5245fa9d14a42883e31fa7",
      "5dd72803ab56476c9c51b81a9e645a30",
      "0e14dec434134d11b5704e70a0cf0163",
      "938649a48ea94419b7334105bd5f4200",
      "c03aba70411d49e78e7652b4a3744bf9",
      "055b9613349245cfa5cb28098c9f4545",
      "abc2e5895ff5405da4f997dafe5f6388",
      "1264fc5d44894e39acb7b7b937c33237"
     ]
    },
    "colab_type": "text",
    "outputId": "381365a1-d591-49e7-c2ed-babff3261e42"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_dbd8d3f3.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=558 height=413 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_dbd8d3f3_3.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the number of negative pairs used per image in pre-training a SimCLR encoder, classification accuracy drops to 56.27% on the test dataset, after 50 classifier training epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 9: Good representations enable few-shot learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "cf0bbaf9-343c-4789-d195-4e5ab847d19c"
   },
   "outputs": [],
   "source": [
    "#@title Video 9: Few-shot Learning\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"J1X91oXpvF0\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.1: The benefits of pre-training an encoder network in a few-short learning scenario, i.e., when only few labelled examples are available.\n",
    "\n",
    "The toy dataset we have been using, **dSprites**, is thoroughly labelled along 5 different dimensions. However, this is not the case for many datasets. Some very large datasets may have few if any labels.\n",
    "\n",
    "One of our last steps is to examine how each of our models perform in such a case when only few labelled images are available for training. In this scenario, we will train classifiers on different fractions of the training data (between 0.01 and 1.0), and see how they perform on the test set. \n",
    "\n",
    "For the difference types of encoder, this means:\n",
    "*    **Supervised encoder:** As the supervised encoder can only be trained with labels, we will start from untrained encoders and train them end-to-end on the classification task with the fraction of labelled images allowed.  \n",
    "_N.B.: Given that that network is trained end-to-end, we will train it for more epochs, and mark it with \"\\*\" in the graphs._\n",
    "*    **Untrained encoder:** By definition, the untrained encoder is untrained.\n",
    "*    **VAE encoder**: As a generative model can be pre-trained on unlabelled data, we will use the VAE encoder pre-trained on the reconstruction task using the full dataset, before training the classifier layer with the fraction of labelled images allowed.\n",
    "*    **SimCLR encoder**: As an SSL model can be pre-trained on unlabelled data, we will use the SimCLR encoder pre-trained on the contrastive task using the full dataset, before training the classifier layer with the fraction of labelled images allowed.\n",
    "\n",
    "_N.B.: The numbers of epochs are specified below for when the **full training dataset** is used. For each fraction of the dataset a classifier is trained on, the **number of training epochs is scaled up** to compensate for the drop in number of training examples. For example, if we specify 10 epochs for a model, the 0.1 fraction labelled classifier will be trained over ~30 epochs. Also, we use **slightly fewer epochs** than above, here, in the interest of time._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo 9.1.1: Training classifiers on different encoders, using only a fraction of the full labelled dataset.\n",
    "\n",
    "In this demo, we select a few fractions (5 or 6) of the full labelled dataset with which to train classifiers.\n",
    "\n",
    "**Interactive Demo:** Set `labelled_fractions` argument to a list of fractions (5 or 6 values between 0.01 and 1.0) with which to train classifiers for each encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711,
     "referenced_widgets": [
      "9a27751ee37243ab9f16147f6c42ec3b",
      "86b0c0374e4c48f9bf946def8ad2c38c",
      "534d3438669247afbde6470da3f12c1e",
      "215cbb3645bc43faaa44cf50bd059574",
      "d31def8c3dfb46258b6d29a2cb1f905d",
      "e5eb30849a29424ba4ad243eb9de63ed",
      "f11804ab392a43d1b974d06ab8cb94d8",
      "7be69f4cbecc4b2b88918e76c305c936",
      "1aae21c2effe43b4ab22ffdc3bfdd147",
      "a2afc1a3894046eea25447e66b981952",
      "648920b296fd4726b5f440a9aaca73b4",
      "01e1a83dc958423faf64a6839339d974",
      "95a6a1d5e365492193fb2073e4761487",
      "58757dd4f0bc48f2bc41bb8f6be2b7b3",
      "2eeed59ec8bb4a9d944b685cb09b68a6",
      "9bfb6506d89c49ec8aaaa10db0c99620",
      "2f06efef1a524984a4fd8e801f1ac40a",
      "e536c2640a3e47f8a613d84ff39b62f4",
      "1ad96fe2b72b4408b4d6b18d854be810",
      "07cf2cc12e6a49578fbbb2ab7f3d48c7",
      "060bcf88ae844c908daf7e5e3c54f876",
      "9e339d1fd0c74c00b3a9cc514c76b263",
      "0baadd427e2a4308b3f298d7f5645dbb",
      "73e78c3984934ca1b0fbc52179a71a0f",
      "d5b5e7e32ab14c5897ad06174f533c33",
      "9044f35b282c436fa6c2e14367854bef",
      "e422319e8103496e94afd728043c75a8",
      "c1ae8563071c4a41aaa6f56457ed6bf4",
      "06c51ce44475494f96aac6baf5c53163",
      "5f5b03903eb44c4bb73cfd5d2b79b5d6",
      "e42ac03b84ca443fb9a89126d2d3d883",
      "c4df4975b24f401da6a9b48d350ed31d"
     ]
    },
    "outputId": "0bd99f27-68c4-41c3-cdee-47a3169479be"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "new_supervised_encoder = models.EncoderCore() # new, untrained supervised encoder\n",
    "\n",
    "_ = models.train_encoder_clfs_by_fraction_labelled(\n",
    "    encoders=[new_supervised_encoder, untrained_encoder, vae_encoder, simclr_encoder],\n",
    "    dataset=dSprites_torchdataset,\n",
    "    train_sampler=train_sampler,\n",
    "    test_sampler=test_sampler,\n",
    "    labelled_fractions=[0.01], # DEMO: select fractions to run\n",
    "    num_epochs=[20, 8, 8, 8], # train the supervised network (end-to-end) for more epochs\n",
    "    freeze_features=[False, True, True, True], # only train new supervised network end-to-end\n",
    "    subset_seed=SEED,\n",
    "    encoder_labels=[\"supervised\", \"untrained\", \"VAE\", \"SimCLR\"],\n",
    "    title=\"Performance of classifiers trained\\nwith different network encoders\",\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 9.1.1: What can we conclude the advantages and disadvantages of the different encoder network types under different conditions?\n",
    "\n",
    "**A.** Which models are most and least affected by how much labelled data is available?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses 9.1.1\n",
    "**A.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 10: Ethical considerations for self-supervised learning from biased datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "954f3794-1e63-4b42-ff1e-c992a2b5dd97"
   },
   "outputs": [],
   "source": [
    "#@title Video 10: Biased Datasets\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"CkbSSMOQFcU\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10.1: The consequences of training models on biased datasets.\n",
    "\n",
    "If a model is trained on a biased dataset, it is likely to learn a representational encoding that reproduces these biases, impairing its ability to generalize properly and increasing the likelihood that it will propagate these biases forward.  \n",
    "\n",
    "Here, we investigate the effects of training the models on a biased subset of the training dataset. Specifically, we introduce a `train_sampler_biased`, a training dataset sampler that only samples:\n",
    "*    **squares**, if they are centered on the **lefthand** side of an image **(posX: 0 to 0.3)**,\n",
    "*    **ovals**, if they are centered in the **center** of an image **(posX: 0.35 to 0.65)**, \n",
    "*    **hearts**, if they are centered on the **righthand** side of am image **(posX: 0.7 to 1.0)**.\n",
    "\n",
    "This sampling bias introduces a correlation between `shape` and `posX` that does not exist in the original dataset.\n",
    "\n",
    "We then train each model as above on the dataset, and observe their performance when tested on an unbiased dataset.\n",
    "\n",
    "*N.B.: This biased sampling also significantly reduces the size of the training dataset available (approximately 6x). Thus, it would not be fair to compare our results here to those obtained previously in the tutorial, when we were using the full dataset. For this reason, **as a control, we will also separately train the models with `train_sampler_bias_ctrl`**, a training dataset sampler that does not share the same sampling bias as `train_sampler_biased`, but can only sample as many samples as `train_sampler_biased` can.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5510799f-7ebe-419e-cf30-c366cb2359df"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "bias_type = \"shape_posX_spaced\" # name of this bias\n",
    "\n",
    "# initialize a biased training sampler and an unbiased test sampler\n",
    "train_sampler_biased, test_sampler_for_biased = data.train_test_split_idx(\n",
    "    dSprites_torchdataset,\n",
    "    fraction_train=0.95, # 95:5 split to partially compensate for loss of training examples due to bias\n",
    "    randst=SEED,\n",
    "    train_bias=bias_type\n",
    "    )\n",
    "\n",
    "# initialize a control, unbiased training sampler and an unbiased test sampler\n",
    "train_sampler_bias_ctrl, test_sampler_for_bias_ctrl = data.train_test_split_idx(\n",
    "    dSprites_torchdataset,\n",
    "    fraction_train=0.95,\n",
    "    randst=SEED,\n",
    "    train_bias=bias_type,\n",
    "    control = True\n",
    "    )\n",
    "\n",
    "print(f\"Biased dataset: {len(train_sampler_biased)} training, \"\n",
    "      f\"{len(test_sampler_for_biased)} test images\")\n",
    "print(f\"Bias control dataset: {len(train_sampler_bias_ctrl)} training, \"\n",
    "      f\"{len(test_sampler_for_bias_ctrl)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot some images sampled with `train_sampler_biased` to observe the pattern described above where `shape` and `posX` are now correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "outputId": "ffdea292-8053-4d82-a6f1-4d0cc0fc3665"
   },
   "outputs": [],
   "source": [
    "print(\"Plotting sample images from the biased training dataset.\\n\")\n",
    "dSprites.show_images(indices=train_sampler_biased.indices[:20], randst=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot some images sampled with `train_sampler_bias_ctrl` to verify visually that this biased pattern does not appear in the control dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "outputId": "6193a6ba-0f58-4846-97cc-679a0a0444ba"
   },
   "outputs": [],
   "source": [
    "print(\"Plotting sample images from the bias control training dataset.\\n\")\n",
    "dSprites.show_images(indices=train_sampler_bias_ctrl.indices[:20], randst=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @markdown Function to run full training procedure (from initializing and\n",
    "# @markdown pretraining encoders to training classifiers):\n",
    "# @markdown `full_training_procedure(train_sampler, test_sampler)`\n",
    "\n",
    "def full_training_procedure(train_sampler, test_sampler, title=None,\n",
    "                            dataset_type=\"biased\", verbose=True):\n",
    "\n",
    "    if dataset_type not in [\"biased\", \"bias_ctrl\"]:\n",
    "        raise ValueError(\"Expected model_type to be 'biased' or 'bias_ctrl', \"\n",
    "          f\"but found {model_type}.\")\n",
    "\n",
    "    supervised_encoder = models.EncoderCore()\n",
    "    untrained_encoder = models.EncoderCore()\n",
    "\n",
    "    # Load pre-trained VAE encoder\n",
    "    vae_encoder = load.load_encoder(\n",
    "        REPO_PATH, model_type=\"vae\", dataset_type=dataset_type,\n",
    "        verbose=verbose\n",
    "        )\n",
    "\n",
    "    # Load pre-trained SimCLR encoder\n",
    "    simclr_encoder = load.load_encoder(\n",
    "        REPO_PATH, model_type=\"simclr\", dataset_type=dataset_type,\n",
    "        verbose=verbose\n",
    "        )\n",
    "\n",
    "    encoders = [supervised_encoder, untrained_encoder, vae_encoder, simclr_encoder]\n",
    "    freeze_features = [False, True, True, True]\n",
    "    encoder_labels = [\"supervised\", \"untrained\", \"VAE\", \"SimCLR\"]\n",
    "\n",
    "    num_clf_epochs = [80, 30, 30, 30]\n",
    "    print(f\"\\nTraining supervised encoder and classifier for {num_clf_epochs[0]}+ \"\n",
    "      f\"epochs, and all other classifiers for {num_clf_epochs[1]}+ epochs each.\")\n",
    "    _ = models.train_encoder_clfs_by_fraction_labelled(\n",
    "        encoders=encoders,\n",
    "        dataset=dSprites_torchdataset,\n",
    "        train_sampler=train_sampler,\n",
    "        test_sampler=test_sampler,\n",
    "        num_epochs=num_clf_epochs,\n",
    "        freeze_features=freeze_features,\n",
    "        subset_seed=SEED,\n",
    "        encoder_labels=encoder_labels,\n",
    "        title=title,\n",
    "        verbose=verbose\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the same analyses as in **Section 9**, but with our **biased training data sampler** (and unbiased control sampler) to observe how the different models perform. Because the dataset is much smaller, we increase the number of pre-trained and training epochs for the encoders and classifiers.\n",
    "\n",
    "Let us start with our **unbiased control sampler**, to get a sense of the classification performance levels we should expect with a dataset this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949,
     "referenced_widgets": [
      "14d06b56388c40c3bde7be410719284a",
      "ed72bad8379546c29800911aa0e73c2f",
      "394e66bd26e3436f8f331d993ef87e76",
      "aa34b882017e435799a81e15368b6e36",
      "4a313d63f37b4ee5a435f891f809b700",
      "1e722a7d112047cdac4271a2bef50d40",
      "f4370518a1da46bc85982f640ed57726",
      "7ab5d66d88364fbd8ff3b12a45d98ec6",
      "227439a29a4343bfa64962f6cb2fcbdd",
      "390cd0a4f57f4007859564c321007315",
      "5429ed2234b74f2ca2d60dbfce4eb6cd",
      "ee3820f9b4674866b8207543ce18ce36",
      "a991c7e1d4e644c0ba3170e2e896eeb3",
      "b7496a05e4bd4e8cba8c0f91b7e2cd86",
      "0b20618413764ffb861de540127835a4",
      "2f9300ebfebb47bdb83a58ebe4d19729",
      "6270408bb8a84fb3884e61623d2f8120",
      "eb43f71233e54dbba2b52c21073c0b63",
      "2d5250463c5546eb8da94dd28c052d23",
      "9335f4d9b5cd4bfdaad77ae3a9321015",
      "01c9471fabaa42ad8473a15adaa91f30",
      "ed3f1988305940d28bdc3ab6b471e96c",
      "92c41f4b92e44925bc4470e54354bd2a",
      "b086da2257a3429ebc27b36a9f758c42",
      "2773433f7fea4f3c8ac449827dcbe37c",
      "0d6d42cc686e4388b25d22809830ce3f",
      "f135b08841504a8c81a515b717c7a670",
      "38021e523b844aba9870607d205bf1c5",
      "80c4f13c4b9c49f39980a4a44d6618dd",
      "89c595a35adb4aef8dcff7954b003716",
      "41ecf7a05a0a478284940c9e35cdf448",
      "8a54fed398d44514985e4a8f56cf687e"
     ]
    },
    "outputId": "a709e521-17d9-4ef3-db39-294db2592b21"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Training all models using the control, unbiased training dataset\\n\")\n",
    "full_training_procedure(\n",
    "    train_sampler_bias_ctrl, test_sampler_for_bias_ctrl,\n",
    "    title=\"Classifier performances with control, unbiased training dataset\",\n",
    "    dataset_type=\"bias_ctrl\" # for loading correct pre-trained networks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar pattern is observed here as with the full dataset, though notably most performances are a bit weaker, likely due to us (A) using a smaller training dataset, and (B) training and pre-training for fewer iterations, considering to the dataset size, for time-efficiency reasons.\n",
    "\n",
    "Using the same parameters, we now repeat the analysis with the **biased** training data sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949,
     "referenced_widgets": [
      "839b95e4c27346a6b3efd14b1f48d644",
      "06b891cf15a447b08d396460d011b097",
      "5f28e451cdeb44b8894d2db6813eabea",
      "7bcfe2da6e1e4cc5b08393a250abc8a0",
      "ddc1584348b04523b9cdf32f0ff1d7b4",
      "d720b59259874d69aa03f971ba21bfd7",
      "6d5cece9d8c148ae81805de325a43138",
      "90467267cf6e44ee9ad843e15882adf4",
      "df93443f6d1e4185b6a5851609168f9c",
      "e947f4d4338e45f9938b7535b1d11484",
      "12d8e725caa842b392c9f29f5d27bbb2",
      "12e31dfda244484ab7630f9954e63b10",
      "bf2a296ba47b4263bdfe5007977bbb5f",
      "3bfcace8aad14d8e9fb005d534d7dab8",
      "88347103aa054703a5714d8463719227",
      "b09ba10ee72046d68418982f06bde6e5",
      "45d2825fa16d44859d89b7d4f8c86d29",
      "831372e1a0e44f1a854c2f4d0e82f66d",
      "96d5ce2e395b49e1963d8e26451982cb",
      "02a58580adb64e8b8515c951ebf2a522",
      "dcc09e6a53144d0fa0c8dee2881c23a4",
      "d9ac4fc9fd42481da4d264241eb61509",
      "90b965a9ae0a465b807c7e584f097139",
      "6a10e66da33b402bb12bf9d3c6aa4f27",
      "3109b945bc2c4732aa98d17d802b7052",
      "3213c25a90ec44a0a82f69c2e1284d6c",
      "565eef899e27433391a7bec867daafe2",
      "a8eae71769924ccdb1f2fcd9eff90676",
      "456a6a9af7124ffb97e75f2f3c8e28ea",
      "a80b7ef75e0f4437b88cf7c047e22292",
      "19882c3525734ff5b56bd2bbd8be5b56",
      "700db39d1997471bb39de2dc8c4130fd"
     ]
    },
    "outputId": "9dcd5781-0ff4-43bf-edd0-a38851b55bd9"
   },
   "outputs": [],
   "source": [
    "# call this before any dataset/network initializing or training,\n",
    "# to ensure reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Training all models using the biased training dataset\\n\")\n",
    "full_training_procedure(\n",
    "    train_sampler_biased, test_sampler_for_biased,\n",
    "    title=\"Classifier performances with biased training dataset\",\n",
    "    dataset_type=\"biased\" # for loading correct pre-trained networks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the SimCLR network encoder is not only the only network to perform well, it even outperforms its control performance (which uses the same test dataset), at least with this particular dataset and biasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 10.1.1: How do different models cope with a biased training dataset?\n",
    "\n",
    "**A.** Which models are most and least affected by the biased training dataset?  \n",
    "**B.** Which types of images in the test set are most likely causing the observed drop in performance?  \n",
    "**C.** Why might certain models be more robust to the bias introduced here than others?  \n",
    "**D.** What are some methods we can employ to help mitigate the negative effects of biases in our training sets on our ability to learn good data representations with our models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses 10.1.1\n",
    "\n",
    "**A.**   \n",
    "**B.**   \n",
    "**C.**   \n",
    "**D.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 10.1.2: How do these principles apply more generally?\n",
    "\n",
    "We have seen now how self-supervised learning (SSL) can improve a network's ability to learn good representations of data. For the purposes of this tutorial, we presented examples with a simplified dataset: the dSprites dataset, where we know: \n",
    "(1) the latent dimensions for all images,  \n",
    "(2) the joint probability distribution across latent dimensions for the full dataset, and  \n",
    "(3) the precise nature of the bias introduced into our biased dataset **(Section 10)**. \n",
    "\n",
    "In real-world applications, with more complex or difficult datasets,   \n",
    "**A.** What principles can we draw on to successfully apply SSL to learn good data representations?  \n",
    "**B.** What challenges might we face, compared to applying SSL to dSprites?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses 10.1.2\n",
    "\n",
    "**A.**   \n",
    "**B.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 11: Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "07a9b15b-a83d-422f-dec5-f5466e8cf09e"
   },
   "outputs": [],
   "source": [
    "#@title Video 10: Conclusion\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"G41LkC1cQ0Y\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W3D1_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
