{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/fix-w2d3t2/tutorials/W2D3_ModernRecurrentNeuralNetworks/W2D3_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "gUb98iJdpXUk"
      },
      "source": [
        "# Tutorial 2: Modern RNNs and their variants\n",
        "\n",
        "**Week 2, Day 3: Modern RNNs**\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Bhargav Srinivasa Desikan, Anis Zahedifard, James Evans\n",
        "\n",
        "__Content reviewers:__ Lily Cheng, Melvin Selim Atay, Ezekiel Williams, Kelson Shilling-Scrivo\n",
        "\n",
        "__Content editors:__ Gagana B, Spiros Chavlis\n",
        "\n",
        "__Production editors:__ Roberto Guidotti, Gagana B, Spiros Chavlis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "31vi0zszpXUl"
      },
      "source": [
        "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
        "\n",
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "3FGSZ8SRpXUl"
      },
      "source": [
        "---\n",
        "# Tutorial objectives\n",
        "\n",
        "In this tutorial you will learn about:\n",
        "1. Modern Recurrent Neural Networks and their use\n",
        "2. Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) and the memory cell\n",
        "3. Sequence to Sequence and Encoder-Decoder Networks\n",
        "4. Models of Attention for text classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "VAuBe4FnpXUm",
        "outputId": "bcf27d02-cc49-4dd4-f606-e598df0d5229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f6dbe24ea10>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/n23hy/?direct%26mode=render%26action=download%26mode=render\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# @title Tutorial slides\n",
        "\n",
        "from IPython.display import IFrame\n",
        "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/n23hy/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-MDSGSC3pXUm"
      },
      "source": [
        "These are the slides for the videos in this tutorials. If you want to locally download the slides, click [here](https://osf.io/n23hy/download)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UPEcYORrpXUm"
      },
      "source": [
        "---\n",
        "# Setup\n",
        "\n",
        "We will use the IMDB dataset, which consists of a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. We will use torchtext to download the dataset and prepare it for training, validation and testing. Our goal is to build a model that performs binary classification between positive and negative movie reviews.\n",
        "\n",
        "We use `fix_length` argument to pad sentences of length less than `sentence_length` or truncate sentences of length greater than `sentence_length`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "k22Actr6pXUn",
        "outputId": "10077a11-942f-43f8-bf61-8b20fc48ba54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 735.5 MB 14 kB/s \n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 48.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 55.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 23.8 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d2l==0.16.5\n",
            "  Downloading d2l-0.16.5-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.5) (1.3.5)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.5) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.5) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.5) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.5) (1.21.6)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.5) (5.3.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.5) (7.7.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.5) (5.3.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.5) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.5) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.5) (5.6.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.16.5) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.16.5) (5.1.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.16.5) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.16.5) (5.3.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.5) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.5) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.5) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.5) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.5) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.5) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.5) (1.0.18)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.5) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.5) (0.2.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.16.5) (5.4.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.16.5) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.16.5) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.16.5) (1.1.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.16.5) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.16.5) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.16.5) (4.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.16.5) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.16.5) (4.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.16.5) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.16.5) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.16.5) (5.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.16.5) (3.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.16.5) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.16.5) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.16.5) (1.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==0.16.5) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==0.16.5) (23.0.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.16.5) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l==0.16.5) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.16.5) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.16.5) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.16.5) (0.11.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.5) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.5) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.5) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.5) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.5) (5.0.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.5) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==0.16.5) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==0.16.5) (2022.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l==0.16.5) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->d2l==0.16.5) (21.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.16.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.16.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.16.5) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.16.5) (1.24.3)\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.16.5\n",
            "  Building wheel for evaltools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# @title Install dependecies\n",
        "\n",
        "# @markdown There may be *errors* and/or *warnings* reported during the installation. However, they are to be ignored.\n",
        "!pip install torch==1.8.0 torchtext==0.9.0 torchaudio==0.8.0 torchvision==0.9.0 --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install d2l==0.16.5\n",
        "\n",
        "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
        "from evaltools.airtable import AirtableForm\n",
        "\n",
        "atform = AirtableForm('appn7VdPRseSoMXEG','W2D3_T2','https://portal.neuromatchacademy.org/api/redirect/to/3412a777-eb0e-4312-9254-eec266f0bee4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {},
        "id": "t0MzJlvZpXUn"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import math\n",
        "import time\n",
        "import nltk\n",
        "import random\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "C5Hg_5XipXUn"
      },
      "outputs": [],
      "source": [
        "# @title Figure Settings\n",
        "import ipywidgets as widgets\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "jXKixUzRpXUn",
        "outputId": "e9a0adb9-cfde-4783-cfd8-5e607314a4b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# @title Download the dataset\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "nltk.download('webtext')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "3e33eb9YpXUo"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions\n",
        "\n",
        "def plot_train_val(x, train, val, train_label,\n",
        "                   val_label, title, y_label,\n",
        "                   color):\n",
        "  \"\"\"\n",
        "  Plots training/validation performance per epoch\n",
        "\n",
        "  Args:\n",
        "    x: np.ndarray\n",
        "      Input data\n",
        "    train: list\n",
        "      Training data performance\n",
        "    val: list\n",
        "      Validation data performance\n",
        "    train_label: string\n",
        "      Train Label [specifies training criterion]\n",
        "    color: string\n",
        "      Specifies color of plot\n",
        "    val_label: string\n",
        "      Validation Label [specifies validation criterion]\n",
        "    title: string\n",
        "      Specifies title of plot\n",
        "    y_label: string\n",
        "      Specifies performance criterion\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  plt.plot(x, train, label=train_label, color=color)\n",
        "  plt.plot(x, val, label=val_label, color=color, linestyle='--')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel(y_label)\n",
        "  plt.title(title)\n",
        "\n",
        "def count_parameters(model):\n",
        "  \"\"\"\n",
        "  Helper function to count parameters\n",
        "\n",
        "  Args:\n",
        "    model: nn.module\n",
        "      NeuralNet instance\n",
        "\n",
        "  Returns:\n",
        "    parameters: int\n",
        "      Number of parameters in model\n",
        "  \"\"\"\n",
        "  parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  return parameters\n",
        "\n",
        "def init_weights(m):\n",
        "  \"\"\"\n",
        "  Helper function to initialize weights\n",
        "\n",
        "  Args:\n",
        "    m: nn.module\n",
        "      Type of layer\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  if type(m) in (nn.Linear, nn.Conv1d):\n",
        "    nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "def load_dataset(sentence_length=50, batch_size=32, seed=2021):\n",
        "  \"\"\"\n",
        "  Dataset Loader\n",
        "\n",
        "  Args:\n",
        "    sentence_length: int\n",
        "      Length of sentence\n",
        "    seed: int\n",
        "      Set seed for reproducibility\n",
        "    batch_size: int\n",
        "      Batch size\n",
        "\n",
        "  Returns:\n",
        "    TEXT: Field instance\n",
        "      Text\n",
        "    vocab_size: int\n",
        "      Specifies size of TEXT\n",
        "    train_iter: BucketIterator\n",
        "      Training iterator\n",
        "    valid_iter: BucketIterator\n",
        "      Validation iterator\n",
        "    test_iter: BucketIterator\n",
        "      Test iterator\n",
        "  \"\"\"\n",
        "  TEXT = data.Field(sequential=True,\n",
        "                    tokenize=nltk.word_tokenize,\n",
        "                    lower=True,\n",
        "                    include_lengths=True,\n",
        "                    batch_first=True,\n",
        "                    fix_length=sentence_length)\n",
        "  LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "  train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "  # If no specific vector embeddings are specified,\n",
        "  # Torchtext initializes random vector embeddings\n",
        "  # which would get updated during training through backpropagation.\n",
        "  TEXT.build_vocab(train_data)\n",
        "  LABEL.build_vocab(train_data)\n",
        "\n",
        "  train_data, valid_data = train_data.split(split_ratio=0.7,\n",
        "                                            random_state=random.seed(seed))\n",
        "  train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                                                  batch_size=batch_size, sort_key=lambda x: len(x.text),\n",
        "                                                                  repeat=False, shuffle=True)\n",
        "  vocab_size = len(TEXT.vocab)\n",
        "\n",
        "  print(f\"Data loading is completed. Sentence length: {sentence_length}, \"\n",
        "        f\"Batch size: {batch_size}, and seed: {seed}\")\n",
        "\n",
        "  return TEXT, vocab_size, train_iter, valid_iter, test_iter\n",
        "\n",
        "\n",
        "def text_from_dict(arr, dictionary):\n",
        "  \"\"\"\n",
        "  Helper function to extract text from dictionary\n",
        "\n",
        "  Args:\n",
        "    dictionary: dict\n",
        "      Dictionary of words and corresponding indices\n",
        "    arr: list\n",
        "      Sequence of words\n",
        "\n",
        "  Returns:\n",
        "    text: list\n",
        "      Log of keys from dictionary\n",
        "  \"\"\"\n",
        "  text = []\n",
        "  for element in arr:\n",
        "    text.append(dictionary[element])\n",
        "  return text\n",
        "\n",
        "def view_data(TEXT, train_iter):\n",
        "  \"\"\"\n",
        "  Helper function to view data\n",
        "\n",
        "  Args:\n",
        "    TEXT: Field instance\n",
        "      Text\n",
        "    train_iter: BucketIterator\n",
        "      Training iterator\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  for idx, batch in enumerate(train_iter):\n",
        "    text = batch.text[0]\n",
        "    target = batch.label\n",
        "\n",
        "    for itr in range(25, 30):\n",
        "      print('Review: ', ' '.join(text_from_dict(text[itr], TEXT.vocab.itos)))\n",
        "      print('Label: ', int(target[itr].item()), '\\n')\n",
        "\n",
        "    print('[0: Negative Review, 1: Positive Review]')\n",
        "    if idx==0:\n",
        "      break\n",
        "\n",
        "\n",
        "def train(model, device, train_iter, valid_iter,\n",
        "          epochs, learning_rate):\n",
        "  \"\"\"\n",
        "  Training function\n",
        "\n",
        "  Args:\n",
        "    model: nn.module\n",
        "      NeuralNet instance\n",
        "    device: string\n",
        "      GPU if available, CPU otherwise\n",
        "    epochs: int\n",
        "      Number of epochs to train model for\n",
        "    learning_rate: float\n",
        "      Learning rate\n",
        "    train_iter: BucketIterator\n",
        "      Training iterator\n",
        "    valid_iter: BucketIterator\n",
        "      Validation iterator\n",
        "\n",
        "  Returns:\n",
        "    train_loss: list\n",
        "      Log of training loss\n",
        "    validation_loss: list\n",
        "      Log of validation loss\n",
        "    train_acc: list\n",
        "      Log of training accuracy\n",
        "    validation_acc: list\n",
        "      Log of validation accuracy\n",
        "  \"\"\"\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  train_loss, validation_loss = [], []\n",
        "  train_acc, validation_acc = [], []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # Train\n",
        "    model.train()\n",
        "    running_loss = 0.\n",
        "    correct, total = 0, 0\n",
        "    steps = 0\n",
        "\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "      text = batch.text[0]\n",
        "      target = batch.label\n",
        "      target = torch.autograd.Variable(target).long()\n",
        "      text, target = text.to(device), target.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = model(text)\n",
        "\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      steps += 1\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # Get accuracy\n",
        "      _, predicted = torch.max(output, 1)\n",
        "      total += target.size(0)\n",
        "      correct += (predicted == target).sum().item()\n",
        "\n",
        "    train_loss.append(running_loss/len(train_iter))\n",
        "    train_acc.append(correct/total)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}, '\n",
        "          f'Training Loss: {running_loss/len(train_iter):.4f}, '\n",
        "          f'Training Accuracy: {100*correct/total: .2f}%')\n",
        "\n",
        "    # Evaluate on validation data\n",
        "    model.eval()\n",
        "    running_loss = 0.\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for idx, batch in enumerate(valid_iter):\n",
        "        text = batch.text[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        text, target = text.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # get accuracy\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "    validation_loss.append(running_loss/len(valid_iter))\n",
        "    validation_acc.append(correct/total)\n",
        "\n",
        "    print (f'Validation Loss: {running_loss/len(valid_iter):.4f}, '\n",
        "           f'Validation Accuracy: {100*correct/total: .2f}%')\n",
        "\n",
        "  return train_loss, train_acc, validation_loss, validation_acc\n",
        "\n",
        "\n",
        "def test(model, device, test_iter):\n",
        "  \"\"\"\n",
        "  Testing function\n",
        "\n",
        "  Args:\n",
        "    model: nn.module\n",
        "      NeuralNet instance\n",
        "    device: string\n",
        "      GPU if available,\n",
        "    test_iter: BucketIterator\n",
        "      Test iterator\n",
        "\n",
        "  Returns:\n",
        "    acc: float\n",
        "      Test Accuracy\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_iter):\n",
        "      text = batch.text[0]\n",
        "      target = batch.label\n",
        "      target = torch.autograd.Variable(target).long()\n",
        "      text, target = text.to(device), target.to(device)\n",
        "\n",
        "      outputs = model(text)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += target.size(0)\n",
        "      correct += (predicted == target).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "Cg-hbNBGpXUp"
      },
      "outputs": [],
      "source": [
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# For DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness.\n",
        "  NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "xFis3IvvpXUp"
      },
      "outputs": [],
      "source": [
        "# @title Set device (GPU or CPU). Execute `set_device()`\n",
        "# Inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {},
        "id": "1UgdUS5WpXUp",
        "outputId": "24ae412a-25c5-4507-d3a7-e35c4eb1c742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is enabled in this notebook.\n",
            "Random seed 2021 has been set.\n"
          ]
        }
      ],
      "source": [
        "DEVICE = set_device()\n",
        "SEED = 2021\n",
        "set_seed(seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "mzFOvmZIpXUq"
      },
      "source": [
        "---\n",
        "# Section 1: Recurrent Neural Networks (RNNs)\n",
        "\n",
        "*Time estimate: ~27mins*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "aYympc_6pXUq",
        "outputId": "f372cc6c-cd27-4f01-cb2a-42c1070b083e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581,
          "referenced_widgets": [
            "1f9ca39261bc4758a53bf83c39562647",
            "9420890745a842d7b6b29e4a13ce12f0",
            "3ae27b023f1f4b3e8d9717ced0275f19",
            "3e18e895f3db49ea80595ade44b44312",
            "f9604ed2c474411d9f95c88cc78ded72",
            "7f5b9f32614f4ba19535f7042ae85d20"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f9ca39261bc4758a53bf83c39562647"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Video 1: Recurrent Neural Networks\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1Ng41177az\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"hIHocwdyY7M\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 1: Recurrent Neural Networks')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "deor4-wcpXUq"
      },
      "source": [
        "Recurrent neural networks, or RNNs , are a family of neural networks for processing sequential data. Just as a convolutional network is specialized for processing a grid of values X such as an image, a recurrent neural network is specialized for processing a sequence of values. RNNs prove useful in many scenarios where other deep learning models are not effective.\n",
        "\n",
        "* Not all problems can be converted into one with fixed length inputs and outputs.\n",
        "\n",
        "* The deep learning models we have seen so far pick samples randomly. This might not be the best strategy for a task of understanding meaning from a piece of text. Words in a text occur in a sequence and therefore cannot be permuted randomly to get the meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "5cWwhBBPpXUq"
      },
      "source": [
        "The following provides more data than the video (but can be skipped for now). For more detail, see the sources, the [deep learning book](https://www.deeplearningbook.org/contents/rnn.html), and [d2l.ai](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n",
        "\n",
        "When the recurrent network is trained to perform a task that requires predicting the future from the past, the network typically learns to use a hidden state at time step $t$, $H_t$ as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to $t$. This summary is in general necessarily lossy, since it maps an arbitrary length sequence $(X_t, X_{t-1}, X_{t-2}, \\dots, X_{2}, X_{1})$ to a ﬁxed length vector $H_t$.\n",
        "\n",
        "We can represent the unfolded recurrence after $t$ steps with a function $G_t$:\n",
        "\n",
        "\\begin{align}\n",
        "H_t &= G_t(X_t, X_{t-1}, X_{t-2}, \\dots, X_{2}, X_{1}) \\\\\n",
        "&= f(H_{t−1}, X_{t}; \\theta)\n",
        "\\end{align}\n",
        "\n",
        "where $\\theta$ denotes the model parameters, i.e., weights and biases.\n",
        "\n",
        "<figure>\n",
        "  <img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/rnn-2.gif\">\n",
        "  <figcaption>\n",
        "  Source <a href=\"https://blog.floydhub.com/\">blog.floydhub.com</a>\n",
        "  </figcaption>\n",
        "</figure>\n",
        "\n",
        "The function $g_t$ takes the whole past sequence $(X_t, X_{t-1}, X_{t-2}, \\dots , X_{2}, X_{1})$ as input and produces the current state, but the unfolded recurrent structure allows us to factorize $g_t$ into repeated application of a function f. The unfolding process thus introduces two major advantages:\n",
        "\n",
        "* Regardless of the sequence length, the learned model always has the same input size, because it is speciﬁed in terms of transition from one state to another state, rather than speciﬁed in terms of a variable-length history of states.\n",
        "\n",
        "* It is possible to use the same transition function $f$ with the same parameters at every time step.\n",
        "\n",
        "We will now formally write down the equations of a recurrent unit.\n",
        "\n",
        "Assume that we have a minibatch of inputs $X_t \\in R^{n \\times d}$ at time step $t$. In other words, for a minibatch of $n$ sequence examples, each row of $X_t$ corresponds to one example at time step $t$ from the sequence. Next, we denote by $H_t \\in R^{n \\times h}$ the hidden variable of time step $t$. Unlike the MLP, here we save the hidden variable $H_{t-1}$ from the previous time step and introduce a new weight parameter $W_{hh} \\in R^{h \\times h}$ to describe how to use the hidden variable of the previous time step in the current time step. Specifically, the calculation of the hidden variable of the current time step is determined by the input of the current time step together with the hidden variable of the previous time step:\n",
        "\n",
        "\\begin{equation}\n",
        "H_t = \\phi(X_t W_{xh} + H_{t-1}W_{hh} + b_h)\n",
        "\\end{equation}\n",
        "\n",
        "For time step $t$, the output of the output layer is similar to the computation in the MLP:\n",
        "\n",
        "\\begin{equation}\n",
        "O_t = H_t W_{hq} + b_q\n",
        "\\end{equation}\n",
        " \n",
        "Parameters of the RNN include the weights $W_{xh} \\in R^{d \\times h}, W_{hh} \\in R^{h \\times h}$ , and the bias $b_h \\in R^{1 \\times h}$ of the hidden layer, together with the weights $W_{hq} \\in R^{h \\times  q}$ and the bias $b_q \\in R^{1 \\times q}$ of the output layer. It is worth mentioning that even at different time steps, RNNs always use these model parameters. Therefore, the parameterization cost of an RNN does not grow as the number of time steps increases.\n",
        "\n",
        "<figure>\n",
        "  <img width=700 align=\"center\" src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/rnn.svg\">\n",
        "  <figcaption>\n",
        "  Source <a href=\"https://d2l.ai/\">d2l.ai</a>\n",
        "  </figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jWh_fR0ZpXUq"
      },
      "source": [
        "## Section 1.1: Load and View of the dataset\n",
        "\n",
        "Let us first load the dataset using the helper function `load_data`, which takes three arguments; the `sentence_length`, `batch_size`, and the `seed`. The default values are 50, 32, and 522, respectively. Execute the cell below to load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "Ba-WteRZpXUq",
        "outputId": "8ced6b56-bf3d-46a8-9098-5d6c4015411b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 33.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading is completed. Sentence length: 50, Batch size: 32, and seed: 2021\n"
          ]
        }
      ],
      "source": [
        "# @markdown Dataset Loading with default params\n",
        "TEXT, vocab_size, train_iter, valid_iter, test_iter = load_dataset(seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "1bxjAn6wpXUr"
      },
      "source": [
        "Now, let's view the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "seuW8UrWpXUr",
        "outputId": "54ed2ba6-e18f-4e89-bf9d-ea9488cf605f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review:  that 's my bush is a live action project made by south park creators trey parker and matt stone.the show was cancelled after one season , not because of bad reviews ( it actually got good reviews ) , but because it was very expensive.that 's my bush is a\n",
            "Label:  1 \n",
            "\n",
            "Review:  the actual crime story at the core of in cold blood might seem a little 'tame ' for those who are weened on the classic serial killer stories ( gein , bundy , dahmer ) , or just the more notorious cases out in hollywood ( oj , manson )\n",
            "Label:  1 \n",
            "\n",
            "Review:  of all arnold 's mid-'80s movies who would have thought that most relevant today would be the running man . a chilling and surprisingly realistic tale of reality tv gone mad . it may have been far-fetched back then but not so now . not when you think about it\n",
            "Label:  1 \n",
            "\n",
            "Review:  zombie nation 2004 r < br / > < br / > hey , i was bored . i looked in my comcastic little box to find a movie to watch . zombie nation ? hey , i love zombie movies . says the filmmaker has some sort of cult\n",
            "Label:  0 \n",
            "\n",
            "Review:  i actually have a fondness for christopher lee , but this just was n't up to his other performances ... and he was one of the better actors. < br / > < br / > the film does not live up to its premise . it 's not that\n",
            "Label:  0 \n",
            "\n",
            "[0: Negative Review, 1: Positive Review]\n"
          ]
        }
      ],
      "source": [
        "# @markdown Visualize dataset\n",
        "view_data(TEXT, train_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "0CeksVKPpXUr"
      },
      "source": [
        "### Coding Exercise 1.1: Vanilla RNN\n",
        "\n",
        "Now it's your turn to write a Vanilla RNN using PyTorch.\n",
        "\n",
        "* Once again we will use `nn.Embedding`. You are given the `vocab_size` which is the size of the dictionary of embeddings, and the `embed_size` which is the size of each embedding vector.  \n",
        "\n",
        "* Add 2 [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) layers. This would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results.\n",
        "\n",
        "* Determine the size of inputs and outputs to the fully-connected layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {},
        "id": "TIfoJkkHpXUr"
      },
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Vanilla RNN with following structure:\n",
        "  Embedding of size vocab_size * embed_size # Embedding Layer\n",
        "  RNN of size embed_size * hidden_size * self.n_layers # RNN Layer\n",
        "  Linear of size self.n_layers*hidden_size * output_size # Fully connected layer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, layers, output_size, hidden_size, vocab_size, embed_size,\n",
        "               device):\n",
        "    \"\"\"\n",
        "    Initialize parameters of VanillaRNN\n",
        "\n",
        "    Args:\n",
        "      layers: int\n",
        "        Number of layers\n",
        "      output_size: int\n",
        "        Size of final fully connected layer\n",
        "      hidden_size: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Size of vocabulary\n",
        "      device: string\n",
        "        GPU if available, CPU otherwise\n",
        "      embed_size: int\n",
        "        Size of embedding\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(VanillaRNN, self).__init__()\n",
        "    self.n_layers= layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"Define the Vanilla RNN components\")\n",
        "    ####################################################################\n",
        "    # Define the embedding\n",
        "    self.embeddings = ...\n",
        "    # Define the RNN layer\n",
        "    self.rnn = ...\n",
        "    # Define the fully connected layer\n",
        "    self.fc = ...\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    \"\"\"\n",
        "    Forward pass of VanillaRNN\n",
        "\n",
        "    Args:\n",
        "      inputs: torch.tensor\n",
        "        Input features\n",
        "\n",
        "    Returns:\n",
        "      logits: torch.tensor\n",
        "        Output of final fully connected layer\n",
        "    \"\"\"\n",
        "    input = self.embeddings(inputs)\n",
        "    input = input.permute(1, 0, 2)\n",
        "    h_0 = torch.zeros(2, input.size()[1], self.hidden_size).to(self.device)\n",
        "    output, h_n = self.rnn(input, h_0)\n",
        "    h_n = h_n.permute(1, 0, 2)\n",
        "    # Reshape the data and create a copy of the tensor such that the\n",
        "    # order of its elements in memory is the same as if it had been created\n",
        "    # from scratch with the same data. Without contiguous it may raise an error\n",
        "    # RuntimeError: input is not contiguous;\n",
        "    # Note that this is necessary as permute may return a non-contiguous tensor\n",
        "    h_n = h_n.contiguous().reshape(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Coding Exercise 1.1: Vanilla RNN')\n",
        "\n",
        "## Uncomment to test VanillaRNN class\n",
        "# sampleRNN = VanillaRNN(2, 10, 50, 1000, 300, DEVICE)\n",
        "# print(sampleRNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {},
        "id": "UkF1M50xpXUr",
        "outputId": "56a48773-82a8-4836-8725-721e9d2c33d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VanillaRNN(\n",
            "  (embeddings): Embedding(1000, 300)\n",
            "  (rnn): RNN(300, 50, num_layers=2)\n",
            "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# to_remove solution\n",
        "class VanillaRNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Vanilla RNN with following structure:\n",
        "  Embedding of size vocab_size * embed_size # Embedding Layer\n",
        "  RNN of size embed_size * hidden_size * self.n_layers # RNN Layer\n",
        "  Linear of size self.n_layers*hidden_size * output_size # Fully connected layer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, layers, output_size, hidden_size, vocab_size, embed_size,\n",
        "               device):\n",
        "    \"\"\"\n",
        "    Initialize parameters of VanillaRNN\n",
        "\n",
        "    Args:\n",
        "      layers: int\n",
        "        Number of layers\n",
        "      output_size: int\n",
        "        Size of final fully connected layer\n",
        "      hidden_size: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Size of vocabulary\n",
        "      device: string\n",
        "        GPU if available, CPU otherwise\n",
        "      embed_size: int\n",
        "        Size of embedding\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(VanillaRNN, self).__init__()\n",
        "    self.n_layers= layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    # Define the embedding\n",
        "    self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    # Define the RNN layer\n",
        "    self.rnn = nn.RNN(embed_size, hidden_size, self.n_layers)\n",
        "    # Define the fully connected layer\n",
        "    self.fc = nn.Linear(self.n_layers *hidden_size, output_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    \"\"\"\n",
        "    Forward pass of VanillaRNN\n",
        "\n",
        "    Args:\n",
        "      inputs: torch.tensor\n",
        "        Input features\n",
        "\n",
        "    Returns:\n",
        "      logits: torch.tensor\n",
        "        Output of final fully connected layer\n",
        "    \"\"\"\n",
        "    input = self.embeddings(inputs)\n",
        "    input = input.permute(1, 0, 2)\n",
        "    h_0 = torch.zeros(2, input.size()[1], self.hidden_size).to(self.device)\n",
        "    output, h_n = self.rnn(input, h_0)\n",
        "    h_n = h_n.permute(1, 0, 2)\n",
        "    # Reshape the data and create a copy of the tensor such that the\n",
        "    # order of its elements in memory is the same as if it had been created\n",
        "    # from scratch with the same data. Without contiguous it may raise an error\n",
        "    # RuntimeError: input is not contiguous;\n",
        "    # Note that this is necessary as permute may return a non-contiguous tensor\n",
        "    h_n = h_n.contiguous().reshape(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Coding Exercise 1.1: Vanilla RNN')\n",
        "\n",
        "## Uncomment to test VanillaRNN class\n",
        "sampleRNN = VanillaRNN(2, 10, 50, 1000, 300, DEVICE)\n",
        "print(sampleRNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6ja0ygnbpXUs"
      },
      "source": [
        "```\n",
        "VanillaRNN(\n",
        "  (embeddings): Embedding(1000, 300)\n",
        "  (rnn): RNN(300, 50, num_layers=2)\n",
        "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EICd-KU4pXUs"
      },
      "source": [
        "## Section 1.2: Train and test the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {},
        "id": "UeRpslwepXUs",
        "outputId": "b91becb1-74aa-4e31-cde4-3eeb145a7677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2021 has been set.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6569aab3571c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                                                                \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                                                                \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                                                                                learning_rate)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Time taken to train = %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvanilla_rnn_start_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvanilla_rnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7fd28100d860>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_iter, valid_iter, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    215\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m       \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Model hyperparamters\n",
        "learning_rate = 0.0002\n",
        "layers = 2\n",
        "output_size = 2\n",
        "hidden_size = 50  # 100\n",
        "embedding_length = 100\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "# Initialize model, training and testing\n",
        "set_seed(SEED)\n",
        "vanilla_rnn_model = VanillaRNN(layers, output_size, hidden_size, vocab_size,\n",
        "                               embedding_length, DEVICE)\n",
        "vanilla_rnn_model.to(DEVICE)\n",
        "vanilla_rnn_start_time = time.time()\n",
        "vanilla_train_loss, vanilla_train_acc, vanilla_validation_loss, vanilla_validation_acc = train(vanilla_rnn_model,\n",
        "                                                                                               DEVICE,\n",
        "                                                                                               train_iter,\n",
        "                                                                                               valid_iter,\n",
        "                                                                                               epochs,\n",
        "                                                                                               learning_rate)\n",
        "print(\"--- Time taken to train = %s seconds ---\" % (time.time() - vanilla_rnn_start_time))\n",
        "test_accuracy = test(vanilla_rnn_model, DEVICE, test_iter)\n",
        "print(f'Test Accuracy: {test_accuracy} with len=50\\n')\n",
        "\n",
        "# Number of model parameters\n",
        "print(f'Number of parameters = {count_parameters(vanilla_rnn_model)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "vcT1kO4dpXUs"
      },
      "source": [
        "Now, let's plot the accuracies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "psJrl9KrpXUs"
      },
      "outputs": [],
      "source": [
        "# Plot accuracy curves\n",
        "plt.figure()\n",
        "plt.subplot(211)\n",
        "plot_train_val(np.arange(0, epochs), vanilla_train_acc, vanilla_validation_acc,\n",
        "               'train accuracy', 'val accuracy',\n",
        "               'Vanilla RNN on IMDB text classification', 'accuracy',\n",
        "               color='C0')\n",
        "plt.legend(loc='upper left')\n",
        "plt.subplot(212)\n",
        "plot_train_val(np.arange(0, epochs), vanilla_train_loss,\n",
        "               vanilla_validation_loss,\n",
        "               'train loss', 'val loss',\n",
        "               'Vanilla RNN on IMDB text classification',\n",
        "               'loss [a.u.]',\n",
        "               color='C0')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "5y9RidyBpXUs"
      },
      "source": [
        "### Change the input length\n",
        "\n",
        "Now let's increase the `sentence_length` to see how RNN performs when long reviews are allowed..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "9lNyfd_ZpXUs"
      },
      "outputs": [],
      "source": [
        "# @markdown Load dataset with `sentence_length=200`\n",
        "TEXT_long, vocab_size_long, train_iter_long, valid_iter_long, test_iter_long = load_dataset(sentence_length=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "L_scOj_TpXUs"
      },
      "source": [
        "### Re-run the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "uCOcy-vGpXUt"
      },
      "outputs": [],
      "source": [
        "# Model hyperparamters\n",
        "learning_rate = 0.0002\n",
        "layers = 2\n",
        "output_size = 2\n",
        "hidden_size = 50  # 100\n",
        "embedding_length = 100\n",
        "epochs = 10\n",
        "\n",
        "# Initialize model, training, testing\n",
        "set_seed(SEED)\n",
        "vanilla_rnn_model_long = VanillaRNN(layers, output_size, hidden_size,\n",
        "                                    vocab_size_long, embedding_length, DEVICE)\n",
        "vanilla_rnn_model_long.to(DEVICE)\n",
        "vanilla_rnn_start_time_long = time.time()\n",
        "vanilla_train_loss_long, vanilla_train_acc_long, vanilla_validation_loss_long, vanilla_validation_acc_long = train(vanilla_rnn_model_long,\n",
        "                                                                                                                   DEVICE,\n",
        "                                                                                                                   train_iter_long,\n",
        "                                                                                                                   valid_iter_long,\n",
        "                                                                                                                   epochs,\n",
        "                                                                                                                   learning_rate)\n",
        "print(\"--- Time taken to train = %s seconds ---\" % (time.time() - vanilla_rnn_start_time_long))\n",
        "test_accuracy = test(vanilla_rnn_model_long, DEVICE, test_iter_long)\n",
        "print(f'Test Accuracy: {test_accuracy} with len=200\\n')\n",
        "\n",
        "# Number of parameters\n",
        "print(f'\\nNumber of parameters = {count_parameters(vanilla_rnn_model_long)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "q0KdYJQdpXUt"
      },
      "outputs": [],
      "source": [
        "# Compare accuracies of model trained on different sentence lengths\n",
        "plot_train_val(np.arange(0, epochs), vanilla_train_acc,\n",
        "               vanilla_validation_acc,\n",
        "               'train accuracy, len=50', 'val accuracy, len=50',\n",
        "               '', 'accuracy',\n",
        "               color='C0')\n",
        "plot_train_val(np.arange(0, epochs), vanilla_train_acc_long,\n",
        "               vanilla_validation_acc_long,\n",
        "               'train accuracy, len=200', 'val accuracy, len=200',\n",
        "               'Training and Validation Accuracy for Sentence Lengths 50 and 200',\n",
        "               'accuracy',\n",
        "               color='C1')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Go2U2EHdpXUt"
      },
      "source": [
        "## Section 1.3: Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "JNQOVHeTpXUt"
      },
      "outputs": [],
      "source": [
        "# @title Video 2: Bidirectional RNNs\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1X64y1x7BA\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"TpgJwqB4i1c\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 2: Bidirectional RNNs')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8oXWvsv-pXUt"
      },
      "source": [
        "RNN models are mostly used in the fields of natural language processing and speech recognition. Below are types of RNNs. Depending on which outputs we use, RNN can be used for variety of tasks. The text classification problem we solved was an instance of the many to one architecture. Write down the applications of other architectures. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "a77fGNSZpXUt"
      },
      "source": [
        "<figure>\n",
        "  <img width=800 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/karpathy.jpeg\">\n",
        "  <figcaption>\n",
        "  Source <a href=\"https://blog.floydhub.com/\">blog.floydhub.com</a>\n",
        "  </figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "sJa42SkepXUu"
      },
      "source": [
        "## Section 1.4: Vanishing and Exploding Gradients\n",
        "\n",
        "For an RNN to learn via backprop through time on a loss calculated at time $T$, $\\mathcal{L}_T$, with respect to an input $t$ time steps in the past, the RNN weights must be updated based on how they contributed to the hidden state at this past time step. This contribution is learned through the term $\\frac{\\partial h_{-t}}{\\partial W}$, in the gradient of the loss, $\\frac{\\partial\\mathcal{L}_T}{\\partial W}$.\n",
        "\n",
        "However, because one has to backpropagate error through $t-1$ hidden states, $\\frac{\\partial h_{-t}}{\\partial W}$ is multiplied by $\\prod_{i=0}^{t-1} \\frac{\\partial{h_i}}{\\partial{h_{i-1}}}$ in the expression for $\\frac{\\partial\\mathcal{L}_T}{\\partial W}$, which are summarized mathematically:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial{\\mathcal{L}_T}}{\\partial{W}} \\propto \\frac{\\partial h_t }{ \\partial W} + \\sum_{k=0}^{t-1} \\left( \\prod_{i=k+1}^{t} \\frac{\\partial{h_i}}{\\partial{h_{i-1}}} \\right) \\frac{\\partial{h_k}}{\\partial{W}}\n",
        "\\end{equation}\n",
        "\n",
        "The product term leads to two common problems during the backpropagation of time-series data:\n",
        "\n",
        "* *Vanishing gradients*, *if* $ \\left| \\left| \\frac{\\partial{h_i}}{\\partial{h_{i-1}}} \\right| \\right|_2 < 1$\n",
        "\n",
        "\n",
        "* *Exploding gradients*, *if* $ \\left| \\left| \\frac{\\partial{h_i}}{\\partial{h_{i-1}}} \\right| \\right|_2 > 1$\n",
        "\n",
        "Given a sufficiently long sequence, the gradients get multiplied by the weight matrix at every time step. If the weight matrix contains very small values, then the norm of gradients will become smaller and smaller exponentially, the so-called **vanishing gradient** problem. On the other hand, if we have a weight matrix with very large values, the gradients will increase exponentially, leading to the **exploding gradients** problem: where the weights diverge at the update step.\n",
        "\n",
        "An example that has the vanishing gradient problem:\n",
        "\n",
        "The input is the characters from a *C* Program. The system will tell whether it is a syntactically correct program. A syntactically correct program should have a valid number of braces and parentheses. Thus, the network should remember how many open parentheses and braces there are to check, and whether we have closed them all. The network has to store such information in hidden states like a counter. However, because of vanishing gradients, it will fail to preserve such information in a long program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UlI1ITkNpXUu"
      },
      "source": [
        "---\n",
        "# Section 2: LSTM, GRU and Memory Cell\n",
        "\n",
        "*Time estimate: ~28mins*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "Mkw7cJ14pXUu"
      },
      "outputs": [],
      "source": [
        "# @title Video 3: LSTM, GRU & The Memory Cells\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1Zq4y1X7kj\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"qC7O-2sXLAU\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 3: LSTM, GRU & The Memory Cells')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "5TrmCRVUpXUu"
      },
      "source": [
        "## Section 2.1: Architecture\n",
        "\n",
        "The core idea behind an LSTM is the cell state $C_t$ that runs along all the LSTM units in a layer, and gets updated along the way. These updates are possible through \"gates\". Gates are made out of a sigmoid neural net layer and a pointwise multiplication operation. \n",
        "\n",
        "Each LSTM unit performs the following distinct steps using the input $X_t$, current cell state $C_t$ and previous hidden state $H_{t-1}$:\n",
        "\n",
        "* Forget Gate: *Should I throw away information from this cell?*\n",
        "\n",
        "\\begin{equation}\n",
        "F_t = \\sigma (W_f \\cdot [H_{t-1}, X_t] + b_f)\n",
        "\\end{equation}\n",
        "\n",
        "* Input Gate:\n",
        "    * *Should I add new values to this cell?*\n",
        "      \\begin{equation}\n",
        "      I_t = \\sigma (W_i \\cdot [H_{t-1}, X_t] + b_i)\n",
        "      \\end{equation}\n",
        "    * *What new candidate values should I store?*\n",
        "      \\begin{equation}\n",
        "      \\tilde{C}_t = tanh (W_C \\cdot [H_{t-1}, X_t] + b_C)\n",
        "      \\end{equation}\n",
        "\n",
        "* Update cell state: *Forget things from the past and add new things from the candidates*\n",
        "  \\begin{equation}\n",
        "  C_t = (F_t \\cdot C_{t-1}) + (I_t \\cdot \\tilde{C}_t)\n",
        "  \\end{equation}\n",
        "\n",
        "* Output Gate: \n",
        "    * *What information should I output?*\n",
        "      \\begin{equation}\n",
        "      O_t = \\sigma (W_o \\cdot [H_{t-1}, X_t] + b_o)\n",
        "      \\end{equation}\n",
        "    * *How much of the cell state should I store in the hidden state?*\n",
        "      \\begin{equation}\n",
        "      H_t = O_t \\cdot tanh(C_t)\n",
        "      \\end{equation}\n",
        "\n",
        "The architecture can be summarized by the diagram below:\n",
        "\n",
        "<center>\n",
        "<figure>\n",
        "  <img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/lstm-1.svg\">\n",
        "  <figcaption>\n",
        "  Source <a href=\"https://d2l.ai/\">d2l.ai</a>\n",
        "  </figcaption>\n",
        "</figure>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "npOhjY6tpXUu"
      },
      "source": [
        "### Coding Exercise 2.1: Implementing LSTM\n",
        "\n",
        "It is now your turn to build an LSTM network in PyTorch. Feel free to refer to the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM). \n",
        "\n",
        "* Once again we will use `nn.Embedding`. You are given the `vocab_size` and the `embed_size`.\n",
        "* Add the [`LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) layers. \n",
        "* Define a dropout layer of 0.5. \n",
        "* Determine the size of inputs and outputs to the fully-connected layer.\n",
        "* Pay special attention to the shapes of your inputs and outputs as you write the forward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "maDb5x20pXUu"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  \"\"\"\n",
        "  LSTM (Long Short Term Memory) with following structure\n",
        "  Embedding layer of size vocab_size * embed_size\n",
        "  Dropout layer with dropout_probability of 0.5\n",
        "  LSTM layer of size embed_size * hidden_size * num_layers\n",
        "  Fully connected layer of n_layers*hidden_size * output_size\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, layers, output_size, hidden_size, vocab_size, embed_size,\n",
        "               device):\n",
        "    \"\"\"\n",
        "    Initialize parameters of LSTM\n",
        "\n",
        "    Args:\n",
        "      layers: int\n",
        "        Number of layers\n",
        "      output_size: int\n",
        "        Size of final fully connected layer\n",
        "      hidden_size: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Size of vocabulary\n",
        "      device: string\n",
        "        GPU if available, CPU otherwise\n",
        "      embed_size: int\n",
        "        Size of embedding\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(LSTM, self).__init__()\n",
        "    self.n_layers = layers\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"LSTM Init\")\n",
        "    ####################################################################\n",
        "    # Define the word embeddings\n",
        "    self.word_embeddings = ...\n",
        "    # Define the dropout layer\n",
        "    self.dropout = ...\n",
        "    # Define the lstm layer\n",
        "    self.lstm = ...\n",
        "    # Define the fully-connected layer\n",
        "    self.fc = ...\n",
        "\n",
        "\n",
        "  def forward(self, input_sentences):\n",
        "    \"\"\"\n",
        "    Forward pass of LSTM\n",
        "    Hint: Make sure the shapes of your tensors match the requirement\n",
        "\n",
        "    Args:\n",
        "      input_sentences: torch.tensor\n",
        "        Input Sentences\n",
        "\n",
        "    Returns:\n",
        "      logits: torch.tensor\n",
        "        Output of final fully connected layer\n",
        "    \"\"\"\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"LSTM Forward\")\n",
        "    ####################################################################\n",
        "    # Embeddings\n",
        "    # `input` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "    input = ...\n",
        "\n",
        "    hidden = (torch.randn(self.n_layers, input.shape[1],\n",
        "                          self.hidden_size).to(self.device),\n",
        "              torch.randn(self.n_layers, input.shape[1],\n",
        "                          self.hidden_size).to(self.device))\n",
        "    # Dropout for regularization\n",
        "    input = self.dropout(input)\n",
        "    # LSTM\n",
        "    output, hidden = ...\n",
        "\n",
        "    h_n = hidden[0].permute(1, 0, 2)\n",
        "    h_n = h_n.contiguous().view(h_n.shape[0], -1)\n",
        "\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Coding Exercise 2.1: Implementing LSTM')\n",
        "\n",
        "## Uncomment to run\n",
        "# sampleLSTM = LSTM(3, 10, 100, 1000, 300, DEVICE)\n",
        "# print(sampleLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "I6zVIcx1pXUv"
      },
      "outputs": [],
      "source": [
        "# to_remove solution\n",
        "class LSTM(nn.Module):\n",
        "  \"\"\"\n",
        "  LSTM (Long Short Term Memory) with following structure\n",
        "  Embedding layer of size vocab_size * embed_size\n",
        "  Dropout layer with dropout_probability of 0.5\n",
        "  LSTM layer of size embed_size * hidden_size * num_layers\n",
        "  Fully connected layer of n_layers*hidden_size * output_size\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, layers, output_size, hidden_size, vocab_size, embed_size,\n",
        "               device):\n",
        "    \"\"\"\n",
        "    Initialize parameters of LSTM\n",
        "\n",
        "    Args:\n",
        "      layers: int\n",
        "        Number of layers\n",
        "      output_size: int\n",
        "        Size of final fully connected layer\n",
        "      hidden_size: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Size of vocabulary\n",
        "      device: string\n",
        "        GPU if available, CPU otherwise\n",
        "      embed_size: int\n",
        "        Size of embedding\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(LSTM, self).__init__()\n",
        "    self.n_layers = layers\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    # Define the word embeddings\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    # Define the dropout layer\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    # Define the lstm layer\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=self.n_layers)\n",
        "    # Define the fully-connected layer\n",
        "    self.fc = nn.Linear(self.n_layers*self.hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, input_sentences):\n",
        "    \"\"\"\n",
        "    Forward pass of LSTM\n",
        "    Hint: Make sure the shapes of your tensors match the requirement\n",
        "\n",
        "    Args:\n",
        "      input_sentences: torch.tensor\n",
        "        Input Sentences\n",
        "\n",
        "    Returns:\n",
        "      logits: torch.tensor\n",
        "        Output of final fully connected layer\n",
        "    \"\"\"\n",
        "    # Embeddings\n",
        "    # `input` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "    input = self.word_embeddings(input_sentences).permute(1, 0, 2)\n",
        "\n",
        "    hidden = (torch.randn(self.n_layers, input.shape[1],\n",
        "                          self.hidden_size).to(self.device),\n",
        "              torch.randn(self.n_layers, input.shape[1],\n",
        "                          self.hidden_size).to(self.device))\n",
        "    # Dropout for regularization\n",
        "    input = self.dropout(input)\n",
        "    # LSTM\n",
        "    output, hidden = self.lstm(input, hidden)\n",
        "\n",
        "    h_n = hidden[0].permute(1, 0, 2)\n",
        "    h_n = h_n.contiguous().view(h_n.shape[0], -1)\n",
        "\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Coding Exercise 2.1: Implementing LSTM')\n",
        "\n",
        "## Uncomment to run\n",
        "sampleLSTM = LSTM(3, 10, 100, 1000, 300, DEVICE)\n",
        "print(sampleLSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "cSrAdTgepXUv"
      },
      "source": [
        "```\n",
        "LSTM(\n",
        "  (word_embeddings): Embedding(1000, 300)\n",
        "  (dropout): Dropout(p=0.5, inplace=False)\n",
        "  (lstm): LSTM(300, 100, num_layers=3)\n",
        "  (fc): Linear(in_features=300, out_features=10, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "6CkUgyl-pXUv"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.0003\n",
        "layers = 2\n",
        "output_size = 2\n",
        "hidden_size = 16\n",
        "embedding_length = 100\n",
        "epochs = 10\n",
        "\n",
        "# Model, training, testing\n",
        "set_seed(SEED)\n",
        "lstm_model = LSTM(layers, output_size, hidden_size, vocab_size,\n",
        "                  embedding_length, DEVICE)\n",
        "lstm_model.to(DEVICE)\n",
        "lstm_train_loss, lstm_train_acc, lstm_validation_loss, lstm_validation_acc = train(lstm_model,\n",
        "                                                                                   DEVICE,\n",
        "                                                                                   train_iter,\n",
        "                                                                                   valid_iter,\n",
        "                                                                                   epochs,\n",
        "                                                                                   learning_rate)\n",
        "test_accuracy = test(lstm_model, DEVICE, test_iter)\n",
        "print(f'\\n\\nTest Accuracy: {test_accuracy} of the LSTM model\\n')\n",
        "\n",
        "# Plotting accuracy curve\n",
        "plt.figure()\n",
        "plt.subplot(211)\n",
        "plot_train_val(np.arange(0, epochs), lstm_train_acc, lstm_validation_acc,\n",
        "               'train accuracy',\n",
        "               'val accuracy',\n",
        "               'LSTM on IMDB text classification',\n",
        "               'accuracy',\n",
        "               color='C0')\n",
        "plt.legend(loc='upper left')\n",
        "plt.subplot(212)\n",
        "plot_train_val(np.arange(0, epochs), lstm_train_loss, lstm_validation_loss,\n",
        "               'train loss',\n",
        "               'val loss',\n",
        "               '',\n",
        "               'loss',\n",
        "               color='C0')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "GpqUgln8pXUv"
      },
      "source": [
        "## Section 2.2: Gated Recurrent Units (GRU)\n",
        "\n",
        "The GRU architecture looks very similar to the LSTM, and is often used as an alternative to the traditional LSTM. It also contains some variations that reduce it's complexity. For example, it combines the forget and input gates into a single “update gate”; it contains a \"hidden state\" but not a \"cell state\". In the next section we will be using GRUs as the choice of recurrent unit in our models, but you can always swap out the GRU for an LSTM later on (make sure that you take care of input and output dimensions in this case). Here is a description of the parts of the GRU:\n",
        "\n",
        "* Reset Gate: *How much of the previous hidden state should I remember?*\n",
        "  \\begin{equation}\n",
        "  R_t = \\sigma (W_r \\cdot [H_{t-1}, X_t])\n",
        "  \\end{equation}\n",
        "\n",
        "* Update Gate: \n",
        "    * *How much of the new state is different from the old state?*\n",
        "      \\begin{equation}\n",
        "      Z_t = \\sigma (W_z \\cdot [H_{t-1}, X_t])\n",
        "      \\end{equation}\n",
        "    * *What new candidate values should I store?*\n",
        "      \\begin{equation}\n",
        "      \\tilde{H}_t = tanh (W \\cdot [R_t \\cdot H_{t-1}, X_t])\n",
        "      \\end{equation}\n",
        "\n",
        "* Update hidden state: *Deciding how much of the old hidden state to keep and discard*\n",
        "  \\begin{equation}\n",
        "  H_t = ((1-Z_t) \\cdot H_{t-1} ) + (Z_t \\cdot \\tilde{H}_t)\n",
        "  \\end{equation}\n",
        "\n",
        "Here is what the architecture looks like:\n",
        "\n",
        "<center>\n",
        "<figure>\n",
        "  <img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/gru-3.svg\">\n",
        "  <figcaption>\n",
        "  Source <a href=\"https://d2l.ai/\">d2l.ai</a>\n",
        "  </figcaption>\n",
        "</figure>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Lj6YWw6HpXUv"
      },
      "source": [
        "### Coding Exercise 2.2: BiLSTM\n",
        "\n",
        "Let's apply the knowledge to write a bi-LSTM using PyTorch.\n",
        "\n",
        "* Use an Embedding layer\n",
        "* Dropout of 0.5\n",
        "* Add 2 LSTM layers\n",
        "* Linear layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "_m8r_W_BpXUv"
      },
      "outputs": [],
      "source": [
        "class biLSTM(nn.Module):\n",
        "  \"\"\"\n",
        "  Bidirectional LSTM with following structure\n",
        "  Embedding layer of size vocab_size * embed_size\n",
        "  Dropout layer with dropout_probability of 0.5\n",
        "  biLSTM layer of size embed_size * hidden_size * num_layers\n",
        "  Fully connected layer of n_layers*hidden_size * output_size\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_size, hidden_size, vocab_size, embed_size,\n",
        "               device):\n",
        "    \"\"\"\n",
        "    Initialize parameters of biLSTM\n",
        "\n",
        "    Args:\n",
        "      output_size: int\n",
        "        Size of final fully connected layer\n",
        "      hidden_size: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Size of vocabulary\n",
        "      device: string\n",
        "        GPU if available, CPU otherwise\n",
        "      embed_size: int\n",
        "        Size of embedding\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(biLSTM, self).__init__()\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...)\n",
        "    raise NotImplementedError(\"biLSTM\")\n",
        "    ####################################################################\n",
        "    # Define the word embeddings\n",
        "    self.word_embeddings = ...\n",
        "    # Define the dropout layer\n",
        "    self.dropout = ...\n",
        "    # Define the bilstm layer\n",
        "    self.bilstm = ...\n",
        "    # Define the fully-connected layer; 4 = 2*2: 2 for stacking and 2 for bidirectionality\n",
        "    self.fc = ...\n",
        "\n",
        "  def forward(self, input_sentences):\n",
        "    \"\"\"\n",
        "    Forward pass of biLSTM\n",
        "\n",
        "    Args:\n",
        "      input_sentences: torch.tensor\n",
        "        Input Sentences\n",
        "\n",
        "    Returns:\n",
        "      logits: torch.tensor\n",
        "        Output of final fully connected layer\n",
        "    \"\"\"\n",
        "    input = self.word_embeddings(input_sentences).permute(1, 0, 2)\n",
        "    hidden = (torch.randn(4, input.shape[1], self.hidden_size).to(self.device),\n",
        "              torch.randn(4, input.shape[1], self.hidden_size).to(self.device))\n",
        "    input = self.dropout(input)\n",
        "\n",
        "    output, hidden = self.bilstm(input, hidden)\n",
        "\n",
        "    h_n = hidden[0].permute(1, 0, 2)\n",
        "    h_n = h_n.contiguous().view(h_n.shape[0], -1)\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Coding Exercise 2.2: BiLSTM')\n",
        "\n",
        "## Uncomment to run\n",
        "# sampleBiLSTM = biLSTM(10, 100, 1000, 300, DEVICE)\n",
        "# print(sampleBiLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "8wHpjepIpXUw"
      },
      "outputs": [],
      "source": [
        "# to_remove solution\n",
        "class biLSTM(nn.Module):\n",
        "  \"\"\"\n",
        "  Bidirectional LSTM with following structure\n",
        "  Embedding layer of size vocab_size * embed_size\n",
        "  Dropout layer with dropout_probability of 0.5\n",
        "  biLSTM layer of size embed_size * hidden_size * num_layers\n",
        "  Fully connected layer of n_layers*hidden_size * output_size\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_size, hidden_size, vocab_size, embed_size,\n",
        "               device):\n",
        "    \"\"\"\n",
        "    Initialize parameters of biLSTM\n",
        "\n",
        "    Args:\n",
        "      output_size: int\n",
        "        Size of final fully connected layer\n",
        "      hidden_size: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Size of vocabulary\n",
        "      device: string\n",
        "        GPU if available, CPU otherwise\n",
        "      embed_size: int\n",
        "        Size of embedding\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(biLSTM, self).__init__()\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    # Define the word embeddings\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    # Define the dropout layer\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    # Define the bilstm layer\n",
        "    self.bilstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=True)\n",
        "    # Define the fully-connected layer; 4 = 2*2: 2 for stacking and 2 for bidirectionality\n",
        "    self.fc = nn.Linear(4*hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, input_sentences):\n",
        "    \"\"\"\n",
        "    Forward pass of biLSTM\n",
        "\n",
        "    Args:\n",
        "      input_sentences: torch.tensor\n",
        "        Input Sentences\n",
        "\n",
        "    Returns:\n",
        "      logits: torch.tensor\n",
        "        Output of final fully connected layer\n",
        "    \"\"\"\n",
        "    input = self.word_embeddings(input_sentences).permute(1, 0, 2)\n",
        "    hidden = (torch.randn(4, input.shape[1], self.hidden_size).to(self.device),\n",
        "              torch.randn(4, input.shape[1], self.hidden_size).to(self.device))\n",
        "    input = self.dropout(input)\n",
        "\n",
        "    output, hidden = self.bilstm(input, hidden)\n",
        "\n",
        "    h_n = hidden[0].permute(1, 0, 2)\n",
        "    h_n = h_n.contiguous().view(h_n.shape[0], -1)\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Coding Exercise 2.2: BiLSTM')\n",
        "\n",
        "## Uncomment to run\n",
        "sampleBiLSTM = biLSTM(10, 100, 1000, 300, DEVICE)\n",
        "print(sampleBiLSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "wippd7MlpXUw"
      },
      "source": [
        "```\n",
        "biLSTM(\n",
        "  (word_embeddings): Embedding(1000, 300)\n",
        "  (dropout): Dropout(p=0.5, inplace=False)\n",
        "  (bilstm): LSTM(300, 100, num_layers=2, bidirectional=True)\n",
        "  (fc): Linear(in_features=400, out_features=10, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "jpLm-zAxpXUw"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.0003\n",
        "output_size = 2\n",
        "hidden_size = 16\n",
        "embedding_length = 100\n",
        "epochs = 10\n",
        "\n",
        "# Model, training, testing\n",
        "set_seed(SEED)\n",
        "bilstm_model = biLSTM(output_size, hidden_size, vocab_size,\n",
        "                      embedding_length, DEVICE)\n",
        "bilstm_model.to(DEVICE)\n",
        "bilstm_train_loss, bilstm_train_acc, bilstm_validation_loss, bilstm_validation_acc = train(bilstm_model,\n",
        "                                                                                           DEVICE,\n",
        "                                                                                           train_iter,\n",
        "                                                                                           valid_iter,\n",
        "                                                                                           epochs,\n",
        "                                                                                           learning_rate)\n",
        "test_accuracy = test(bilstm_model, DEVICE, test_iter)\n",
        "print(f'Test Accuracy: {test_accuracy} of the biLSTM model\\n')\n",
        "\n",
        "# Plotting accuracy curve\n",
        "plt.figure()\n",
        "plt.subplot(211)\n",
        "plot_train_val(np.arange(0, epochs), bilstm_train_acc, bilstm_validation_acc,\n",
        "               'train accuracy',\n",
        "               'val accuracy',\n",
        "               'biLSTM on IMDB text classification',\n",
        "               'accuracy',\n",
        "               color='C1')\n",
        "plt.legend(loc='upper left')\n",
        "plt.subplot(212)\n",
        "plot_train_val(np.arange(0, epochs), bilstm_train_loss, bilstm_validation_loss,\n",
        "               'train loss',\n",
        "               'val loss',\n",
        "               '',\n",
        "               'loss',\n",
        "               color='C1')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "bjPyhM0UpXUx"
      },
      "outputs": [],
      "source": [
        "# Compare accuracies of LSTM and biLSTM\n",
        "plot_train_val(np.arange(0, epochs), lstm_train_acc,\n",
        "               lstm_validation_acc,\n",
        "               'train accuracy LSTM', 'val accuracy LSTM',\n",
        "               '', 'accuracy',\n",
        "               color='C0')\n",
        "plot_train_val(np.arange(0, epochs), bilstm_train_acc,\n",
        "               bilstm_validation_acc,\n",
        "               'train accuracy biLSTM', 'val accuracy biLSTM',\n",
        "               'Training and Validation Accuracy for LSTM and biLSTM models',\n",
        "               'accuracy',\n",
        "               color='C1')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8-fafy13pXUx"
      },
      "source": [
        "---\n",
        "# Section 3: Sequence to Sequence (Seq2Seq) & Encoder/ Decoder Networks\n",
        "\n",
        "*Time estimate: ~15mins*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "lOTCSSvqpXUx"
      },
      "outputs": [],
      "source": [
        "# @title Video 4: Seq2Seq & Encoder-Decoder Nets\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1WM4y1N7P7\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"q-hf7mnZsXo\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 4: Seq2Seq & Encoder-Decoder Nets')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Tunlp0erpXUx"
      },
      "source": [
        "Sources: [d2l.ai on encoders](https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html); [d2l.ai on seq2seq](https://d2l.ai/chapter_recurrent-modern/seq2seq.html); [Jalammar's blog](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
        "\n",
        "Sequence-to-sequence models take in a sequence of items (words, characters, etc) as input and produces another sequence of items as output. The most \n",
        "simple seq2seq models are composed of two parts: the encoder, the context (\"state\" in the figure) and the decoder. The encoder and decoder usually consist of recurrent units that we've seen before (RNNs, GRUs or LSTMs). A high-level schematic of the architecture is as follows:\n",
        "\n",
        "<center>\n",
        "<figure>\n",
        "  <img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/seq2seq-predict.svg\">\n",
        "  <figcaption>\n",
        "  Source <a href=\"https://d2l.ai/\">d2l.ai</a>\n",
        "  </figcaption>\n",
        "</figure>\n",
        "</center>\n",
        "\n",
        "The encoder's recurrent unit processes the input one item at a time. Once the entire sequence is processed, the final hidden state vector produced is known as a context vector. The size of the context vector is defined while setting up the model, and is equal to the number of hidden states used in the encoder RNN. The encoder then passes the context to the decoder. The decoder's recurrent unit uses the context to produce the items for the output sequence one by one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "1WADKQYZpXUx"
      },
      "source": [
        "One of the most popular applications of seq2seq models is \"machine translation\": the task of taking in a sentence in one language (the source) and producing its translation in another language (the target); with words in both languages being the sequence units. This is a supervised learning task, and requires the dataset to have \"parallel sentences\"; i.e., each sentence in the source language must be labelled with its translation in the target language. \n",
        "\n",
        "[Here is an intuitive visualization for understanding seq2seq models for machine translation from English to French](https://i.imgur.com/HJ6t8up.mp4).\n",
        "\n",
        "Since the vocabulary of an entire language is very large, training such models to give meaningful performance requires significant time and resources. In this section, we will train a seq2seq model to perform machine translation from English to [Pig-Latin](https://en.wikipedia.org/wiki/Pig_Latin). We will modify the task to perform character-level machine translation, so that vocabulary size does not grow exponentially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TIHJou7WpXUy"
      },
      "source": [
        "## Coding Exercise 3: Encoder\n",
        "\n",
        "Let us consider a sequence example (`batch_size=1`). Suppose that the input sequence is $x_1, \\ldots, x_T$, such that $x_t$ is the $t^{\\mathrm{th}}$ token in the input text sequence. At time step $t$, the RNN transforms the input feature vector $\\mathbf{x}_t$ for $x_t$ and the hidden state $\\mathbf{h} _{t-1}$ from the previous time step into the current hidden state $\\mathbf{h}_t$.\n",
        "\n",
        "We can use a function $f$ to express the transformation of the RNN's recurrent layer:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1})\n",
        "\\end{equation}\n",
        "\n",
        "In general, the encoder transforms the hidden states at all the time steps into the context variable through a customized function $q$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{c} =  q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T)\n",
        "\\end{equation}\n",
        "\n",
        "For example, when choosing $q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T) = \\mathbf{h}_T$  the context variable is just the hidden state $\\mathbf{h}_T$ of the input sequence at the final time step.\n",
        "\n",
        "So far we have used a unidirectional RNN to design the encoder, where a hidden state only depends on the input subsequence at and before the time step of the hidden state. We can also construct encoders using bidirectional RNNs. In this case, a hidden state depends on the subsequence before and after the time step (including the input at the current time step), which encodes the information of the entire sequence.\n",
        "\n",
        "Now let us implement the RNN encoder. Note that we use an *embedding layer*\n",
        "to obtain the feature vector for each token in the input sequence. The weight of an embedding layer is a matrix whose number of rows is equal to the size of the input vocabulary (`vocab_size`) and the number of columns equals to the feature vector's dimension (`embed_size`). For any input token index $i$,\n",
        "the embedding layer fetches the $i^{\\mathrm{th}}$ row (starting from 0) of the weight matrix to return its feature vector. Here we choose a multilayer GRU to implement the encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "K2XUKujZpXUy"
      },
      "source": [
        "The returned variables of recurrent layers have been completely explained at [this link](https://www.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html#sec-rnn-concise). Let us still use a concrete example to illustrate the above encoder implementation. Below we instantiate a two-layer GRU encoder whose number of hidden units is 16. Given a minibatch of sequence inputs $X$ (`batch_size=4`, `number_of_time_steps=7`), the hidden states of the last layer at all the time steps (`output` returned by the encoder's recurrent layers) are a tensor of shape (number of time steps, batch size, number of hidden units)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "YIYe6ZjipXUy"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqEncoder(d2l.Encoder):\n",
        "  \"\"\"\n",
        "  RNN encoder for sequence to sequence learning.\n",
        "  RNN has the following structure:\n",
        "  Embedding layer with size vocab_size * embed_size\n",
        "  RNN layer with size embed_size * num_hiddens * num_layers + dropout\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                dropout=0, **kwargs):\n",
        "    \"\"\"\n",
        "    Initialize parameters of Seq2SeqEncoder\n",
        "\n",
        "    Args:\n",
        "      num_layers: int\n",
        "        Number of layers in GRU/RNN\n",
        "      num_hiddens: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Size of vocabulary\n",
        "      embed_size: int\n",
        "        Size of embedding\n",
        "      dropout: int\n",
        "        Dropout [default: 0]\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"Encoder Unit\")\n",
        "    ####################################################################\n",
        "    # Embedding layer\n",
        "    self.embedding = ...\n",
        "    # Here you're going to implement a GRU as the RNN unit\n",
        "    self.rnn = ...\n",
        "\n",
        "  def forward(self, X, *args):\n",
        "    \"\"\"\n",
        "    Forward pass of Seq2SeqEncoder\n",
        "\n",
        "    Args:\n",
        "      X: torch.tensor\n",
        "        Input features\n",
        "\n",
        "    Returns:\n",
        "      output: torch.tensor\n",
        "        Output with shape (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "      state: torch.tensor\n",
        "        State with shape (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "    \"\"\"\n",
        "    # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
        "    X = self.embedding(X)\n",
        "    # In RNN models, the first axis corresponds to time steps\n",
        "    X = X.permute(1, 0, 2)\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"Forward pass\")\n",
        "    ####################################################################\n",
        "    # When state is not mentioned, it defaults to zeros, the output should be a RNN function of X!\n",
        "    output, state = ...\n",
        "    # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "    # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "\n",
        "    return output, state\n",
        "\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Coding Exercise 3: Encoder')\n",
        "\n",
        "X = torch.zeros((4, 7), dtype=torch.long)\n",
        "## uncomment the lines below.\n",
        "# encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
        "# encoder.eval()\n",
        "# output, state = encoder(X)\n",
        "# print(output.shape)\n",
        "# print(state.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "WXGHHMttpXUy"
      },
      "outputs": [],
      "source": [
        "# to_remove solution\n",
        "class Seq2SeqEncoder(d2l.Encoder):\n",
        "  \"\"\"\n",
        "  RNN encoder for sequence to sequence learning.\n",
        "  RNN has the following structure:\n",
        "  Embedding layer with size vocab_size * embed_size\n",
        "  RNN layer with size embed_size * num_hiddens * num_layers + dropout\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                dropout=0, **kwargs):\n",
        "    \"\"\"\n",
        "    Initialize parameters of Seq2SeqEncoder\n",
        "\n",
        "    Args:\n",
        "      num_layers: int\n",
        "        Number of layers in GRU/RNN\n",
        "      num_hiddens: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Size of vocabulary\n",
        "      embed_size: int\n",
        "        Size of embedding\n",
        "      dropout: int\n",
        "        Dropout [default: 0]\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "    # Here you're going to implement a GRU as the RNN unit\n",
        "    self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
        "                      dropout=dropout)\n",
        "\n",
        "  def forward(self, X, *args):\n",
        "    \"\"\"\n",
        "    Forward pass of Seq2SeqEncoder\n",
        "\n",
        "    Args:\n",
        "      X: torch.tensor\n",
        "        Input features\n",
        "\n",
        "    Returns:\n",
        "      output: torch.tensor\n",
        "        Output with shape (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "      state: torch.tensor\n",
        "        State with shape (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "    \"\"\"\n",
        "    # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
        "    X = self.embedding(X)\n",
        "    # In RNN models, the first axis corresponds to time steps\n",
        "    X = X.permute(1, 0, 2)\n",
        "    # When state is not mentioned, it defaults to zeros, the output should be a RNN function of X!\n",
        "    output, state = self.rnn(X)\n",
        "    # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "    # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "    return output, state\n",
        "\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Coding Exercise 3: Encoder')\n",
        "\n",
        "X = torch.zeros((4, 7), dtype=torch.long)\n",
        "## uncomment the lines below.\n",
        "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
        "encoder.eval()\n",
        "output, state = encoder(X)\n",
        "print(output.shape)\n",
        "print(state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UFVGpNE4pXUy"
      },
      "source": [
        "```\n",
        "torch.Size([7, 4, 16])\n",
        "torch.Size([2, 4, 16])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "qKYt7jAnpXUy"
      },
      "source": [
        "## Section 3.1: Decoder\n",
        "\n",
        "As we just mentioned, the context variable $\\mathbf{c}$ of the encoder's output encodes the entire input sequence $x_1, \\ldots, x_T$. Given the output sequence $y_1, y_2, \\ldots, y_{T'}$ from the training dataset, for each time step $t'$\n",
        "(the symbol differs from the time step $t$ of input sequences or encoders),\n",
        "the probability of the decoder output $y_{t'}$ is conditional on the previous output subsequence $y_1, \\ldots, y_{t'-1}$ and the context variable $\\mathbf{c}$, i.e., $P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$.\n",
        "\n",
        "\n",
        "To model this conditional probability on sequences, we can use another RNN as the decoder. At any time step $t^\\prime$ on the output sequence, the RNN takes the output $y_{t^\\prime-1}$ from the previous time step and the context variable $\\mathbf{c}$ as its input, then transforms them and the previous hidden state $\\mathbf{s}_{t^\\prime-1}$ into the hidden state $\\mathbf{s}_{t^\\prime}$ at the current time step.\n",
        "\n",
        "As a result, we can use a function $g$ to express the transformation of the decoder's hidden layer:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{s}_{t^\\prime} = g(y_{t^\\prime-1}, \\mathbf{c}, \\mathbf{s}_{t^\\prime-1})\n",
        "\\end{equation}\n",
        "\n",
        "After obtaining the hidden state of the decoder, we can use an output layer and the softmax operation to compute the conditional probability distribution\n",
        "$P(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$ for the output at time step $t^\\prime$.\n",
        "\n",
        "Following `fig_seq2seq`, when implementing the decoder as follows, we directly use the hidden state at the final time step of the encoder to initialize the hidden state of the decoder.\n",
        "\n",
        "This requires that the RNN encoder and the RNN decoder have the same number of layers and hidden units. To further incorporate the encoded input sequence information, the context variable is concatenated with the decoder input at all the time steps. To predict the probability distribution of the output token,\n",
        "a fully-connected layer is used to transform the hidden state at the final layer of the RNN decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "51DuPjqtpXUz"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqDecoder(d2l.Decoder):\n",
        "  \"\"\"\n",
        "  RNN decoder for sequence to sequence learning.\n",
        "  Seq2SeqDecoder has the following structure:\n",
        "  nn.Embedding(vocab_size, embed_size) # Embedding Layer\n",
        "  nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) # RNN Layer\n",
        "  nn.Linear(num_hiddens, vocab_size) # Fully connected layer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                dropout=0, **kwargs):\n",
        "    \"\"\"\n",
        "    Initialize parameters of Seq2SeqDecoder\n",
        "\n",
        "    Args:\n",
        "      num_layers: int\n",
        "        Number of layers in GRU/RNN\n",
        "      num_hiddens: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Size of vocabulary\n",
        "      embed_size: int\n",
        "        Size of embedding\n",
        "      dropout: int\n",
        "        Dropout [default: 0]\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "    self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
        "                      dropout=dropout)\n",
        "    self.dense = nn.Linear(num_hiddens, vocab_size)\n",
        "\n",
        "  def init_state(self, enc_outputs, *args):\n",
        "    \"\"\"\n",
        "    Initialise Seq2SeqDecoder state\n",
        "\n",
        "    Args:\n",
        "      enc_outputs: Seq2SeqEncoder instance\n",
        "        Output of the Seq2SeqEncoder\n",
        "\n",
        "    Returns:\n",
        "      Init state of Seq2SeqDecoder as enc_outputs\n",
        "    \"\"\"\n",
        "    return enc_outputs[1]\n",
        "\n",
        "  def forward(self, X, state):\n",
        "    \"\"\"\n",
        "    Forward pass of Seq2SeqDecoder\n",
        "\n",
        "    Args:\n",
        "      X: torch.tensor\n",
        "        Input features\n",
        "      state: Seq2SeqEncoder instance\n",
        "        Output of the Seq2SeqEncoder\n",
        "\n",
        "    Returns:\n",
        "      output: torch.tensor\n",
        "        Output with shape (`batch_size`, `num_steps`, `vocab_size`)\n",
        "      state: torch.tensor\n",
        "        State with shape (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "    \"\"\"\n",
        "    # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
        "    X = self.embedding(X).permute(1, 0, 2)\n",
        "    # Broadcast `context` so it has the same `num_steps` as `X`\n",
        "    context = state[-1].repeat(X.shape[0], 1, 1)\n",
        "    X_and_context = torch.cat((X, context), 2)\n",
        "    output, state = self.rnn(X_and_context, state)\n",
        "    output = self.dense(output).permute(1, 0, 2)\n",
        "    # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "    return output, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8axwnGp5pXUz"
      },
      "source": [
        "To illustrate the implemented decoder,\n",
        "below we instantiate it with the same hyperparameters from the aforementioned encoder.\n",
        "As we can see, the output shape of the decoder becomes (batch size, number of time steps, vocabulary size),\n",
        "where the last dimension of the tensor stores the predicted token distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Ro9TjOMNpXUz"
      },
      "outputs": [],
      "source": [
        "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
        "                         num_layers=2)\n",
        "state = decoder.init_state(encoder(X))\n",
        "output, state = decoder(X, state)\n",
        "output.shape, len(state), state[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "R_k7hjRSpXUz"
      },
      "source": [
        "## Section 3.2: Loss Function\n",
        "\n",
        "At each time step, the decoder predicts a probability distribution for the output tokens. Similar to language modeling, we can apply softmax to obtain the distribution and calculate the cross-entropy loss for optimization. Recall that the special padding tokens are appended to the end of sequences so sequences of varying lengths can be efficiently loaded in minibatches of the same shape.\n",
        "However, prediction of padding tokens should be excluded from loss calculations.\n",
        "\n",
        "To this end, we can use the following `sequence_mask` function to mask irrelevant entries with zero values so later multiplication of any irrelevant prediction with zero equals to zero. For example, if the valid length of two sequences excluding padding tokens (i.e., pads each sequence to the same length usually matching the longest sequence) are one and two, respectively, the remaining entries after the first one and the first two entries are cleared to zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "NZUf6aG8pXUz"
      },
      "outputs": [],
      "source": [
        "def sequence_mask(X, valid_len, value=0):\n",
        "  \"\"\"\n",
        "  Mask irrelevant entries in sequences.\n",
        "\n",
        "  Args:\n",
        "    X: torch.tensor\n",
        "      Unmasked sequence as input\n",
        "    valid_len: torch.tensor\n",
        "      Valid Length\n",
        "    value: int\n",
        "      Mask valur\n",
        "\n",
        "  Returns:\n",
        "    X: torch.tensor\n",
        "      Output post masking\n",
        "  \"\"\"\n",
        "  maxlen = X.size(1)\n",
        "  mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                      device=X.device)[None, :] < valid_len[:, None]\n",
        "  X[~mask] = value\n",
        "  return X\n",
        "\n",
        "\n",
        "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(sequence_mask(X, torch.tensor([1, 2])))\n",
        "X = torch.ones(2, 3, 4)\n",
        "print(sequence_mask(X, torch.tensor([1, 2]), value=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "uKN97afypXUz"
      },
      "source": [
        "Now we can extend the softmax cross-entropy loss\n",
        "to allow the masking of irrelevant predictions.\n",
        "Initially,\n",
        "masks for all the predicted tokens are set to one.\n",
        "Once the valid length is given,\n",
        "the mask corresponding to any padding token\n",
        "will be cleared to zero.\n",
        "In the end,\n",
        "the loss for all the tokens\n",
        "will be multiplied by the mask to filter out\n",
        "irrelevant predictions of padding tokens in the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "vpCC9mJ2pXUz"
      },
      "outputs": [],
      "source": [
        "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
        "  \"\"\"\n",
        "  The softmax cross-entropy loss with masks.\n",
        "  \"\"\"\n",
        "\n",
        "  def forward(self, pred, label, valid_len):\n",
        "    \"\"\"\n",
        "    Forward pass of MaskedSoftmaxCELoss\n",
        "\n",
        "    Args:\n",
        "      pred: torch.tensor\n",
        "        Predictions of shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "      label: torch.tensor\n",
        "        Label of shape: (`batch_size`, `num_steps`)\n",
        "      valid_len: torch.tensor\n",
        "        Valid Length of shape (`batch_size`,)\n",
        "\n",
        "    Returns:\n",
        "      weighted_loss: float\n",
        "        Weighted Loss\n",
        "    \"\"\"\n",
        "    weights = torch.ones_like(label)\n",
        "    weights = sequence_mask(weights, valid_len)\n",
        "    self.reduction = 'none'\n",
        "    unweighted_loss = super(MaskedSoftmaxCELoss,\n",
        "                            self).forward(pred.permute(0, 2, 1), label)\n",
        "    weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
        "\n",
        "    return weighted_loss\n",
        "\n",
        "\n",
        "loss = MaskedSoftmaxCELoss()\n",
        "loss(torch.ones(3, 4, 10),\n",
        "     torch.ones((3, 4), dtype=torch.long),\n",
        "     torch.tensor([4, 2, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-vjv22QjpXU0"
      },
      "source": [
        "In the following training loop,\n",
        "we concatenate the special beginning-of-sequence token\n",
        "and the original output sequence excluding the final token as\n",
        "the input to the decoder.\n",
        "This is called *teacher forcing* because\n",
        "the original output sequence (token labels) is fed into the decoder.\n",
        "Alternatively,\n",
        "we could also feed the *predicted* token\n",
        "from the previous time step\n",
        "as the current input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "CIMkIWPtpXU0"
      },
      "outputs": [],
      "source": [
        "#@title Training\n",
        "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
        "  \"\"\"\n",
        "  Train a model for sequence to sequence.\n",
        "  \"\"\"\n",
        "\n",
        "  def xavier_init_weights(m):\n",
        "    \"\"\"\n",
        "    Function to initialise weights\n",
        "\n",
        "    Args:\n",
        "      m: nn.module\n",
        "        Type of layer\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    if type(m) == nn.Linear:\n",
        "      nn.init.xavier_uniform_(m.weight)\n",
        "    if type(m) == nn.GRU:\n",
        "      for param in m._flat_weights_names:\n",
        "        if \"weight\" in param:\n",
        "          nn.init.xavier_uniform_(m._parameters[param])\n",
        "\n",
        "\n",
        "  net.apply(xavier_init_weights)\n",
        "  net.to(device)\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss = MaskedSoftmaxCELoss()\n",
        "  net.train()\n",
        "  animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                          xlim=[10, num_epochs])\n",
        "  for epoch in range(num_epochs):\n",
        "    timer = d2l.Timer()\n",
        "    metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "    for batch in data_iter:\n",
        "      optimizer.zero_grad()\n",
        "      X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
        "      bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
        "                          device=device).reshape(-1, 1)\n",
        "      dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
        "      Y_hat, _ = net(X, dec_input, X_valid_len)\n",
        "      l = loss(Y_hat, Y, Y_valid_len)\n",
        "      l.sum().backward()  # Make the loss scalar for `backward`\n",
        "      d2l.grad_clipping(net, 1)\n",
        "      num_tokens = Y_valid_len.sum()\n",
        "      optimizer.step()\n",
        "      with torch.no_grad():\n",
        "          metric.add(l.sum(), num_tokens)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      animator.add(epoch + 1, (metric[0] / metric[1],))\n",
        "  print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
        "        f'tokens/sec on {str(device)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "IfmBT074pXU0"
      },
      "source": [
        "Now we can create and train an RNN encoder-decoder model\n",
        "for sequence to sequence learning on the machine translation dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "2HMMgJdipXU0"
      },
      "outputs": [],
      "source": [
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
        "batch_size, num_steps = 64, 10\n",
        "lr, num_epochs = 0.005, 300\n",
        "\n",
        "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
        "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
        "                         dropout)\n",
        "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
        "                         dropout)\n",
        "net = d2l.EncoderDecoder(encoder, decoder)\n",
        "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rI0O-MxfpXU0"
      },
      "source": [
        "To predict the output sequence token by token, at each decoder time step\n",
        "the predicted token from the previous time step is fed into the decoder as an input.\n",
        "\n",
        "Similar to training, at the initial time step the beginning-of-sequence (\"&lt;bos&gt;\") token is fed into the decoder. This prediction process is illustrated in `seq2seq` figure. When the end-of-sequence (\"&lt;eos&gt;\") token is predicted, the prediction of the output sequence is complete.\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/seq2seq-predict.svg\">\n",
        "  <figcaption>\n",
        "  Source <a href=\"https://d2l.ai/\">d2l.ai</a>\n",
        "  </figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "PEahDoeupXU0"
      },
      "outputs": [],
      "source": [
        "# @title Prediction\n",
        "def predict_seq2seq(net, src_sentence, src_vocab,\n",
        "                    tgt_vocab, num_steps,\n",
        "                    device, save_attention_weights=False):\n",
        "  \"\"\"\n",
        "  Predict for sequence to sequence.\n",
        "\n",
        "  Args:\n",
        "    net: nn.module\n",
        "      Instance of model\n",
        "    src_sentence: string\n",
        "      Source Sentence\n",
        "    src_vocab: dict\n",
        "      Source vocabulary\n",
        "    tgt_vocab: dict\n",
        "      Target vocabulary\n",
        "    num_steps: int\n",
        "      Number of steps\n",
        "    save_attention_weights: boolean\n",
        "      If true, save attention weights\n",
        "    device: string\n",
        "      If available, GPU/CUDA. CPU otherwise.\n",
        "\n",
        "  Returns:\n",
        "    Sequence predicted using tokenized target vocabulary\n",
        "    obtained through attention weights\n",
        "  \"\"\"\n",
        "  # Set `net` to eval mode for inference\n",
        "  net.eval()\n",
        "  src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
        "      src_vocab['<eos>']]\n",
        "  enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
        "  src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
        "\n",
        "  # Add the batch axis\n",
        "  enc_X = torch.unsqueeze(\n",
        "      torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
        "  enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
        "  dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
        "\n",
        "  # Add the batch axis\n",
        "  dec_X = torch.unsqueeze(\n",
        "      torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device),\n",
        "      dim=0)\n",
        "  output_seq, attention_weight_seq = [], []\n",
        "  for _ in range(num_steps):\n",
        "    Y, dec_state = net.decoder(dec_X, dec_state)\n",
        "\n",
        "    # We use the token with the highest prediction likelihood as the input\n",
        "    # of the decoder at the next time step\n",
        "    dec_X = Y.argmax(dim=2)\n",
        "    pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
        "\n",
        "    # Save attention weights (to be covered later)\n",
        "    if save_attention_weights:\n",
        "        attention_weight_seq.append(net.decoder.attention_weights)\n",
        "\n",
        "    # Once the end-of-sequence token is predicted, the generation of the\n",
        "    # output sequence is complete\n",
        "    if pred == tgt_vocab['<eos>']:\n",
        "        break\n",
        "    output_seq.append(pred)\n",
        "  return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "uuP7Ud6cpXU1"
      },
      "source": [
        "We can evaluate a predicted sequence by comparing it with the label sequence (the ground-truth). BLEU (Bilingual Evaluation Understudy), though originally proposed for evaluating machine translation results in [Papieni et al., 2002](https://dl.acm.org/doi/10.3115/1073083.1073135), has been extensively used in measuring the quality of output sequences for different applications.\n",
        "\n",
        "In principle, for any $n$-grams in the predicted sequence, BLEU evaluates whether this $n$-grams appears in the label sequence.\n",
        "\n",
        "Denote by $p_n$ the precision of $n$-grams, which is the ratio of the number of matched $n$-grams in the predicted and label sequences to the number of $n$-grams in the predicted sequence.\n",
        "To explain, given a label sequence $A$, $B$, $C$, $D$, $E$, $F$, and a predicted sequence $A$, $B$, $B$, $C$, $D$, we have $p_1 = 4/5$,  $p_2 = 3/4$, $p_3 = 1/3$, and $p_4 = 0$.\n",
        "\n",
        "Besides, let $\\mathrm{len}_{\\text{label}}$ and $\\mathrm{len}_{\\text{pred}}$\n",
        "be the numbers of tokens in the label sequence and the predicted sequence, respectively.\n",
        "\n",
        "Then, BLEU is defined as\n",
        "\n",
        "\\begin{equation}\n",
        "\\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len}_{\\text{label}}}{\\mathrm{len}_{\\text{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n},\n",
        "\\end{equation}\n",
        "\n",
        "where $k$ is the longest $n$-grams for matching.\n",
        "\n",
        "Based on the definition of BLEU in the above equation, whenever the predicted sequence is the same as the label sequence, BLEU is 1.\n",
        "\n",
        "Moreover, since matching longer $n$-grams is more difficult, BLEU assigns a greater weight to a longer $n$-gram precision. Specifically, when $p_n$ is fixed, $p_n^{1/2^n}$ increases as $n$ grows (the original paper uses $p_n^{1/n}$).\n",
        "\n",
        "Furthermore, since predicting shorter sequences tends to obtain a higher $p_n$ value, the coefficient before the multiplication term in the above equation\n",
        "penalizes shorter predicted sequences.\n",
        "\n",
        "For example, when $k=2$, given the label sequence $A$, $B$, $C$, $D$, $E$, $F$ and the predicted sequence $A$, $B$, although $p_1 = p_2 = 1$, the penalty factor $\\exp(1-6/2) \\approx 0.14$ lowers the BLEU.\n",
        "\n",
        "We implement the BLEU measure as follows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "9rXTBsb4pXU1"
      },
      "outputs": [],
      "source": [
        "#@title Evaluation of Predicted Sequences\n",
        "def bleu(pred_seq, label_seq, k):\n",
        "  \"\"\"\n",
        "  Compute the BLEU Score\n",
        "\n",
        "  Args:\n",
        "    pred_seq: string\n",
        "      Predicted Sequence\n",
        "    label_seq: string\n",
        "      Ground truth\n",
        "    k: int\n",
        "      Number of iterations\n",
        "\n",
        "  Returns:\n",
        "    score: float\n",
        "      BLEU score\n",
        "      The score between 0 and 1, indicates how\n",
        "      similar the predicted and reference statements are.\n",
        "  \"\"\"\n",
        "  pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
        "  len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
        "  score = math.exp(min(0, 1 - len_label / len_pred))\n",
        "  for n in range(1, k + 1):\n",
        "    num_matches, label_subs = 0, collections.defaultdict(int)\n",
        "    for i in range(len_label - n + 1):\n",
        "      label_subs[''.join(label_tokens[i:i + n])] += 1\n",
        "    for i in range(len_pred - n + 1):\n",
        "      if label_subs[''.join(pred_tokens[i:i + n])] > 0:\n",
        "        num_matches += 1\n",
        "        label_subs[''.join(pred_tokens[i:i + n])] -= 1\n",
        "    score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
        "  return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UQMNpHFIpXU1"
      },
      "source": [
        "In the end,\n",
        "we use the trained RNN encoder-decoder\n",
        "to translate a few English sentences into French\n",
        "and compute the BLEU of the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Ryq9ewvCpXU3"
      },
      "outputs": [],
      "source": [
        "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
        "# fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
        "fras = [ 'je suis chez moi .', 'j\\'ai perdu .','va !', 'il est calme .']\n",
        "for eng, fra in zip(engs, fras):\n",
        "  translation, attention_weight_seq = predict_seq2seq(net,\n",
        "                                                      eng,\n",
        "                                                      src_vocab,\n",
        "                                                      tgt_vocab,\n",
        "                                                      num_steps,\n",
        "                                                      DEVICE)\n",
        "  print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jSY6V2gcpXU3"
      },
      "source": [
        "---\n",
        "# Section 4: Ethical aspects\n",
        "\n",
        "*Time estimate: ~7mins*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "iNTkcrAopXU3"
      },
      "outputs": [],
      "source": [
        "# @title Video 5: Ethics of Representation and Generation\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1v54y1J7SC\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"2ULMWtaGQrQ\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 5: Ethics of Representation and Generation')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "yKbpSSFLpXU3"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "During this day, we have learned about modern RNNs and their variants. Now let's see some ethical aspects of representation and Generation, and then we will close the tutorials with an overview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "bBQQd9Q5pXU4"
      },
      "outputs": [],
      "source": [
        "# @title Video 6: Beyond Sequence\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1864y1x7Ek\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"lVcpMSJileA\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 6: Beyond Sequence')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "RbtcN01JpXU4"
      },
      "outputs": [],
      "source": [
        "# @title Airtable Submission Link\n",
        "from IPython import display as IPydisplay\n",
        "IPydisplay.HTML(\n",
        "   f\"\"\"\n",
        " <div>\n",
        "   <a href= \"{atform.url()}\" target=\"_blank\">\n",
        "   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\"\n",
        " alt=\"button link end of day Survey\" style=\"width:410px\"></a>\n",
        "   </div>\"\"\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rBgikshepXU4"
      },
      "source": [
        "---\n",
        "# Bonus: Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "Txfn721XpXU4"
      },
      "outputs": [],
      "source": [
        "# @title Video 7: Attention mechanisms\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1Mo4y1S7oK\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"Vt9GhJKcEiM\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 7: Attention mechanisms')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "_0-NyhtppXU4"
      },
      "source": [
        "Previously, we designed an encoder-decoder architecture based on two RNNs for sequence to sequence learning. Specifically, the RNN encoder transforms a variable-length sequence into a fixed-shape context variable, then the RNN decoder generates the output (target) sequence token by token based on the generated tokens and the context variable. However, even though not all the input (source) tokens are useful for decoding a certain token, the same context variable that encodes the entire input sequence is still used at each decoding step. It is challenging for the models to deal with long sentences. \n",
        "\n",
        "In [Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473), the authors proposed a technique called attention. When predicting a token, if not all the input tokens are relevant, the model aligns (or attends) only to parts of the input sequence that are relevant to the current prediction. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "_9cFs304pXU4"
      },
      "source": [
        "In contrast to seq2seq model, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder. \n",
        "\n",
        "In order to focus on the parts of input relevant to the decoder, look at the set of encoder hidden states it received. Each encoder hidden state is at most associated with a certain word in the input sentence. We can assign each hidden state a score and multiply it with the softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores.\n",
        "\n",
        "Reference Links:\n",
        "\n",
        "* [https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
        "* [https://d2l.ai/chapter_attention-mechanisms/attention-cues.html](https://d2l.ai/chapter_attention-mechanisms/attention-cues.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "vYgZpixUpXU4"
      },
      "outputs": [],
      "source": [
        "# @markdown Media 1: Sequence to Sequence model with Attention\n",
        "\n",
        "url = \"https://jalammar.github.io/images/seq2seq_7.mp4\"\n",
        "from IPython.display import HTML\n",
        "HTML(f\"\"\"<video src={url} width=750 controls/>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "uq2vJoV1pXU5"
      },
      "outputs": [],
      "source": [
        "# @markdown Media 2: Mapping input to output\n",
        "\n",
        "url = \"https://jalammar.github.io/images/seq2seq_9.mp4\"\n",
        "from IPython.display import HTML\n",
        "HTML(f\"\"\"<video src={url} width=750 controls/>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "82_zi20KpXU5"
      },
      "source": [
        "## Queries, Keys, and Values\n",
        "\n",
        "\n",
        "To calculate the attention mechanism we make use of Queries, Keys and Values. But what are Queries, Keys and Values? Query, Value and Key are the transformations of the input vector. \n",
        "\n",
        "In an attention mechanism the context vector is computed as a weighted sum of values, where the weight assigned to each value is computed through an attention score. The score is usually the dot product between the query and key. The scores then go through the softmax function to yield a set of weights whose sum equals 1.\n",
        "\n",
        "The query is from the decoder hidden state whereas the key and value are from the encoder hidden state. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "F9YdzQ85pXU5"
      },
      "source": [
        "Take a minute and look at this [article](https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html). It has detailed graphical explanation on how to calculate attention scores. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "C0cH0_oqpXU5"
      },
      "source": [
        "### Bonus Coding Exercise: Attention for Text Classification\n",
        "\n",
        "Until now, we looked at attention aimed at seq2seq networks. Let’s try implementing attention for the above IMDB sentiment analysis dataset. Previously, using the LSTM, the classification completely depended on the last hidden state. In this exercise, we will compute the attention scores between the last hidden state and output of each sequence. The final attention vector will be the weighted average of the outputs at each sequence, with the weights being the attention scores. Lastly, we will concatenate the attention vector and the last hidden state to get the final output.\n",
        "\n",
        "For simplicity's sake, let's implement attention over an LSTM with 1 layer.\n",
        "\n",
        "\n",
        "[*Code reference*](https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/main.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "memYsOMEpXU5"
      },
      "outputs": [],
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Attention Model with following structure:\n",
        "  nn.Embedding(vocab_size, embedding_length) + nn.Parameter(weights, requires_grad=False) # Embedding Layer\n",
        "  nn.LSTM(embedding_length, hidden_size) # LSTM layer\n",
        "  nn.Linear(2*hidden_size, output_size) # First Fully Connected layer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, batch_size, output_size, hidden_size, vocab_size,\n",
        "               embedding_length, weights, device):\n",
        "    \"\"\"\n",
        "    Initialize parameters of AttentionModel\n",
        "\n",
        "    Args:\n",
        "      batch_size: int\n",
        "        Batch size\n",
        "      output_size: int\n",
        "        Size of output layer\n",
        "      hidden_size: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Vocabulary size\n",
        "      weights: torch.tensor\n",
        "        Attention Weights\n",
        "      device: string\n",
        "        GPU/CUDA if available. CPU otherwise.\n",
        "      embedding_length: int\n",
        "        Length of the embeddding\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "    self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "    self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "    self.fc1 = nn.Linear(2*hidden_size, output_size)\n",
        "    self.device = device\n",
        "    self.num_seq = sentence_length\n",
        "\n",
        "  def attention_net(self, lstm_output, final_state):\n",
        "    \"\"\"\n",
        "    Returns hidden states based on AttentionNet\n",
        "\n",
        "    Args:\n",
        "      lstm_output : torch.tensor\n",
        "        LSTM Output of shape: (num_seq, batch_size, hidden_size)\n",
        "      final_state : torch.tensor\n",
        "        Final State of shape: (1, batch_size, hidden_size)\n",
        "\n",
        "    Returns:\n",
        "      new_hidden_state: torch.tensor\n",
        "        Weighted LSTM output\n",
        "    \"\"\"\n",
        "    ####################################################\n",
        "    # Implement the AttentionNet\n",
        "    # Fill in missing code below (...)\n",
        "    raise NotImplementedError(\"perform the convolution\")\n",
        "    ####################################################\n",
        "    # Permute the output to get the shape (batch_size, num_seq, hidden_size)\n",
        "    # Get the attention weights\n",
        "    # Use torch.bmm to compute the attention weights between each output and last hidden state\n",
        "    # Pay attention to the tensor shapes, you may have to use squeeze and unsqueeze functions\n",
        "    # Softmax the attention weights\n",
        "    # Get the new hidden state, use torch.bmm to get the weighted lstm output\n",
        "    # Pay attention to the tensor shapes, you may have to use squeeze and unsqueeze functions\n",
        "    lstm_output = ...\n",
        "    hidden = ...\n",
        "    attn_weights = ...  # Expected shape: (batch_size, num_seq)\n",
        "    soft_attn_weights = ...\n",
        "    new_hidden_state = ...\n",
        "\n",
        "    return new_hidden_state\n",
        "\n",
        "  def forward(self, input_sentences):\n",
        "    \"\"\"\n",
        "    Forward pass of NeuralNet\n",
        "\n",
        "    Args:\n",
        "      input_sentences: string\n",
        "        Input Sentences\n",
        "\n",
        "    Returns:\n",
        "      logits: torch.tensor\n",
        "        Output of the final fully connected layer\n",
        "    \"\"\"\n",
        "    input = self.word_embeddings(input_sentences)\n",
        "    input = input.permute(1, 0, 2)\n",
        "\n",
        "    h_0 = torch.zeros(1, input.shape[1], self.hidden_size).to(self.device)\n",
        "    c_0 = torch.zeros(1, input.shape[1], self.hidden_size).to(self.device)\n",
        "\n",
        "    output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "    attn_output = self.attention_net(output, final_hidden_state)\n",
        "    final_output = torch.cat((attn_output, final_hidden_state[0]), 1)\n",
        "    logits = self.fc1(final_output)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "# Uncomment to check AttentionModel class\n",
        "# attention_model = AttentionModel(32, 2, 16, 20, 200, TEXT.vocab.vectors, DEVICE)\n",
        "# print(attention_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "YX3mDeDBpXU5"
      },
      "outputs": [],
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Attention Model with following structure:\n",
        "  nn.Embedding(vocab_size, embedding_length) + nn.Parameter(weights, requires_grad=False) # Embedding Layer\n",
        "  nn.LSTM(embedding_length, hidden_size) # LSTM layer\n",
        "  nn.Linear(2*hidden_size, output_size) # First Fully Connected layer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, sentence_length, weights, device):\n",
        "    \"\"\"\n",
        "    Initialize parameters of AttentionModel\n",
        "\n",
        "    Args:\n",
        "      batch_size: int\n",
        "        Batch size\n",
        "      output_size: int\n",
        "        Size of output layer\n",
        "      hidden_size: int\n",
        "        Size of hidden layer\n",
        "      vocab_size: int\n",
        "        Vocabulary size\n",
        "      weights: torch.tensor\n",
        "        Attention Weights\n",
        "      device: string\n",
        "        GPU/CUDA if available. CPU otherwise.\n",
        "      embedding_length: int\n",
        "        Length of the embeddding\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "    self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "    self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "    self.fc1 = nn.Linear(2*hidden_size, output_size)\n",
        "    self.device = device\n",
        "    self.num_seq = sentence_length\n",
        "\n",
        "  def attention_net(self, lstm_output, final_state, batch_size=32):\n",
        "    \"\"\"\n",
        "    Returns hidden states based on AttentionNet\n",
        "\n",
        "    Args:\n",
        "      lstm_output : torch.tensor\n",
        "        LSTM Output of shape: (num_seq, batch_size, hidden_size)\n",
        "      final_state : torch.tensor\n",
        "        Final State of shape: (1, batch_size, hidden_size)\n",
        "\n",
        "    Returns:\n",
        "      new_hidden_state: torch.tensor\n",
        "        Weighted LSTM output\n",
        "    \"\"\"\n",
        "    # Permute the output to get the shape (batch_size, num_seq, hidden_size)\n",
        "    # Get the attention weights\n",
        "    # Use torch.bmm to compute the attention weights between each output and last hidden state\n",
        "    # Pay attention to the tensor shapes, you may have to use squeeze and unsqueeze functions\n",
        "    # Softmax the attention weights\n",
        "    # Get the new hidden state, use torch.bmm to get the weighted lstm output\n",
        "    # Pay attention to the tensor shapes, you may have to use squeeze and unsqueeze functions\n",
        "    lstm_output = lstm_output.permute(1, 0, 2)\n",
        "    hidden = final_state.squeeze(0).unsqueeze(2)\n",
        "    attn_weights = torch.matmul(lstm_output, hidden)\n",
        "    attn_weights = torch.reshape(attn_weights, ([batch_size,self.num_seq])) # Expected shape: (batch_size, num_seq)\n",
        "    soft_attn_weights = F.softmax(attn_weights, 1)\n",
        "    new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "    return new_hidden_state\n",
        "\n",
        "  def forward(self, input_sentences):\n",
        "    \"\"\"\n",
        "    Forward pass of NeuralNet\n",
        "\n",
        "    Args:\n",
        "      input_sentences: string\n",
        "        Input Sentences\n",
        "\n",
        "    Returns:\n",
        "      logits: torch.tensor\n",
        "        Output of the final fully connected layer\n",
        "    \"\"\"\n",
        "    input = self.word_embeddings(input_sentences)\n",
        "    input = input.permute(1, 0, 2)\n",
        "    h_0 = torch.zeros(1, input.shape[1], self.hidden_size).to(self.device)\n",
        "    c_0 = torch.zeros(1, input.shape[1], self.hidden_size).to(self.device)\n",
        "    output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "    attn_output = self.attention_net(output, final_hidden_state, input.shape[1])\n",
        "    final_output = torch.cat((attn_output, final_hidden_state[0]), 1)\n",
        "    logits = self.fc1(final_output)\n",
        "    return logits\n",
        "\n",
        "# Uncomment to check AttentionModel class\n",
        "attention_model = AttentionModel(32, 2, 16, 20, 200, 50, TEXT.vocab.vectors, DEVICE)\n",
        "print(attention_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "uX0YlEu_pXU5"
      },
      "source": [
        "```\n",
        "AttentionModel(\n",
        "  (word_embeddings): Embedding(20, 200)\n",
        "  (lstm): LSTM(200, 16)\n",
        "  (fc1): Linear(in_features=32, out_features=2, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "1ItyHRZOpXU6"
      },
      "outputs": [],
      "source": [
        "# @markdown Reload dataset using the default params since variables have been overwritten\n",
        "TEXT, vocab_size, train_iter, valid_iter, test_iter = load_dataset(seed=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "j3K7wBDIpXU6"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001\n",
        "batch_size = 32  # Initially was 16\n",
        "output_size = 2\n",
        "hidden_size = 16\n",
        "embedding_length = 300\n",
        "epochs = 10  # Initially was 12\n",
        "sentence_length = 50\n",
        "\n",
        "word_embeddings = TEXT.vocab.vectors\n",
        "vocab_size = len(TEXT.vocab)\n",
        "\n",
        "attention_model = AttentionModel(batch_size,\n",
        "                                 output_size,\n",
        "                                 hidden_size,\n",
        "                                 vocab_size,\n",
        "                                 embedding_length, sentence_length,\n",
        "                                 word_embeddings,\n",
        "                                 DEVICE)\n",
        "attention_model.to(DEVICE)\n",
        "attention_start_time = time.time()\n",
        "set_seed(SEED)\n",
        "attention_train_loss, attention_train_acc, attention_validation_loss, attention_validation_acc = train(attention_model,\n",
        "                                                                                                       DEVICE,\n",
        "                                                                                                       train_iter,\n",
        "                                                                                                       valid_iter,\n",
        "                                                                                                       epochs,\n",
        "                                                                                                       learning_rate)\n",
        "print(\"--- Time taken to train = %s seconds ---\" % (time.time() - attention_start_time))\n",
        "test_accuracy = test(attention_model, DEVICE, test_iter)\n",
        "print(f'\\n\\nTest Accuracy: {test_accuracy}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "eTokeM8gpXU6"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.subplot(211)\n",
        "plot_train_val(np.arange(0, epochs),\n",
        "               attention_train_acc,\n",
        "               attention_validation_acc,\n",
        "               'train accuracy',\n",
        "               'val accuracy',\n",
        "               'attention on IMDB text classification',\n",
        "               'loss',\n",
        "               color='C0')\n",
        "plt.legend(loc='upper left')\n",
        "plt.subplot(212)\n",
        "plot_train_val(np.arange(0, epochs),\n",
        "               attention_train_loss,\n",
        "               attention_validation_loss,\n",
        "               'train loss',\n",
        "               'val loss',\n",
        "               '',\n",
        "               'loss',\n",
        "               color='C1')\n",
        "plt.tight_layout()\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "W2D3_Tutorial2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f9ca39261bc4758a53bf83c39562647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TabModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TabModel",
            "_titles": {
              "0": "Youtube",
              "1": "Bilibili"
            },
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TabView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9420890745a842d7b6b29e4a13ce12f0",
              "IPY_MODEL_3ae27b023f1f4b3e8d9717ced0275f19"
            ],
            "layout": "IPY_MODEL_3e18e895f3db49ea80595ade44b44312",
            "selected_index": 0
          }
        },
        "9420890745a842d7b6b29e4a13ce12f0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f9604ed2c474411d9f95c88cc78ded72",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://youtube.com/watch?v=hIHocwdyY7M\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f6d4537e190>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://www.youtube.com/embed/hIHocwdyY7M?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        ",
                  "image/jpeg": "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\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "3ae27b023f1f4b3e8d9717ced0275f19": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_7f5b9f32614f4ba19535f7042ae85d20",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://www.bilibili.com/video/BV1Ng41177az\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<__main__.BiliVideo at 0x7f6d4537e3d0>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1Ng41177az&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
                },
                "metadata": {}
              }
            ]
          }
        },
        "3e18e895f3db49ea80595ade44b44312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9604ed2c474411d9f95c88cc78ded72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f5b9f32614f4ba19535f7042ae85d20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}