{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/W2D5_Initial/tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuromatch Academy: Week 2, Day 5, Tutorial 1\n",
    "\n",
    "# VAEs and GANs : Intro to GANs\n",
    "\n",
    "__Content creators:__ Kai Xu, Seungwook Han, Akash Srivastava\n",
    "\n",
    "__Content reviewers:__ Polina Turishcheva, Melvin Selim Atay, Hadi Vafaei, Deepak Raya\n",
    "\n",
    "__Production editors:__ Arush Tagade, Spiros Chavlis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tutorial Objectives\n",
    "\n",
    "The goal of this tutorial is to introduce GANs training.\n",
    "\n",
    "By the end of this tutorial you will be able to:\n",
    "- Understand, at a high level, how GANs are implemented.\n",
    "- Understand the training dynamics of GANs. \n",
    "- Know about a few failure modes of GAN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "outputId": "45ab80a3-325a-433f-b431-72ad6d220838"
   },
   "outputs": [],
   "source": [
    "#@markdown Tutorial slides\n",
    "# you should link the slides for all tutorial videos here (we will store pdfs on osf)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://docs.google.com/presentation/d/10YzNRq57FdXMaWGQKBHMErOnfqM1ou8Q\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: How to train GANs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "568d000c-fe62-4e2d-ebaf-50e74ca47976"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Generative Adversarial Networks\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"aY9ANHYeJfQ\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs consist two networks: A critic or discriminator (`disc`) and a generator (`gen`) that are trained by alternating between the following two steps:\n",
    "- In step 1, we update the parameters (`disc.params`) of the discriminator by backpropogating through the discriminator loss (BCE loss) `disc.loss`.\n",
    "- In step 2, we update the parameters (`gen.params`) of the generator by back propogating throught the generator loss, `gen.loss`.\n",
    "\n",
    "We will now implement a simple GAN training loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 1: The GAN training loop\n",
    "\n",
    "To get you started we have implemented a simple GAN in pseudocode. All you have to do is to implement the training loop.\n",
    "\n",
    "__Your goal__ is to arrange the functions given below in the correct order in the `train_gan_iter` function\n",
    "- `disc.loss(x_real, x_fake)`: Discriminator loss\n",
    "- `disc.classify(x)`: Classify `x` as real or fake\n",
    "- `gen.loss(x_fake, disc_fn)`: Generator loss\n",
    "- `disc_fn(x)` is a function to check `x` is real or fake.\n",
    "- `gen.sample(num_samples)`: Generate samples from the generator\n",
    "- `backprop(loss, model)`: Compute gradient of `loss` wrt `model`\n",
    "- `model` is either `disc` or `gen`\n",
    "\n",
    "We have already taken care of most of these functions. So you only have to figure out the placement of `disc.loss` and `gen.loss` functions.\n",
    "\n",
    "<!-- Note that this `train_gan_iter` is designed to give a propoer overview of the trianing loop of GANs rather than actually being functional in this notebook. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @markdown *Execute this cell to enable helper functions*\n",
    "\n",
    "def get_data():\n",
    "  return \"get_data\"\n",
    "\n",
    "\n",
    "class Disc:\n",
    "  def loss(self, x_real, x_fake):\n",
    "    assert x_real == \"get_data\" and x_fake == \"gen.sample\", \"Inputs to disc.loss is wrong\"\n",
    "  def classify(self, x):\n",
    "    return \"disc.classify\"\n",
    "\n",
    "\n",
    "class Gen:\n",
    "  def loss(self, x_fake, disc_fn):\n",
    "    assert x_fake == \"gen.sample\" and disc_fn(None) == \"disc.classify\", \"Inputs to gen.loss is wrong\"\n",
    "\n",
    "\n",
    "  def sample(self, num_samples):\n",
    "    return \"gen.sample\"\n",
    "\n",
    "\n",
    "def backprop(loss, model):\n",
    "  pass\n",
    "\n",
    "\n",
    "def update(model, grad):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_iter(data, disc, gen):\n",
    "  \"\"\"Update the discriminator (`disc`) and the generator (`gen`) using `data`\n",
    "\n",
    "  Args:\n",
    "    data (ndarray): An array of shape (N,) that contains the data\n",
    "    disc (Disc): The discriminator\n",
    "    gen (Gen): The generator\n",
    "\n",
    "  Returns:\n",
    "  \"\"\"\n",
    "  #################################################\n",
    "  # Intructions for students:                        #\n",
    "  # Fill out [TODO] in the function and remove below #\n",
    "  #################################################\n",
    "\n",
    "  # Number of samples in the data batch\n",
    "  num_samples = 200\n",
    "\n",
    "  # The data is the real samples\n",
    "  x_real = data\n",
    "\n",
    "  ## Discriminator training\n",
    "\n",
    "  # Ask the generator to generate some fake samples\n",
    "  x_fake = gen.sample(num_samples)\n",
    "\n",
    "  #################################################\n",
    "  ## TODO for students: details of what they should do ##\n",
    "  # Fill out function and remove\n",
    "  raise NotImplementedError(\"Student exercise: Write code to compute disc_loss\")\n",
    "  #################################################\n",
    "  # Compute the discriminator loss\n",
    "  disc_loss = ...\n",
    "\n",
    "  # Compute the gradient for discriminator\n",
    "  disc_grad = backprop(disc_loss, disc)\n",
    "\n",
    "  # Update the discriminator\n",
    "  update(disc, disc_grad)\n",
    "\n",
    "  ## Generator training\n",
    "\n",
    "  # Ask the generator to generate some fake samples\n",
    "  x_fake = gen.sample(num_samples)\n",
    "\n",
    "  #################################################\n",
    "  ## TODO for students: details of what they should do ##\n",
    "  # Fill out function and remove\n",
    "  raise NotImplementedError(\"Student exercise: Write code to compute gen_loss\")\n",
    "  #################################################\n",
    "  # Compute the generator loss\n",
    "  gen_loss = ...\n",
    "\n",
    "  # Compute the gradient for generator\n",
    "  gen_grad = backprop(gen_loss, gen)\n",
    "\n",
    "  # Update the generator\n",
    "  update(gen, gen_grad)\n",
    "\n",
    "  print(\"Your implementation passes the check!\")\n",
    "\n",
    "\n",
    "data = get_data()\n",
    "disc = Disc()\n",
    "gen = Gen()\n",
    "## Uncomment below to check your function\n",
    "# train_gan_iter(data, disc, gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "e4025bf0-34c2-4781-e10a-15ae7cf2edd9"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_GenerativeModels/solutions/W2D5_Tutorial1_Solution_dc5b38a9.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: The difficulty of GAN trianing\n",
    "\n",
    "In this section we will develop an intuition for the training dynamics of GANs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Demo 1: Failure modes of GAN trianing\n",
    "\n",
    "GAN training is notoriously difficult because \n",
    "it is very sensitive to hyper-parameters such form learning rate to model architecture. To help you develop a sense of this, here is a very simple GAN training demo that we have borrowed from Andrej Karpathy's website.  \n",
    "\n",
    "Even though the GAN in this demo is very simple and operates in either 1D or 2D spaces, it is however very sensitive to the learning rate. Try it for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "outputId": "e6c54d1f-9a41-48ca-ca33-2ffa070c2c24"
   },
   "outputs": [],
   "source": [
    "#@title GAN training demo\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://xukai92.github.io/gan_demo/index.html', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What makes GANs hard to train?\n",
    "\n",
    "You have played with the demo and it's time to think about a few questions\n",
    "\n",
    "1. Which target is more stable to train, 1D or 2D?\n",
    "2. If you keep increasing the learning rate, what happens? Does it happen in both the cases, i.e., 1D/2D targets?\n",
    "3. Can you think of some drawbacks of using small learning rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "Through this tutorial, we have learned\n",
    "\n",
    "- How to implement the training loop of GANs and,\n",
    "- Developed an intuition about the training dynamics of GANs "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D5_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
