{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/W2D5_Initial/tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuromatch Academy: Week 2, Day 5, Tutorial 3\n",
    "\n",
    "# VAEs and GANs : Conditional GANs\n",
    "\n",
    "__Content creators:__ Kai Xu, Seungwook Han, Akash Srivastava\n",
    "\n",
    "__Content reviewers:__ Polina Turishcheva, Melvin Selim Atay, Hadi Vafaei, Deepak Raya\n",
    "\n",
    "__Production editors:__ Arush Tagade, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tutorial Objectives\n",
    "\n",
    "The goal of this notebook is to understand conditional GANs.\n",
    "\n",
    "By the end of this tutorial you will be able to:\n",
    "- Understand the differences in conditional GANs\n",
    "- Generate high-dimensional natural images from a BigGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "outputId": "eccd4684-daa7-49dc-b93c-9a9237c24c24"
   },
   "outputs": [],
   "source": [
    "#@markdown Tutorial slides\n",
    "# you should link the slides for all tutorial videos here (we will store pdfs on osf)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://docs.google.com/presentation/d/1eP79mRMzD2Q7Utol3kZ5hooYIXRTQMMb\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2bbccad0-7eac-4d6d-89a3-8bb500d1b032"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Install Huggingface BigGAN library\n",
    "!pip install pytorch-pretrained-biggan --quiet\n",
    "!pip install Pillow libsixel-python --quiet\n",
    "\n",
    "# Import libraries\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names,\n",
    "                                       truncated_noise_sample,\n",
    "                                       save_as_images, display_in_terminal)\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "410e51fb-f2db-4700-9627-dce5307979b0"
   },
   "outputs": [],
   "source": [
    "# @title Set seed for reproducibility in Pytorch\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "def set_seed(seed):\n",
    "  \"\"\"\n",
    "  Set random seed for reproducibility\n",
    "\n",
    "  Args:\n",
    "    seed: integer\n",
    "      A positive integer to ensure reproducibility\n",
    "  \"\"\"\n",
    "  random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  print(f'Seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_seed(522)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Generating with a conditional GAN (BigGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "7e9cdf91-345b-4055-89ef-1abbdcb5a5f6"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Conditional Generative Models\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"lV6zH2xDZck\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load a pre-trained conditional GAN, BigGAN, which is the state-of-the-art model in conditional high-dimensional natural image generation, and generate samples from it. Since it is a class conditional model, we will be able to use the class label to generate images from the respective classes of objects.\n",
    "\n",
    "Read here for more details on BigGAN: https://arxiv.org/pdf/1809.11096.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load respective BigGAN model for the specified resolution (biggan-deep-128, biggan-deep-256, biggan-deep-512)\n",
    "def load_biggan(model_res):\n",
    "  return BigGAN.from_pretrained('biggan-deep-{}'.format(model_res))\n",
    "\n",
    "\n",
    "# Create class and noise vectors for sampling from BigGAN\n",
    "def create_class_noise_vectors(class_str, trunc, num_samples):\n",
    "  class_vector = one_hot_from_names([class_str]*num_samples, batch_size=num_samples)\n",
    "  noise_vector = truncated_noise_sample(truncation=trunc, batch_size=num_samples)\n",
    "\n",
    "  return class_vector, noise_vector\n",
    "\n",
    "\n",
    "# Generate samples from BigGAN\n",
    "def generate_biggan_samples(model, class_vector, noise_vector, truncation=0.4,\n",
    "                            device=device):\n",
    "  # Convert to tensor\n",
    "  noise_vector = torch.from_numpy(noise_vector)\n",
    "  class_vector = torch.from_numpy(class_vector)\n",
    "\n",
    "  # Move to GPU\n",
    "  noise_vector = noise_vector.to(device)\n",
    "  class_vector = class_vector.to(device)\n",
    "  model.to(device)\n",
    "\n",
    "  # Generate an image\n",
    "  with torch.no_grad():\n",
    "      output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "  # Back to CPU\n",
    "  output = output.to('cpu')\n",
    "\n",
    "  # The output layer of BigGAN has a tanh layer, resulting the range of [-1, 1] for the output image\n",
    "  # Therefore, we normalize the images properly to [0, 1] range.\n",
    "  # Clipping is only in case of numerical instability problems\n",
    "\n",
    "  output = torch.clip(((output.detach().clone() + 1) / 2.0), 0, 1)\n",
    "  output = output\n",
    "\n",
    "  # Make grid and show generated samples\n",
    "  output_grid = torchvision.utils.make_grid(output, nrow=min(4, output.shape[0]), padding=5)\n",
    "  plt.imshow(output_grid.permute(1,2,0))\n",
    "\n",
    "  return output_grid\n",
    "\n",
    "\n",
    "def generate(b):\n",
    "  # Create BigGAN model\n",
    "  model = load_biggan(MODEL_RESOLUTION)\n",
    "\n",
    "  # Use specified parameters (resolution, class, number of samples, etc) to generate from BigGAN\n",
    "  class_vector, noise_vector = create_class_noise_vectors(CLASS, TRUNCATION, NUM_SAMPLES)\n",
    "  samples_grid = generate_biggan_samples(model, class_vector, noise_vector, TRUNCATION, device)\n",
    "  torchvision.utils.save_image(samples_grid, 'samples.png')\n",
    "  ### If CUDA out of memory issue, lower NUM_SAMPLES (number of samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1:  Define configurations\n",
    "\n",
    "We will now define the configurations (resolution of model, number of samples, class to sample from, truncation level) under which we will sample from BigGAN. \n",
    "\n",
    "***Question***: What is the truncation trick employed by BigGAN? How does sample variety and fidelity change by varying the truncation level? (Hint: play with the truncation slider and try sampling at different levels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title { run: \"auto\" }\n",
    "\n",
    "### RUN THIS BLOCK EVERY TIME YOU CHANGE THE PARAMETERS FOR GENERATION\n",
    "\n",
    "# Resolution at which to generate\n",
    "MODEL_RESOLUTION = \"256\" #@param [128, 256, 512]\n",
    "\n",
    "# Number of images to generate\n",
    "NUM_SAMPLES = 1 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "\n",
    "# Class of images to generate\n",
    "CLASS = 'German shepherd' #@param ['tench', 'magpie', 'jellyfish', 'German shepherd', 'bee', 'acoustic guitar', 'coffee mug', 'minibus', 'monitor']\n",
    "\n",
    "# Truncation level of the normal distribution we sample z from\n",
    "TRUNCATION = 0.4 #@param {type:\"slider\", min:0.1, max:1, step:0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "991759c4fab643b481bb82236fe1f0c6",
      "659cbcfae0ce4cbbae8f773a9eedbd4d",
      "b40bcaf0d6884e5e9c5247d167915137",
      "2964c55c2a8442ec8657b1b3bf9728e4",
      "91759ada1eac4b45b0f9fcb04aa60ea3"
     ]
    },
    "outputId": "c34ccc22-7331-4d17-b4dc-0dc24291751c"
   },
   "outputs": [],
   "source": [
    "# Create generate button, given parameters specified above\n",
    "button = widgets.Button(description=\"GENERATE!\",\n",
    "                        layout=widgets.Layout(width='30%', height='80px'),\n",
    "                        button_style='danger')\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "button.on_click(generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this section, we have learned: \n",
    "- How conditional GANs differ from unconditional models\n",
    "- How to use a pre-trained BigGAN model to generate high-dimensional photo-realistic images and its tricks to modulate diversity and image fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food for thought\n",
    "\n",
    "1. How does BigGAN differ from previous state-of-the-art generative models for high-dimensional natural images? In other words, how does BigGAN solve high-dimensional image generation? (Hint: look into model architecture and training configurations) (BigGAN paper: https://arxiv.org/pdf/1809.11096.pdf) \n",
    "2. Continuing from Question 1, what are the drawbacks of introducing such techniques into training large models for high-dimensional, diverse datasets?\n",
    "3. Play with other pre-trained generative models like StyleGAN here -- where code for sampling and interpolation in the latent space is available: https://github.com/NVlabs/stylegan"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D5_Tutorial3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
