{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/w2d5_reformatting/tutorials/W2D5_GenerativeModels/W2D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuromatch Academy: Week 2, Day 5, Tutorial 1\n",
    "\n",
    "# VAEs and GANs : Conditional GANs and Implications of GAN Technology\n",
    "\n",
    "__Content creators:__ Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Spiros Chavlis \n",
    "\n",
    "*Taken from UPenn course with slide modifications*:\n",
    "__Instructor:__ Konrad Kording, __Original Content creators:__ Richard Lange, Arash Ash\n",
    "\n",
    "**ALERT**: for prepod use only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tutorial Objectives\n",
    "In the first tutorial of the *Generative Models* day, we are going to\n",
    "\n",
    "- Think about unsupervised learning and get a bird's eye view of why it is useful\n",
    "- See the connection between AutoEncoding and dimensionality reduction\n",
    "- Start thinking about neural networks as generative models\n",
    "- Put on our Bayesian hats and turn AEs into VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "outputId": "135d1e8d-e890-4c9c-8e87-1a64e964f44f"
   },
   "outputs": [],
   "source": [
    "#@markdown Tutorial slides\n",
    "# you should link the slides for all tutorial videos here (we will store pdfs on osf)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://docs.google.com/presentation/d/1_Nsq8OHIpls5iPlbA0WF53J1Ypqp02BGBGVj0zgec3c/edit?usp=sharing\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a4909cb9-5112-40f2-bfd5-92337d90cbe0"
   },
   "outputs": [],
   "source": [
    "# we need to first upgrade the Colab's TorchVision\n",
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import matplotlib.pylab as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "fig_w, fig_h = (8, 6)\n",
    "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
    "\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "def image_moments(image_batches, n_batches=None):\n",
    "    \"\"\"\n",
    "    Compute mean an covariance of all pixels from batches of images\n",
    "    \"\"\"\n",
    "    m1, m2 = torch.zeros((), device=DEVICE), torch.zeros((), device=DEVICE)\n",
    "    n = 0\n",
    "    for im in tqdm(image_batches, total=n_batches, leave=False,\n",
    "                   desc='Computing pixel mean and covariance...'):\n",
    "        im = im.to(DEVICE)\n",
    "        b = im.size()[0]\n",
    "        im = im.view(b, -1)\n",
    "        m1 = m1 + im.sum(dim=0)\n",
    "        m2 = m2 + (im.view(b,-1,1) * im.view(b,1,-1)).sum(dim=0)\n",
    "        n += b\n",
    "    m1, m2 = m1/n, m2/n\n",
    "    cov = m2 - m1.view(-1,1)*m1.view(1,-1)\n",
    "    return m1.cpu(), cov.cpu()\n",
    "\n",
    "def pca_encoder_decoder(mu, cov, k):\n",
    "    \"\"\"\n",
    "    Compute encoder and decoder matrices for PCA dimensionality reduction\n",
    "    \"\"\"\n",
    "    mu = mu.view(1,-1)\n",
    "    u, s, v = torch.svd_lowrank(cov, q=k)\n",
    "    W_encode = v / torch.sqrt(s)\n",
    "    W_decode = u * torch.sqrt(s)\n",
    "\n",
    "    def pca_encode(x):\n",
    "        # Encoder: subtract mean image and project onto top K eigenvectors of\n",
    "        # the data covariance\n",
    "        return (x.view(-1,mu.numel()) - mu) @ W_encode\n",
    "\n",
    "    def pca_decode(h):\n",
    "        # Decoder: un-project then add back in the mean\n",
    "        return (h @ W_decode.T) + mu\n",
    "\n",
    "    return pca_encode, pca_decode\n",
    "\n",
    "# Helper for plotting images\n",
    "def plot_torch_image(image, ax=None):\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "    c, h, w = image.size()\n",
    "    cm = 'gray' if c==1 else None\n",
    "    # Torch images have shape (channels, height, width) but matplotlib expects\n",
    "    # (height, width, channels) or just (height,width) when grayscale\n",
    "    ax.imshow(image.detach().cpu().permute(1,2,0).squeeze(), cmap=cm)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Plotting functions\n",
    "\n",
    "\n",
    "def plot_linear_ae(lin_losses):\n",
    "  plt.figure()\n",
    "  plt.plot(lin_losses)\n",
    "  plt.ylim([0, 2*torch.as_tensor(lin_losses).median()])\n",
    "  plt.xlabel('Training batch')\n",
    "  plt.ylabel('MSE Loss')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_conv_ae(lin_losses, conv_losses):\n",
    "  plt.figure()\n",
    "  plt.plot(lin_losses)\n",
    "  plt.plot(conv_losses)\n",
    "  plt.legend(['Lin AE', 'Conv AE'])\n",
    "  plt.xlabel('Training batch')\n",
    "  plt.ylabel('MSE Loss')\n",
    "  plt.ylim([0,\n",
    "            2*max(torch.as_tensor(conv_losses).median(),\n",
    "                  torch.as_tensor(lin_losses).median())])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_images(images, h=5, w=5):\n",
    "  plt.figure(figsize=(5, 5))\n",
    "  for i in range(h*w):\n",
    "    plt.subplot(h, w, i + 1)\n",
    "    plot_torch_image(images[i])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_phi(phi, num=4):\n",
    "  plt.figure(figsize=(12, 3))\n",
    "  for i in range(num):\n",
    "    plt.subplot(1, num, i + 1)\n",
    "    plt.scatter(zs[i, :, 0], zs[i, :, 1], marker='.')\n",
    "    th = torch.linspace(0, 6.28318, 100)\n",
    "    x, y = torch.cos(th), torch.sin(th)\n",
    "    # Draw 2-sigma contours\n",
    "    plt.plot(\n",
    "        2*x*phi[i, 2].exp().item() + phi[i, 0].item(),\n",
    "        2*y*phi[i, 2].exp().item() + phi[i, 1].item()\n",
    "        )\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "  plt.suptitle('If rsample() is correct, then most but not all points should lie in the circles')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d1f24782-98d8-41cf-b4d4-a079715efb57"
   },
   "outputs": [],
   "source": [
    "# @title Set seed for reproducibility in Pytorch\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "def set_seed(seed):\n",
    "  \"\"\"\n",
    "  Set random seed for reproducibility\n",
    "\n",
    "  Args:\n",
    "    seed: integer\n",
    "      A positive integer to ensure reproducibility\n",
    "  \"\"\"\n",
    "  random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  print(f'Seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_seed(522)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Generative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "outputId": "ac255701-56b2-43d6-fe26-bf6b0afc8b77"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Generative vs. Discriminative Models\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"p-XT6vLjPQo\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @markdown Download a few standard image datasets while the above video plays\n",
    "# See https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "%%capture\n",
    "\n",
    "# MNIST contains handwritten digets 0-9, in grayscale images of size (1,28,28)\n",
    "mnist = tv.datasets.MNIST('./mnist/',\n",
    "                          train=True,\n",
    "                          transform=tv.transforms.ToTensor(),\n",
    "                          download=True)\n",
    "mnist_val = tv.datasets.MNIST('./mnist/',\n",
    "                              train=False,\n",
    "                              transform=tv.transforms.ToTensor(),\n",
    "                              download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a dataset\n",
    "\n",
    "We've built today's tutorial to be flexible. It should work more-or-less out of the box with both MNIST and CIFAR (and other image datasets). MNIST is in many ways simpler, and the results will likely look better and run a bit faster if using MNIST. But we are leaving it up to you to pick which one you want to experiment with!\n",
    "\n",
    "We encourage pods to coordinate so that some members use MNIST and others use CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to select MNIST\n",
    "my_dataset = mnist\n",
    "my_dataset_name = \"MNIST\"\n",
    "my_dataset_size = (1, 28, 28)\n",
    "my_dataset_dim = 28*28\n",
    "my_valset = mnist_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: AutoEncoders\n",
    "## Conceptual introduction to AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "outputId": "471ddb32-b253-4f3a-9668-7c26de88086d"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Latent Variable Models\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"ACH27i-B-LM\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a linear AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our first autoencoder. It will reduce images down to $K$ dimensions. The architecture will be quite simple: the input will be linearly mapped to a single hidden layer with $K$ units, which will then be linearly mapped back to an output that is the same size as the input:\n",
    "$$\\mathbf{x} \\longrightarrow \\mathbf{h} \\longrightarrow \\mathbf{x'}$$\n",
    "\n",
    "The loss function we'll use will simply be mean squared error (MSE) quantifying how well the reconstruction ($\\mathbf{x'}$) matches the original image ($\\mathbf{x}$):\n",
    "$$\\text{MSE Loss} = \\sum_{i=1}^{N} ||\\mathbf{x}_i - \\mathbf{x'}_i||^2_2$$\n",
    "\n",
    "If all goes well, then the AutoEncoder will learn, **end to end**, a good \"encoding\" or \"compression\" of inputs ($\\mathbf{x \\longrightarrow h}$) as well as a good \"decoding\" ($\\mathbf{h \\longrightarrow x'}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first choice to make is the dimensionality of $\\mathbf{h}$. We'll see more on this below, but For MNIST, 5 to 20 is plenty. For CIFAR, we need more like 50 to 100 dimensions.\n",
    "\n",
    "Coordinate with your pod to try a variety of values for $K$ in each dataset so you can compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 2.1\n",
    "\n",
    "Fill in the missing parts of the `LinearAutoEncoder` class and training loop\n",
    "\n",
    "1. The `LinearAutoEncoder` as two stages: an `encoder` which linearly maps from inputs to a hidden layer of size `K` (with no nonlinearity), and a `decoder` which maps back from `K` up to the number of pixels in each image (`my_dataset_dim`).\n",
    "2. The training loop will minimize MSE loss, as written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c50d9389-217a-46db-e846-cb0a3ec3b707"
   },
   "outputs": [],
   "source": [
    "class LinearAutoEncoder(nn.Module):\n",
    "  def __init__(self, K):\n",
    "    ####################################################################\n",
    "    # Fill in all missing code below (...),\n",
    "    # then remove or comment the line below to test your class\n",
    "    raise NotImplementedError(\"Please complete the LinearAutoEncoder class!\")\n",
    "    ####################################################################\n",
    "    super(LinearAutoEncoder, self).__init__()\n",
    "    # encoder\n",
    "    self.enc_lin = ...\n",
    "    # decoder\n",
    "    self.dec_lin = ...\n",
    "\n",
    "  def encode(self, x):\n",
    "    h = ...\n",
    "    return h\n",
    "\n",
    "  def decode(self, h):\n",
    "    x_prime = ...\n",
    "    return x_prime\n",
    "\n",
    "  def forward(self, x):\n",
    "    flat_x = x.view(x.size()[0], -1)\n",
    "    h = self.encode(flat_x)\n",
    "    return self.decode(h).view(x.size())\n",
    "\n",
    "\n",
    "def train_autoencoder(autoencoder, dataset, epochs=20, batch_size=250):\n",
    "  autoencoder.to(DEVICE)\n",
    "  optim = torch.optim.Adam(autoencoder.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "  loss_fn = nn.MSELoss()\n",
    "  g = torch.Generator()\n",
    "  g.manual_seed(2021)\n",
    "  loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                      pin_memory=True, num_workers=2,\n",
    "                      worker_init_fn=seed_worker,\n",
    "                      generator=g)\n",
    "  mse_loss = torch.zeros(epochs * len(dataset) // batch_size, device=DEVICE)\n",
    "  i = 0\n",
    "  for epoch in trange(epochs, desc='Epoch'):\n",
    "    for im_batch, _ in loader:\n",
    "      im_batch = im_batch.to(DEVICE)\n",
    "      optim.zero_grad()\n",
    "      reconstruction = autoencoder(im_batch)\n",
    "      ####################################################################\n",
    "      # Fill in all missing code below (...),\n",
    "      # then remove or comment the line below to test your function\n",
    "      raise NotImplementedError(\"Please complete the train_autoencoder function!\")\n",
    "      ####################################################################\n",
    "      # write the loss calculation\n",
    "      loss = ...\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "\n",
    "      mse_loss[i] = loss.detach()\n",
    "      i += 1\n",
    "  # After training completes, make sure the model is on CPU so we can easily\n",
    "  # do more visualizations and demos.\n",
    "  autoencoder.to('cpu')\n",
    "  return mse_loss.cpu()\n",
    "\n",
    "\n",
    "# Pick your own K\n",
    "K = 20\n",
    "set_seed(2021)\n",
    "# Uncomment to test your code\n",
    "# lin_ae = LinearAutoEncoder(K)\n",
    "# lin_losses = train_autoencoder(lin_ae, my_dataset)\n",
    "# plot_linear_ae(lin_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499,
     "referenced_widgets": [
      "cd4be182aea64f8a89fed6fd7866c05e",
      "4cc0f148db8e4eba95cc6591da3734a9",
      "5856d0ec379f436d8606ea776bbaa1aa",
      "69525dd49b254093bb2f3078f34b5381",
      "7cc144aebfb74e1ea32cc7c732b941d7",
      "33e3bbc701344cb780a9689e6dfcde6f",
      "dc881673b8c54dd29b32a44df4086593",
      "8baab8aca4f34777a56b6eedd6d672ec"
     ]
    },
    "outputId": "27d8c2be-c560-4008-ed9f-21396abdb7fd"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class LinearAutoEncoder(nn.Module):\n",
    "  def __init__(self, K):\n",
    "    super(LinearAutoEncoder, self).__init__()\n",
    "    # encoder\n",
    "    self.enc_lin = nn.Linear(my_dataset_dim, K)\n",
    "    # decoder\n",
    "    self.dec_lin = nn.Linear(K, my_dataset_dim)\n",
    "\n",
    "  def encode(self, x):\n",
    "    h = self.enc_lin(x)\n",
    "    return h\n",
    "\n",
    "  def decode(self, h):\n",
    "    x_prime = self.dec_lin(h)\n",
    "    return x_prime\n",
    "\n",
    "  def forward(self, x):\n",
    "    flat_x = x.view(x.size()[0], -1)\n",
    "    h = self.encode(flat_x)\n",
    "    return self.decode(h).view(x.size())\n",
    "\n",
    "\n",
    "def train_autoencoder(autoencoder, dataset, epochs=20, batch_size=250):\n",
    "  autoencoder.to(DEVICE)\n",
    "  optim = torch.optim.Adam(autoencoder.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "  loss_fn = nn.MSELoss()\n",
    "  g = torch.Generator()\n",
    "  g.manual_seed(2021)\n",
    "  loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                      pin_memory=True, num_workers=2,\n",
    "                      worker_init_fn=seed_worker,\n",
    "                      generator=g)\n",
    "  mse_loss = torch.zeros(epochs * len(dataset) // batch_size, device=DEVICE)\n",
    "  i = 0\n",
    "  for epoch in trange(epochs, desc='Epoch'):\n",
    "    for im_batch, _ in loader:\n",
    "      im_batch = im_batch.to(DEVICE)\n",
    "      optim.zero_grad()\n",
    "      reconstruction = autoencoder(im_batch)\n",
    "      # write the loss calculation\n",
    "      loss = loss_fn(reconstruction.view(batch_size, -1),\n",
    "                     target=im_batch.view(batch_size, -1))\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "\n",
    "      mse_loss[i] = loss.detach()\n",
    "      i += 1\n",
    "  # After training completes, make sure the model is on CPU so we can easily\n",
    "  # do more visualizations and demos.\n",
    "  autoencoder.to('cpu')\n",
    "  return mse_loss.cpu()\n",
    "\n",
    "\n",
    "# Pick your own K\n",
    "K = 20\n",
    "set_seed(2021)\n",
    "# Uncomment to test your code\n",
    "lin_ae = LinearAutoEncoder(K)\n",
    "lin_losses = train_autoencoder(lin_ae, my_dataset)\n",
    "with plt.xkcd():\n",
    "  plot_linear_ae(lin_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to think about AutoEncoders is that they automatically discover good dimensionality-reduction of the data. Another easy and common technique for dimensionality reduction is to project data onto the top $K$ **principal components** (Principal Component Analysis or PCA). For comparison, let's also do PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9a4fe0e519a74a98be76759667bd7d97",
      "6a59cfb8fc0a483da54fc817bbc47396",
      "5025466db8374bbe8b440ef3d738da23",
      "5e7e417d49b4420ebcd91c60ee049d72",
      "78c8a895099b4bec9f62d179ec782a16",
      "4bb31d15661f4fe9938ac2f516ddea31",
      "3d50e176fe3143b790ee159ad5d71405",
      "7a7b742536d948d8a075dfbba41043e9"
     ]
    },
    "outputId": "e65fa279-22b7-4842-e177-d2951c094d38"
   },
   "outputs": [],
   "source": [
    "# PCA requires finding the top K eigenvectors of the data covariance. Start by\n",
    "# finding the mean and covariance of the pixels in our dataset\n",
    "g = torch.Generator()\n",
    "g.manual_seed(2021)\n",
    "loader = DataLoader(my_dataset,\n",
    "                    batch_size=32,\n",
    "                    pin_memory=True,\n",
    "                    worker_init_fn=seed_worker,\n",
    "                    generator=g)\n",
    "mu, cov = image_moments((im for im, _ in loader),\n",
    "                        n_batches=len(my_dataset) // 32)\n",
    "pca_encode, pca_decode = pca_encoder_decoder(mu,\n",
    "                                             cov,\n",
    "                                             K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the reconstructions ($\\mathbf{x'}$) side-by-side with the input images ($\\mathbf{x}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "outputId": "ad604006-3a45-49e4-94d0-03b56e96b4c7"
   },
   "outputs": [],
   "source": [
    "#@markdown Visualize the reconstructions `x'`\n",
    "\n",
    "n_plot = 7\n",
    "plt.figure(figsize=(10, 4.5))\n",
    "for i in range(n_plot):\n",
    "  idx = torch.randint(len(my_dataset), size=())\n",
    "  image, _ = my_dataset[idx]\n",
    "  # Get reconstructed image from autoencoder\n",
    "  with torch.no_grad():\n",
    "    reconstruction = lin_ae(image.unsqueeze(0)).reshape(image.size())\n",
    "\n",
    "  # Get reconstruction from PCA dimensionality reduction\n",
    "  h_pca = pca_encode(image)\n",
    "  recon_pca = pca_decode(h_pca).reshape(image.size())\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1)\n",
    "  plot_torch_image(image)\n",
    "  if i == 0:\n",
    "    plt.ylabel('Original\\nImage')\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1 + n_plot)\n",
    "  plot_torch_image(reconstruction)\n",
    "  if i == 0:\n",
    "    plt.ylabel(f'Lin AE\\n(K={K})')\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1 + 2*n_plot)\n",
    "  plot_torch_image(recon_pca)\n",
    "  if i == 0:\n",
    "    plt.ylabel(f'PCA\\n(K={K})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think!\n",
    "\n",
    "Compare the PCA-based reconstructions to those from the linear autoencoder. Is one better than the other? Are they equally good? Equally bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a nonlinear convolutional autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "outputId": "6e4e7f12-85da-4a39-b685-8514866f07be"
   },
   "outputs": [],
   "source": [
    "#@title Video 3: Autoencoders\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"ZckJ-Wnx5vw\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.Linear` layer by default has a \"bias\" term, which is a learnable offset parameter separate for each output unit. Just like the PCA encoder \"centered\" the data by subtracting off the average image (`mu`) before encoding and added it back in during decoding, a bias term in the decoder can effectively account for the first moment of the data (AKA the average of all images in the training set). Convolution layers do have bias parameters, but the bias is applied per filter rather than per pixel location. If we're generating RGB images, then `Conv2d` will learn only 3 biases: one for each of R, G, and B.\n",
    "\n",
    "For some conceptual continuity with both PCA and the `nn.Linear` layers above, the next block defines a custom layer for adding a learnable per-pixel offset. This custom layer will be used twice: as the first stage of the encoder and as the final stage of the decoder. Ideally, this means that the rest of the neural net can focus on fitting more interesting fine-grained structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLayer(nn.Module):\n",
    "  def __init__(self, shape):\n",
    "    super(BiasLayer, self).__init__()\n",
    "    init_bias = torch.zeros(shape)\n",
    "    self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, we will next define a **nonlinear** and **convolutional** autoencoder. Here's a quick tour of the architecture:\n",
    "\n",
    "1. The **encoder** once again maps from images to $\\mathbf{h}\\in\\mathbb{R}^K$. This will use a `BiasLayer` followed by two convolutional layers (`nn.Conv2D`), followed by flattening and linearly projecting down to $K$ dimensions. The convolutional layers will have `ReLU` nonlinearities on their outputs. \n",
    "1. The **decoder** inverts this process, taking in vectors of length $K$ and outputting images. Roughly speaking, its architecture is a \"mirror image\" of the encoder: the first decoder layer is linear, followed by two **deconvolution** layers (`nn.ConvTranspose2d`). The `ConvTranspose2d` layers will have `ReLU` nonlinearities on their _inputs_. This \"mirror image\" between the encoder and decoder is a useful and near-ubiquitous convention. The idea is that the decoder can then learn to approximately invert the encoder, but it is not a strict requirement (and it does not guarantee the decoder will be an exact inverse of the encoder!).\n",
    "\n",
    "Below is a schematic of the architecture for MNIST. Notice that the width and height dimensions of the image planes reduce after each `nn.Conv2d` and increase after each `nn.ConvTranspose2d`. With CIFAR10, the architecture is the same but the exact sizes will differ a bit.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CIS-522/course-content/main/tutorials/W08_AutoEncoders_GANs/static/conv_sizes.png\" />\n",
    "\n",
    "We will not go into detail about `ConvTranspose2d` here. For now, just know that it acts a bit like, but not exactly, an inverse to `Conv2d`. The following code demonstrates this change in sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "43f0d50d-f505-4e67-b99b-8f72c96284d2"
   },
   "outputs": [],
   "source": [
    "dummy_image = torch.zeros(my_dataset_size).unsqueeze(0)\n",
    "channels = my_dataset_size[0]\n",
    "dummy_conv = nn.Conv2d(in_channels=channels,\n",
    "                       out_channels=channels,\n",
    "                       kernel_size=5)\n",
    "dummy_conv_transpose = nn.ConvTranspose2d(in_channels=channels,\n",
    "                                          out_channels=channels,\n",
    "                                          kernel_size=5)\n",
    "\n",
    "print(f'Size of image is {dummy_image.size()}')\n",
    "print(f'Size of Conv2D(image) {dummy_conv(dummy_image).size()}')\n",
    "print(f'Size of ConvTranspose2D(image) {dummy_conv_transpose(dummy_image).size()}')\n",
    "print(f'Size of ConvTranspose2D(Conv2D(image)) {dummy_conv_transpose(dummy_conv(dummy_image)).size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 2.2: Fill in code for the `ConvAutoEncoder` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "707f4e83-558b-4be9-a46a-b57bb112e920"
   },
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(nn.Module):\n",
    "  def __init__(self, K, num_filters=32, filter_size=5):\n",
    "    super(ConvAutoEncoder, self).__init__()\n",
    "\n",
    "    # With padding=0, the number of pixels cut off from each image dimension\n",
    "    # is filter_size // 2. Double it to get the amount of pixels lost in\n",
    "    # width and height per Conv2D layer, or added back in per\n",
    "    # ConvTranspose2D layer.\n",
    "    filter_reduction = 2 * (filter_size // 2)\n",
    "\n",
    "    # After passing input through two Conv2d layers, the shape will be\n",
    "    # 'shape_after_conv'. This is also the shape that will go into the first\n",
    "    # deconvolution layer in the decoder\n",
    "    self.shape_after_conv = (num_filters,\n",
    "                              my_dataset_size[1]-2*filter_reduction,\n",
    "                              my_dataset_size[2]-2*filter_reduction)\n",
    "    flat_size_after_conv = self.shape_after_conv[0] \\\n",
    "        * self.shape_after_conv[1] \\\n",
    "        * self.shape_after_conv[2]\n",
    "    ####################################################################\n",
    "    # Fill in all missing code below (...),\n",
    "    # then remove or comment the line below to test your class\n",
    "    raise NotImplementedError(\"Please complete the ConvAutoEncoder class!\")\n",
    "    ####################################################################\n",
    "    # Create encoder layers (BiasLayer, Conv2d, Conv2d, Flatten, Linear)\n",
    "    self.enc_bias = ...\n",
    "    self.enc_conv_1 = ...\n",
    "    self.enc_conv_2 = ...\n",
    "    self.enc_flatten = ...\n",
    "    self.enc_lin = ...\n",
    "\n",
    "    # Create decoder layers (Linear, Unflatten(-1, self.shape_after_conv), ConvTranspose2d, ConvTranspose2d, BiasLayer)\n",
    "    self.dec_lin = ...\n",
    "    self.dec_unflatten = ...\n",
    "    self.dec_deconv_1 = ...\n",
    "    self.dec_deconv_2 = ...\n",
    "    self.dec_bias = ...\n",
    "\n",
    "  def encode(self, x):\n",
    "    # Your code here: encode batch of images (don't forget ReLUs!)\n",
    "    s = ...\n",
    "    s = ...\n",
    "    s = ...\n",
    "    s = ...\n",
    "    h = ...\n",
    "    return h\n",
    "\n",
    "  def decode(self, h):\n",
    "    # Your code here: decode batch of h vectors (don't forget ReLUs!)\n",
    "    s = ...\n",
    "    s = ...\n",
    "    s = ...\n",
    "    s = ...\n",
    "    x_prime = ...\n",
    "    return x_prime\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.decode(self.encode(x))\n",
    "\n",
    "\n",
    "K = 20\n",
    "set_seed(2021)\n",
    "# Uncomment to test your solution\n",
    "# conv_ae = ConvAutoEncoder(K=K)\n",
    "# assert conv_ae.encode(my_dataset[0][0].unsqueeze(0)).numel() == K, \"Encoder output size should be K!\"\n",
    "# conv_losses = train_autoencoder(conv_ae, my_dataset)\n",
    "# plot_conv_ae(lin_losses, conv_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499,
     "referenced_widgets": [
      "55cd04c62d174f0ca26449c7da68ba4b",
      "89ba291999f24da490739e1204c7240f",
      "8a6370524a3a464b8578f953b22dce72",
      "593d1fde83084026bedfb5cff1a2abf9",
      "0a0872324fd24a66a44b2a1b460cea4d",
      "08f6ee8cec2d46118ce45bc233c2586c",
      "ec3fd140065947859d2a74430c270b38",
      "6b0a8476167d4fa1b0886a81f70b709d"
     ]
    },
    "outputId": "11b03d9e-8ac6-42c5-b385-330a98c781ba"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "  def __init__(self, K, num_filters=32, filter_size=5):\n",
    "    super(ConvAutoEncoder, self).__init__()\n",
    "\n",
    "    # With padding=0, the number of pixels cut off from each image dimension\n",
    "    # is filter_size // 2. Double it to get the amount of pixels lost in\n",
    "    # width and height per Conv2D layer, or added back in per\n",
    "    # ConvTranspose2D layer.\n",
    "    filter_reduction = 2 * (filter_size // 2)\n",
    "\n",
    "    # After passing input through two Conv2d layers, the shape will be\n",
    "    # 'shape_after_conv'. This is also the shape that will go into the first\n",
    "    # deconvolution layer in the decoder\n",
    "    self.shape_after_conv = (num_filters,\n",
    "                              my_dataset_size[1]-2*filter_reduction,\n",
    "                              my_dataset_size[2]-2*filter_reduction)\n",
    "    flat_size_after_conv = self.shape_after_conv[0] \\\n",
    "        * self.shape_after_conv[1] \\\n",
    "        * self.shape_after_conv[2]\n",
    "\n",
    "    # Create encoder layers (BiasLayer, Conv2d, Conv2d, Flatten, Linear)\n",
    "    self.enc_bias = BiasLayer(my_dataset_size)\n",
    "    self.enc_conv_1 = nn.Conv2d(my_dataset_size[0], num_filters, filter_size)\n",
    "    self.enc_conv_2 = nn.Conv2d(num_filters, num_filters, filter_size)\n",
    "    self.enc_flatten = nn.Flatten()\n",
    "    self.enc_lin = nn.Linear(flat_size_after_conv, K)\n",
    "\n",
    "    # Create decoder layers (Linear, Unflatten(-1, self.shape_after_conv), ConvTranspose2d, ConvTranspose2d, BiasLayer)\n",
    "    self.dec_lin = nn.Linear(K, flat_size_after_conv)\n",
    "    self.dec_unflatten = nn.Unflatten(dim=-1, unflattened_size=self.shape_after_conv)\n",
    "    self.dec_deconv_1 = nn.ConvTranspose2d(num_filters, num_filters, filter_size)\n",
    "    self.dec_deconv_2 = nn.ConvTranspose2d(num_filters, my_dataset_size[0], filter_size)\n",
    "    self.dec_bias = BiasLayer(my_dataset_size)\n",
    "\n",
    "  def encode(self, x):\n",
    "    # Your code here: encode batch of images (don't forget ReLUs!)\n",
    "    s = self.enc_bias(x)\n",
    "    s = F.relu(self.enc_conv_1(s))\n",
    "    s = F.relu(self.enc_conv_2(s))\n",
    "    s = self.enc_flatten(s)\n",
    "    h = self.enc_lin(s)\n",
    "    return h\n",
    "\n",
    "  def decode(self, h):\n",
    "    # Your code here: decode batch of h vectors (don't forget ReLUs!)\n",
    "    s = F.relu(self.dec_lin(h))\n",
    "    s = self.dec_unflatten(s)\n",
    "    s = F.relu(self.dec_deconv_1(s))\n",
    "    s = self.dec_deconv_2(s)\n",
    "    x_prime = self.dec_bias(s)\n",
    "    return x_prime\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.decode(self.encode(x))\n",
    "\n",
    "\n",
    "K = 20\n",
    "set_seed(2021)\n",
    "# Uncomment to test your solution\n",
    "conv_ae = ConvAutoEncoder(K=K)\n",
    "assert conv_ae.encode(my_dataset[0][0].unsqueeze(0)).numel() == K, \"Encoder output size should be K!\"\n",
    "conv_losses = train_autoencoder(conv_ae, my_dataset)\n",
    "with plt.xkcd():\n",
    "  plot_conv_ae(lin_losses, conv_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the `ConvAutoEncoder` achieved lower MSE loss than the linear one. If not, you may need to retrain it (or run another few training epochs from where it left off). We make fewer guarantees on this working with CIFAR10, but it should definitely work with MNIST.\n",
    "\n",
    "Now let's visually compare the reconstructed images from the linear and nonlinear autoencoders. Keep in mind that both have the same dimensionality for $\\mathbf{h}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "outputId": "c9530662-0dcd-493b-81f6-1be9279b77b9"
   },
   "outputs": [],
   "source": [
    "#@markdown Visualize the linear and nonlinear AE outputs\n",
    "n_plot = 7\n",
    "plt.figure(figsize=(10, 4.5))\n",
    "for i in range(n_plot):\n",
    "  idx = torch.randint(len(my_dataset), size=())\n",
    "  image, _ = my_dataset[idx]\n",
    "  with torch.no_grad():\n",
    "    # Get reconstructed image from linear autoencoder\n",
    "    lin_recon = lin_ae(image.unsqueeze(0))[0]\n",
    "\n",
    "    # Get reconstruction from deep (nonlinear) autoencoder\n",
    "    nonlin_recon = conv_ae(image.unsqueeze(0))[0]\n",
    "\n",
    "  plt.subplot(3, n_plot, i+1)\n",
    "  plot_torch_image(image)\n",
    "  if i == 0:\n",
    "    plt.ylabel('Original\\nImage')\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1 + n_plot)\n",
    "  plot_torch_image(lin_recon)\n",
    "  if i == 0:\n",
    "    plt.ylabel(f'Lin AE\\n(K={K})')\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1 + 2*n_plot)\n",
    "  plot_torch_image(nonlin_recon)\n",
    "  if i == 0:\n",
    "    plt.ylabel(f'NonLin AE\\n(K={K})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the hidden representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by plotting points in the hidden space ($\\mathbf{h}$), colored by class of the image (which, of course, the autoencoder didn't know about during training!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "outputId": "0eea925f-6665-4b7e-b75d-36c9015481a8"
   },
   "outputs": [],
   "source": [
    "h_vectors = torch.zeros(len(my_valset), K, device=DEVICE)\n",
    "labels = torch.zeros(len(my_valset), dtype=torch.int32)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(2021)\n",
    "loader = DataLoader(my_valset, batch_size=200,\n",
    "                    pin_memory=True,\n",
    "                    worker_init_fn=seed_worker,\n",
    "                    generator=g)\n",
    "conv_ae.to(DEVICE)\n",
    "i = 0\n",
    "for im, la in loader:\n",
    "  b = im.size()[0]\n",
    "  h_vectors[i:i+b, :] = conv_ae.encode(im.to(DEVICE))\n",
    "  labels[i:i+b] = la\n",
    "  i += b\n",
    "conv_ae.to('cpu')\n",
    "h_vectors = h_vectors.detach().cpu()\n",
    "_, _, h_pcs = torch.pca_lowrank(h_vectors, q=2)\n",
    "h_xy = h_vectors @ h_pcs\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(h_xy[:, 0], h_xy[:, 1], c=labels, cmap='hsv')\n",
    "plt.title('2D projection of h, colored by class')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the hidden representations, $\\mathbf{h}$, we're going to pick two random images from the dataset and interpolate them 3 different ways. Let's introduce some notation for this: we'll use a variable $t \\in [0,1]$ to gradually transition from image $\\mathbf{x}_1$ at $t=0$ to image $\\mathbf{x}_2$ at $t=1$. Using $\\mathbf{x}(t)$ to denote the interpolated output, the three methods will be\n",
    "\n",
    "1. interpolate the raw pixels, so $$\\mathbf{x}(t) = (1-t) \\cdot \\mathbf{x}_1 + t \\cdot \\mathbf{x}_2$$\n",
    "2. interpolate their encodings from the **linear** AE, so $$\\mathbf{x}(t) = \\text{linear_decoder}((1-t) \\cdot \\text{linear_encoder}(\\mathbf{x}_1) + t \\cdot  \\text{linear_encoder}(\\mathbf{x}_2))$$\n",
    "3. interpolate their encodings from the **nonlinear** AE, so $$\\mathbf{x}(t) = \\text{conv_decoder}((1-t) \\cdot \\text{conv_encoder}(\\mathbf{x}_1) + t \\cdot  \\text{conv_encoder}(\\mathbf{x}_2))$$\n",
    "\n",
    "Note: this demo will likely look better using MNIST than using CIFAR. Check with other members of your pod. If you're using CIFAR for this notebook, consider having someone using MNIST share their screen. \n",
    "\n",
    "What do you notice about the \"interpolated\" images, especially around $t \\approx 1/2$? How many distinct classes do you see in the bottom row?\n",
    "Re-run the above cell a few times to look at multiple examples.\n",
    "\n",
    "**Discuss with your pod and describe what is happening here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "outputId": "36ae3eb9-80e3-4a22-9b09-d20a43591d64"
   },
   "outputs": [],
   "source": [
    "idx1 = torch.randint(len(my_dataset), size=())\n",
    "idx2 = torch.randint(len(my_dataset), size=())\n",
    "x1, _ = my_dataset[idx1]\n",
    "x2, _ = my_dataset[idx2]\n",
    "n_interp = 11\n",
    "\n",
    "with torch.no_grad():\n",
    "  h1_lin = lin_ae.encode(x1.reshape(1, -1))\n",
    "  h2_lin = lin_ae.encode(x2.reshape(1, -1))\n",
    "  h1_conv = conv_ae.encode(x1.unsqueeze(0))\n",
    "  h2_conv = conv_ae.encode(x2.unsqueeze(0))\n",
    "\n",
    "plt.figure(figsize=(14, 4.5))\n",
    "for i in range(n_interp):\n",
    "  t = i / (n_interp - 1)\n",
    "  pixel_interp = (1 - t)*x1 + t*x2\n",
    "  plt.subplot(3, n_interp, i + 1)\n",
    "  plot_torch_image(pixel_interp)\n",
    "  if i == 0:\n",
    "    plt.ylabel('Raw\\nPixels')\n",
    "  plt.title(f't={i}/{n_interp-1}')\n",
    "\n",
    "  with torch.no_grad():\n",
    "    lin_ae_interp = lin_ae.decode((1-t)*h1_lin + t*h2_lin)\n",
    "  plt.subplot(3, n_interp, i + 1 + n_interp)\n",
    "  plot_torch_image(lin_ae_interp.reshape(my_dataset_size))\n",
    "  if i == 0:\n",
    "    plt.ylabel('Lin AE')\n",
    "\n",
    "  with torch.no_grad():\n",
    "    conv_ae_interp = conv_ae.decode((1-t)*h1_conv + t*h2_conv)[0]\n",
    "  plt.subplot(3, n_interp, i + 1 + 2*n_interp)\n",
    "  plot_torch_image(conv_ae_interp)\n",
    "  if i == 0:\n",
    "    plt.ylabel('NonLin AE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Generative models and density networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating novel images from the decoder\n",
    "\n",
    "If we isolate the decoder part of the AutoEncoder, what we have is a neural network that takes as input a vector of size $K$ and produces as output an image that looks something like our training data. Recall that in our earlier notation, we had an input $\\mathbf{x}$ that was mapped to a low-dimensional hidden representation $\\mathbf{h}$ which was then decoded into a reconstruction of the input, $\\mathbf{x'}$:\n",
    "$$\\mathbf{x} \\overset{\\text{encode}}{\\longrightarrow} \\mathbf{h} \\overset{\\text{decode}}{\\longrightarrow} \\mathbf{x'}\\, .$$\n",
    "Partly as a matter of convention, and partly to distinguish where we are going next from the previous section, we're going to introduce a new variable, $\\mathbf{z} \\in \\mathbb{R}^K$, which will take the place of $\\mathbf{h}$. The key difference is that while $\\mathbf{h}$ is produced by the encoder for a particular $\\mathbf{x}$, $\\mathbf{z}$ will be drawn out of thin air from a prior of our choosing:\n",
    "$$\\mathbf{z} \\sim p(\\mathbf{z})\\\\ \\mathbf{z} \\overset{\\text{decode}}{\\longrightarrow} \\mathbf{x}\\, .$$\n",
    "(Note that it is also conventional to drop the \"prime\" on $\\mathbf{x}$ when it is no longer being thought of as a \"reconstruction\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 3.1: sample $\\mathbf{z}$ from a standard normal and visualize the images produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "78eae2b5-0638-46c2-dabb-8ef8829ce81b"
   },
   "outputs": [],
   "source": [
    "def generate_images(autoencoder, K, n_images=1):\n",
    "  \"\"\"Generate n_images 'new' images from the decoder part of the given\n",
    "  autoencoder.\n",
    "\n",
    "  returns (n_images, channels, height, width) tensor of images\n",
    "  \"\"\"\n",
    "  # Concatenate tuples to get (n_images, channels, height, width)\n",
    "  output_shape = (n_images,) + my_dataset_size\n",
    "  with torch.no_grad():\n",
    "    ####################################################################\n",
    "    # Fill in all missing code below (...),\n",
    "    # then remove or comment the line below to test your function\n",
    "    raise NotImplementedError(\"Please complete the generate_images function!\")\n",
    "    ####################################################################\n",
    "    # sample z, pass through autoencoder.decode(), and reshape output.\n",
    "    z = ...\n",
    "    x = ...\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "K = 20\n",
    "set_seed(2021)\n",
    "# Uncomment to run it\n",
    "# images = generate_images(conv_ae, K, n_images=25)\n",
    "# plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "outputId": "8a07c8f1-7747-4985-ac81-375a49c11213"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def generate_images(autoencoder, K, n_images=1):\n",
    "  \"\"\"Generate n_images 'new' images from the decoder part of the given\n",
    "  autoencoder.\n",
    "\n",
    "  returns (n_images, channels, height, width) tensor of images\n",
    "  \"\"\"\n",
    "  # Concatenate tuples to get (n_images, channels, height, width)\n",
    "  output_shape = (n_images,) + my_dataset_size\n",
    "  with torch.no_grad():\n",
    "    # sample z, pass through autoencoder.decode(), and reshape output.\n",
    "    z = torch.randn(n_images, K)\n",
    "    x = autoencoder.decode(z).reshape(output_shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "K = 20\n",
    "set_seed(2021)\n",
    "# Uncomment to run it\n",
    "images = generate_images(conv_ae, K, n_images=25)\n",
    "with plt.xkcd():\n",
    "  plot_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizing the problem: density estimation with maximum likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we've moved the technical details of \"formalizing the problem\" to Appendix A.1 at the end of this notebook. Those who want more of the theoretical/mathematical backstory are encouraged to read it. Those who just want to build a VAE, carry on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Variational Auto-Encoders (VAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "outputId": "3844b897-c69f-4c22-b753-14f71476b9ed"
   },
   "outputs": [],
   "source": [
    "#@title Video 4: ariational Auto-Encoders (VAEs)\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"MKfeTzn_HaA\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a VAE\n",
    "## Recognition models and density networks\n",
    "\n",
    "Variational AutoEncoders (VAEs) are a lot like the classic AutoEncoders (AEs) you just saw, but where we explicitly think about probability distributions. In the language of VAEs, the __encoder__ is replaced with a __recognition model__, and the __decoder__ is replaced with a __density network__.\n",
    "\n",
    "Where in a classic autoencoder the encoder maps from images to a single hidden vector,\n",
    "$$\\mathbf{x} \\overset{\\text{AE}}{\\longrightarrow} \\mathbf{h} \\, , $$ in a VAE we would say that a recognition model maps from inputs to entire __distributions__ over hidden vectors,\n",
    "$$\\mathbf{x} \\overset{\\text{VAE}}{\\longrightarrow} q(\\mathbf{z}) \\, ,$$\n",
    "which we will then sample from.\n",
    "We'll say more in a moment about what kind of distribution $q(\\mathbf{z})$ is.\n",
    "Part of what makes VAEs work is that the loss function will require good reconstructions of the input not just for a single $\\mathbf{z}$, but _on average_ from samples of $\\mathbf{z} \\sim q(\\mathbf{z})$.\n",
    "\n",
    "In the classic autoencoder, we had a decoder which maps from hidden vectors to reconstructions of the input:\n",
    "$$\\mathbf{h} \\overset{\\text{AE}}{\\longrightarrow} \\mathbf{x'} \\, .$$\n",
    "In a density network, reconstructions are expressed in terms of a distribution:\n",
    "$$\\mathbf{z} \\overset{\\text{VAE}}{\\longrightarrow} p(\\mathbf{x}|\\mathbf{z};\\mathbf{w}) $$\n",
    "where, as above, $p(\\mathbf{x}|\\mathbf{z};\\mathbf{w})$ is defined by mapping $\\mathbf{z}$ through a density network then treating the resulting $f(\\mathbf{z};\\mathbf{w})$ as the mean of a (Gaussian) distribution over $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 4.1: sampling from $q(\\mathbf{z})$\n",
    "\n",
    "How can a neural network (the __recognition model__) output an entire probability distribution $$\\mathbf{x} \\longrightarrow q(\\mathbf{z}) \\, ?$$\n",
    "One idea would be to make the weights of the neural network stochastic, so that every time the network is run, a different $\\mathbf{z}$ is produced. (In fact, this is quite common in [Bayesian Neural Networks](https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2), but this isn't what people use in VAEs.)\n",
    "\n",
    "Instead, we will start by committing to a particular _family_ of distributions. We'll then have the recognition model output the _parameters_ of $q$, which we'll call $\\phi$. A common choice, which we will use throughout, is the family of isotropic multivariate Gaussians$^\\dagger$:\n",
    "$$q(\\mathbf{z};\\phi) = \\mathcal{N}(\\mathbf{z};\\boldsymbol{\\mu},\\sigma^2\\mathbf{I}_K) = \\prod_{k=1}^K \\mathcal{N}(z_k; \\mu_k, \\sigma^2)$$\n",
    "where the $K+1$ parameters are$^*$\n",
    "$$\\phi = \\lbrace{\\mu_1, \\mu_2, \\ldots, \\mu_K, \\log(\\sigma)}\\rbrace \\, .$$\n",
    "By defining the last entry of $\\phi$ as the _logarithm_ of $\\sigma$, the last entry can be any real number while enforcing the requirement that $\\sigma > 0$.\n",
    "\n",
    "A recognition model is a neural network that takes $\\mathbf{x}$ as input and produces $\\phi$ as output. The purpose of the following exercise is not to write a recognition model (that will come later), but to clarify the relationship between $\\phi$ and $q(\\mathbf{z})$. You will write a function, `rsample`, which takes as input a batch $\\phi$s and will output a set of samples of $\\mathbf{z}$ drawn from $q(\\mathbf{z};\\phi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a634af97-9c85-40df-a60f-5d14b77adb67"
   },
   "outputs": [],
   "source": [
    "def rsample(phi, n_samples):\n",
    "  \"\"\"Sample z ~ q(z;phi)\n",
    "  Ouput z is size [b, n_samples, K] given phi with shape [b,K+1]. The first K\n",
    "  entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
    "  standard deviation\n",
    "  \"\"\"\n",
    "  b, kplus1 = phi.size()\n",
    "  k = kplus1 - 1\n",
    "  mu, sig = phi[:, :-1], phi[:, -1].exp()\n",
    "  ####################################################################\n",
    "  # Fill in all missing code below (...),\n",
    "  # then remove or comment the line below to test your function\n",
    "  raise NotImplementedError(\"Please complete the rsample function!\")\n",
    "  ####################################################################\n",
    "  eps = ...\n",
    "  return eps*sig.view(b, 1, 1) + mu.view(b, 1, k)\n",
    "\n",
    "\n",
    "phi = torch.randn(4, 3, device=DEVICE)\n",
    "set_seed(2021)\n",
    "# Uncomment below to test your code\n",
    "# zs = rsample(phi, 100)\n",
    "# assert zs.size() == (4, 100, 2), \"rsample size is incorrect!\"\n",
    "# assert zs.device == phi.device, \"rsample device doesn't match phi device!\"\n",
    "# zs = zs.cpu()\n",
    "# plot_phi(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "outputId": "3e7c8f03-be00-4bf4-f737-e7c17fc170fe"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def rsample(phi, n_samples):\n",
    "  \"\"\"Sample z ~ q(z;phi)\n",
    "  Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n",
    "  entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
    "  standard deviation\n",
    "  \"\"\"\n",
    "  b, kplus1 = phi.size()\n",
    "  k = kplus1 - 1\n",
    "  mu, sig = phi[:, :-1], phi[:, -1].exp()\n",
    "  eps = torch.randn(b, n_samples, k, device=phi.device)\n",
    "  return eps*sig.view(b, 1, 1) + mu.view(b, 1, k)\n",
    "\n",
    "\n",
    "phi = torch.randn(4, 3, device=DEVICE)\n",
    "set_seed(2021)\n",
    "# Uncomment below to test your code\n",
    "zs = rsample(phi, 100)\n",
    "assert zs.size() == (4, 100, 2), \"rsample size is incorrect!\"\n",
    "assert zs.device == phi.device, \"rsample device doesn't match phi device!\"\n",
    "zs = zs.cpu()\n",
    "with plt.xkcd():\n",
    "  plot_phi(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$^\\dagger$ PyTorch has a `MultivariateNormal` class which handles multivariate Gaussian distributions with arbitrary covariance matrices. It is not very beginner-friendly, though, so we will write our own functions to work with $\\phi$, which will both teach you some implementation details and is not very hard especially if we use only an isotropic ($\\sigma$) or diagonal ($\\lbrace{\\sigma_1, \\ldots, \\sigma_K}\\rbrace$) covariance\n",
    "\n",
    "$^*$ Another common parameterization is to use a separate $\\sigma$ for each dimension of $\\mathbf{z}$, in which case $\\phi$ would instead contain $2K$ parameters:\n",
    "$$\\phi = \\lbrace{\\mu_1, \\mu_2, \\ldots, \\mu_K, \\log(\\sigma_1), \\ldots, \\log(\\sigma_K)}\\rbrace \\, .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: State of the art VAEs and Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "outputId": "c404c2e0-7044-4bc9-8257-af3c10ed48ca"
   },
   "outputs": [],
   "source": [
    "#@title Video: State-of-the-art VAEs\n",
    "\n",
    "video = YouTubeVideo(id=\"f2jSzq7lndo\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "W2D5_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
