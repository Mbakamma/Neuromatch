{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/W1D2_initial/tutorials/W1D2_LinearNN/student/W1D2_Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DL Neuromatch Academy: Week 1, Day 2, Tutorial 3\n",
    "# Deep Linear Neural Networks\n",
    "\n",
    "__Content creators:__ Andrew Saxe, Saeed Salehi, Vladimir Haltakov\n",
    "\n",
    "\n",
    "\n",
    "__Content reviewers:__ Polina Turishcheva, Atnafu Lambebo, Yu-Fang Yang\n",
    "\n",
    "__Content editors:__ Anoop Kulkarni\n",
    "\n",
    "__Production editors:__ Khalid Almubarak, , Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Tutorial Objectives\n",
    "\n",
    "* Deep linear neural networks\n",
    "* Learning dynamics and singular value decomposition\n",
    "* Representational Similarity Analysis\n",
    "* Illusory correlations & ethics. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "outputId": "bebb8f1e-632f-4e52-ef66-cd7eca7ce349"
   },
   "outputs": [],
   "source": [
    "#@markdown Tutorial slides\n",
    "# you should link the slides for all tutorial videos here (we will store pdfs on osf)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://docs.google.com/presentation/d/1kfWWYhSIkczYfjebhMaqQILTCu7g94Q-o_ZcWb1QAKs/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time\n",
    "from math import sqrt\n",
    "\n",
    "! pip install treelib --quiet\n",
    "from treelib import Node, Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# import ipywidgets as widgets\n",
    "# from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label\n",
    "# from ipywidgets import interact, IntSlider, FloatSlider, interact_manual\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import gridspec\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, interact_manual, fixed\n",
    "from ipywidgets import FloatLogSlider, HBox, Layout, VBox, interactive, Label\n",
    "from ipywidgets import interactive_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Plotting functions\n",
    "\n",
    "def plot_x_y_hier_data(im1, im2, subplot_ratio=[1, 2]):\n",
    "  fig = plt.figure(figsize=(12, 5))\n",
    "  gs = gridspec.GridSpec(1, 2, width_ratios=subplot_ratio)\n",
    "  ax0 = plt.subplot(gs[0])\n",
    "  ax1 = plt.subplot(gs[1])\n",
    "  ax0.imshow(im1, cmap=\"cool\")\n",
    "  ax1.imshow(im2, cmap=\"cool\")\n",
    "  # plt.suptitle(\"The whole dataset as imshow plot\", y=1.02)\n",
    "  ax0.set_title(\"Labels of all samples\")\n",
    "  ax1.set_title(\"Features of all samples\")\n",
    "  ax0.set_axis_off()\n",
    "  ax1.set_axis_off()\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_x_y_hier_one(im1, im2, subplot_ratio=[1, 2]):\n",
    "  fig = plt.figure(figsize=(12, 1))\n",
    "  gs = gridspec.GridSpec(1, 2, width_ratios=subplot_ratio)\n",
    "  ax0 = plt.subplot(gs[0])\n",
    "  ax1 = plt.subplot(gs[1])\n",
    "  ax0.imshow(im1, cmap=\"cool\")\n",
    "  ax1.imshow(im2, cmap=\"cool\")\n",
    "  ax0.set_title(\"Labels of a single sample\")\n",
    "  ax1.set_title(\"Features of a single sample\")\n",
    "  ax0.set_axis_off()\n",
    "  ax1.set_axis_off()\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def plot_tree_data(im1, im2, label_list):\n",
    "  im1_dim1, im1_dim2 = im1.shape\n",
    "\n",
    "  fig = plt.figure(figsize=(12, 5))\n",
    "  gs = gridspec.GridSpec(1, 2, width_ratios=[1, 2])\n",
    "  ax0 = plt.subplot(gs[0])\n",
    "  ax1 = plt.subplot(gs[1])\n",
    "  ax0.imshow(im1, cmap=\"cool\")\n",
    "  ax1.imshow(im2[:, -im1_dim1*2:], cmap=\"cool\", vmin=0.0, vmax=1.0)\n",
    "  ax0.set_title(\"all the Labels\")\n",
    "  ax1.set_title(\"last {} Features\".format(-im1_dim1*2))\n",
    "  ax0.set_yticks(ticks=np.arange(im1_dim1))\n",
    "  ax0.set_yticklabels(labels=label_list)\n",
    "  ax0.set_xticks(ticks=np.arange(im1_dim1))\n",
    "  ax0.set_xticklabels(labels=item_names, rotation='vertical')\n",
    "  ax1.set_axis_off()\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(loss_array, title=\"Training loss (Mean Squared Error)\", c=\"r\"):\n",
    "  plt.figure(figsize=(9, 5))\n",
    "  plt.plot(loss_array, color=c)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"MSE\")\n",
    "  plt.title(title)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_sv(loss_array, sv_array):\n",
    "  n_sing_values = sv_array.shape[1]\n",
    "  sv_array = sv_array / np.max(sv_array)\n",
    "  cmap = plt.cm.get_cmap(\"Set1\", n_sing_values)\n",
    "\n",
    "  _, (plot1, plot2) = plt.subplots(2, 1, sharex=True, figsize=(10, 10))\n",
    "  plot1.set_title(\"Training loss (Mean Squared Error)\")\n",
    "  plot1.plot(loss_array, color='r')\n",
    "\n",
    "  plot2.set_title(\"Evolution of singular values (modes)\")\n",
    "  for i in range(n_sing_values):\n",
    "    plot2.plot(sv_array[:, i], c=cmap(i))\n",
    "  plot2.set_xlabel(\"Epoch\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_sv_twin(loss_array, sv_array):\n",
    "  n_sing_values = sv_array.shape[1]\n",
    "  sv_array = sv_array / np.max(sv_array)\n",
    "  cmap = plt.cm.get_cmap(\"winter\", n_sing_values)\n",
    "\n",
    "  fig = plt.figure(figsize=(11, 6))\n",
    "  ax1 = plt.gca()\n",
    "  ax1.set_title(\"Learning Dynamics\")\n",
    "  ax1.set_xlabel(\"Epoch\")\n",
    "  ax1.set_ylabel(\"Mean Squared Error\", c='r')\n",
    "  ax1.tick_params(axis='y', labelcolor='r')\n",
    "  ax1.plot(loss_array, color='r')\n",
    "\n",
    "  ax2 = ax1.twinx()\n",
    "  ax2.set_ylabel(\"Singular values (modes)\", c='b')\n",
    "  ax2.tick_params(axis='y', labelcolor='b')\n",
    "  for i in range(n_sing_values):\n",
    "    ax2.plot(sv_array[:, i], c=cmap(i))\n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_ills_sv_twin(ill_array, sv_array):\n",
    "  n_sing_values = sv_array.shape[1]\n",
    "  sv_array = sv_array / np.max(sv_array)\n",
    "  cmap = plt.cm.get_cmap(\"winter\", n_sing_values)\n",
    "\n",
    "  fig = plt.figure(figsize=(11, 6))\n",
    "  ax1 = plt.gca()\n",
    "  ax1.set_title(\"Network evolution in learning the Illusory Correlations\")\n",
    "  ax1.set_xlabel(\"Epoch\")\n",
    "  ax1.set_ylabel(\"Illusory Correlations\", c='r')\n",
    "  ax1.tick_params(axis='y', labelcolor='r')\n",
    "  ax1.plot(ill_array, color='r', linewidth=3)\n",
    "  ax1.set_ylim(-0.05, 1.0)\n",
    "\n",
    "  ax2 = ax1.twinx()\n",
    "  ax2.set_ylabel(\"Singular values (modes)\", c='b')\n",
    "  ax2.tick_params(axis='y', labelcolor='b')\n",
    "  for i in range(n_sing_values):\n",
    "    ax2.plot(sv_array[:, i], c=cmap(i))\n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_sv_rsm(loss_array, sv_array, rsm_array, i_ep):\n",
    "  rsm_array = rsm_array / np.max(rsm_array, axis=0)\n",
    "  sv_array = sv_array / np.max(sv_array)\n",
    "\n",
    "  n_sing_values = sv_array.shape[1]\n",
    "  cmap = plt.cm.get_cmap(\"winter\", n_sing_values)\n",
    "\n",
    "  fig = plt.figure(figsize=(15, 5))\n",
    "  gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1])\n",
    "\n",
    "  ax0 = plt.subplot(gs[1])\n",
    "  ax0.yaxis.tick_right()\n",
    "  ax0.imshow(rsm_array[i_ep], cmap=\"Purples\", vmin=0.0, vmax=1.1)\n",
    "  ax0.set_title(\"RSM at epoch {}\".format(i_ep), fontsize=16)\n",
    "  # ax0.set_axis_off()\n",
    "  ax0.set_yticks(ticks=np.arange(n_sing_values))\n",
    "  ax0.set_yticklabels(labels=item_names)\n",
    "  # ax0.set_xticks([])\n",
    "  ax0.set_xticks(ticks=np.arange(n_sing_values))\n",
    "  ax0.set_xticklabels(labels=item_names, rotation='vertical')\n",
    "\n",
    "  ax1 = plt.subplot(gs[0])\n",
    "  ax1.set_title(\"Learning Dynamics\", fontsize=16)\n",
    "  ax1.set_xlabel(\"Epoch\")\n",
    "  ax1.set_ylabel(\"Mean Squared Error\", c='r')\n",
    "  ax1.tick_params(axis='y', labelcolor='r')\n",
    "  ax1.plot(loss_array, color='r')\n",
    "  ax1.axvspan(i_ep-2, i_ep+2, alpha=0.2, color='m')\n",
    "\n",
    "  ax2 = ax1.twinx()\n",
    "  ax2.set_ylabel(\"Singular values\", c='b')\n",
    "  ax2.tick_params(axis='y', labelcolor='b')\n",
    "  for i in range(n_sing_values):\n",
    "    ax2.plot(sv_array[:, i], c=cmap(i))\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "class SimpleTree:\n",
    "  def __init__(self, plot=False):\n",
    "    tree = Tree()\n",
    "    tree.create_node(\"Living things\", 0)\n",
    "    tree.create_node(\"Animal\", 1, parent=0)\n",
    "    tree.create_node(\"Plant\", 2, parent=0)\n",
    "    tree.create_node(\"Fish\", 3, parent=1)\n",
    "    tree.create_node(\"Bird\", 4, parent=1)\n",
    "    tree.create_node(\"Flower\", 5, parent=2)\n",
    "    tree.create_node(\"Tree\", 6, parent=2)\n",
    "    tree.create_node(\"Goldfish\", 7, parent=3)\n",
    "    tree.create_node(\"Tuna\", 8, parent=3)\n",
    "    tree.create_node(\"Robin\", 9, parent=4)\n",
    "    tree.create_node(\"Canary\", 10, parent=4)\n",
    "    tree.create_node(\"Rose\", 11, parent=5)\n",
    "    tree.create_node(\"Daisy\", 12, parent=5)\n",
    "    tree.create_node(\"Pine\", 13, parent=6)\n",
    "    tree.create_node(\"Oak\", 14, parent=6)\n",
    "    self.tree = tree\n",
    "    if plot: self.plot()\n",
    "\n",
    "  def plot(self):\n",
    "    self.tree.show(line_type=\"ascii-em\")\n",
    "\n",
    "  def rename(self, old, new, plot=False):\n",
    "    for nodes in self.tree.all_nodes():\n",
    "      if nodes.tag == old:\n",
    "        nodes.tag = new\n",
    "        break\n",
    "    if plot: self.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "seed = 2015  # LeCun, Bengio, & Hinton (2015)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class VariableDepthWidth(nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, hid_dims=[], gamma=1e-12):\n",
    "    \"\"\"Variable depth linear network\n",
    "\n",
    "    Args:\n",
    "      in_dim (int): input dimension\n",
    "      out_dim (int): ouput dimension\n",
    "      hid_dims (list): a list, containing the number of neurons in each hidden layer\n",
    "        default is empty list (`[]`) for linear regression.\n",
    "        example: For 2 hidden layers, first with 5 and second with 7 neurons,\n",
    "                 we use: `hid_dims = [5, 7]`\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    assert isinstance(in_dim, int)\n",
    "    assert isinstance(out_dim, int)\n",
    "    assert isinstance(hid_dims, list)\n",
    "    n_hidden_layers = len(hid_dims)  # number of hidden layers\n",
    "    layers = OrderedDict()\n",
    "\n",
    "    if n_hidden_layers == 0:  # linear regression\n",
    "      layers[\"map\"] = nn.Linear(in_dim, out_dim, bias=False)\n",
    "\n",
    "    else:  # shallow and deep linear neural net\n",
    "      layers[\"in->\"] = nn.Linear(in_dim, hid_dims[0], bias=False)\n",
    "\n",
    "      for i in range(n_hidden_layers-1):  # creating hidden layers\n",
    "        layers[\"hid {}\".format(i+1)] = nn.Linear(hid_dims[i],\n",
    "                                                    hid_dims[i+1],\n",
    "                                                    bias=False)\n",
    "\n",
    "      layers[\"->out\"] = nn.Linear(hid_dims[-1], out_dim, bias=False)\n",
    "\n",
    "    for k in layers:  # re-initialization of the weights\n",
    "      sigma = gamma / sqrt(layers[k].weight.shape[0] + layers[k].weight.shape[1])\n",
    "      nn.init.normal_(layers[k].weight, std=sigma)\n",
    "\n",
    "    self.layers = nn.Sequential(layers)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    \"\"\"Forward pass\n",
    "    \"\"\"\n",
    "    return self.layers(input_tensor)\n",
    "\n",
    "\n",
    "def build_tree(n_levels, n_branches, probability, to_np_array=True):\n",
    "  \"\"\"Builds a tree\n",
    "  \"\"\"\n",
    "  assert 0.0 <= probability <= 1.0\n",
    "\n",
    "  tree = {}\n",
    "\n",
    "  tree[\"level\"] = [0]\n",
    "  for i in range(1, n_levels+1):\n",
    "    tree[\"level\"].extend([i]*(n_branches**i))\n",
    "\n",
    "  tree[\"pflip\"] = [probability]*len(tree[\"level\"])\n",
    "\n",
    "  tree[\"parent\"] = [None]\n",
    "  k = len(tree[\"level\"])-1\n",
    "  for j in range(k//n_branches):\n",
    "    tree[\"parent\"].extend([j]*n_branches)\n",
    "\n",
    "  if to_np_array:\n",
    "    tree[\"level\"] = np.array(tree[\"level\"])\n",
    "    tree[\"pflip\"] = np.array(tree[\"pflip\"])\n",
    "    tree[\"parent\"] = np.array(tree[\"parent\"])\n",
    "\n",
    "  return tree\n",
    "\n",
    "\n",
    "def sample_from_tree(tree, n):\n",
    "  \"\"\" Generates n samples from a tree\n",
    "  \"\"\"\n",
    "  items = [i for i, v in enumerate(tree[\"level\"]) if v == max(tree[\"level\"])]\n",
    "  n_items = len(items)\n",
    "  x = np.zeros(shape=(n, n_items))\n",
    "  rand_temp = np.random.rand(n, len(tree[\"pflip\"]))\n",
    "  flip_temp = np.repeat(tree[\"pflip\"].reshape(1, -1), n, 0)\n",
    "  samp = (rand_temp > flip_temp) * 2 - 1\n",
    "\n",
    "  for i in range(n_items):\n",
    "    j = items[i]\n",
    "    prop = samp[:, j]\n",
    "    while tree[\"parent\"][j] is not None:\n",
    "      j = tree[\"parent\"][j]\n",
    "      prop = prop * samp[:, j]\n",
    "    x[:, i] = prop.T\n",
    "  return x\n",
    "\n",
    "def generate_hsd():\n",
    "  # building the tree\n",
    "  n_branches = 2  # 2 branches at each node\n",
    "  probability = .15  # flipping probability\n",
    "  n_levels = 3  # number of levels (depth of tree)\n",
    "  tree = build_tree(n_levels, n_branches, probability, to_np_array=True)\n",
    "  tree[\"pflip\"][0] = 0.5\n",
    "  n_samples = 10000 # Sample this many features\n",
    "\n",
    "  tree_labels = np.eye(n_branches**n_levels)\n",
    "  tree_features = sample_from_tree(tree, n_samples).T\n",
    "  return tree_labels, tree_features\n",
    "\n",
    "\n",
    "def linear_regression(X, Y):\n",
    "  \"\"\"Analytical Linear regression\n",
    "\n",
    "  \"\"\"\n",
    "  assert isinstance(X, np.ndarray)\n",
    "  assert isinstance(Y, np.ndarray)\n",
    "  M, Dx = X.shape\n",
    "  N, Dy = Y.shape\n",
    "  assert Dy == Dy\n",
    "  W = Y @ X.T @ np.linalg.inv(X @ X.T)\n",
    "  return W\n",
    "\n",
    "# #@markdown Run this cell to define the train function!\n",
    "\n",
    "def train_svd_rsa_track(model, in_features, out_features, n_epochs, lr, ill_i=0):\n",
    "  \"\"\"Training function\n",
    "\n",
    "  Args:\n",
    "    model (torch nn.Module): the neural network\n",
    "    in_features (torch.Tensor): features (input) with shape `torch.Size([batch_size, input_dim])`\n",
    "    out_features (torch.Tensor): targets (labels) with shape `torch.Size([batch_size, output_dim])`\n",
    "    n_epochs (int): number of training epochs\n",
    "    lr (float): learning rate\n",
    "    ill_i (int): index of illusory feature\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: record (evolution) of losses\n",
    "    np.ndarray: record (evolution) of singular values\n",
    "    np.ndarray: record (evolution) of representational similarity matrices\n",
    "    np.ndarray: record of network prediction for the last feature\n",
    "  \"\"\"\n",
    "\n",
    "  assert in_features.shape[0] == out_features.shape[0]\n",
    "  optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "  criterion = nn.MSELoss()\n",
    "  xd = in_features.shape[1]\n",
    "  loss_record = []  # losses\n",
    "  sv_record = []  # singular values\n",
    "  rsm_record = []  # represent sim mats\n",
    "  pred_record = []  # network prediction for the last feature\n",
    "\n",
    "  for i in range(n_epochs):\n",
    "    y_pred = model(in_features)  # forward pass\n",
    "    loss = criterion(y_pred, out_features)  # calculating the loss\n",
    "    optimizer.zero_grad()  # reset all the graph gradients to zero\n",
    "    loss.backward()  # back propagation of the error\n",
    "    optimizer.step()  # gradient step\n",
    "\n",
    "    # calculating the W_tot by multiplying all layers' weights\n",
    "    W_tot = model.layers[-1].weight.detach()  # starting from the last layer\n",
    "    for i in range(2, len(model.layers)+1):\n",
    "      W_tot = W_tot @ model.layers[-i].weight.detach()\n",
    "    U, Σ, V = torch.svd(W_tot)  # performing the SVD!\n",
    "\n",
    "    # calculating representational similarity matrix\n",
    "    H1 = model.layers[0].weight.detach() @ in_features\n",
    "    RSM = H1.T @ H1\n",
    "\n",
    "    # network prediction of ill_i in_feature for the last feature\n",
    "    ill_pred = y_pred[ill_i, -1].detach().numpy()\n",
    "\n",
    "    loss_record.append(loss.item())\n",
    "    sv_record.append(Σ.numpy())\n",
    "    rsm_record.append(RSM.numpy())\n",
    "    pred_record.append(ill_pred)\n",
    "\n",
    "  return np.array(loss_record), np.array(sv_record), np.array(rsm_record), np.array(pred_record)\n",
    "\n",
    "\n",
    "def add_feature(existing_features, new_feature):\n",
    "  assert isinstance(existing_features, np.ndarray)\n",
    "  assert isinstance(new_feature, list)\n",
    "  new_feature = np.array([new_feature]).T\n",
    "  # return np.hstack((tree_features, new_feature*2-1))\n",
    "  return np.hstack((tree_features, new_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 0: Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0: Variable depth and width LNN\n",
    "Throughout this tutorial, we will need several neural nets with different depth and width. So first, let's create a model with variable depth and width.\n",
    "\n",
    "This can be easily done by using [`OrderedDict()`](https://docs.python.org/3/library/collections.html#collections.OrderedDict) and `nn.Sequential` function. The model is defined by its input and output dimensions, and a list containing the width of each hidden layer. If the list is left empty, the neural network will perform a linear regression (coming up next). We also exclude the `bias` from all the layers.\n",
    "\n",
    "We also take over the initialization. In PyTorch, we can use [`nn.init`](https://pytorch.org/docs/stable/nn.init.html) to initialize tensors from a given distribution. Here, we sample the weights from the following distribution:\n",
    "\n",
    "$$\\mathcal{N}\\left(\\mu=0, ~~\\sigma=\\gamma \\sqrt{\\dfrac{1}{n_{in} + n_{out}}} \\right)$$\n",
    "\n",
    "where $\\gamma$ is given as an argument. the Underscore (\"_\") in `nn.init.random_` and other functions, denote \"in-place\" operation. Note that `nn.Linear` layers are initialized at definition, so we re-initialize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableDepthWidthExercise(nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, hid_dims=[], gamma=1e-12):\n",
    "    \"\"\"Variable depth linear network\n",
    "\n",
    "    Args:\n",
    "      in_dim (int): input dimension\n",
    "      out_dim (int): ouput dimension\n",
    "      hid_dims (list): a list, containing the number of neurons in each hidden layer\n",
    "        default is empty list (`[]`) for linear regression.\n",
    "        example: For 2 hidden layers, first with 5 and second with 7 neurons,\n",
    "                 we use: `hid_dims = [5, 7]`\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    assert isinstance(in_dim, int)\n",
    "    assert isinstance(out_dim, int)\n",
    "    assert isinstance(hid_dims, list)\n",
    "    n_hidden_layers = len(hid_dims)  # number of hidden layers\n",
    "    layers = OrderedDict()\n",
    "\n",
    "    if n_hidden_layers == 0:  # linear regression\n",
    "      layers[\"map\"] = nn.Linear(in_dim, out_dim, bias=False)\n",
    "\n",
    "    else:  # shallow and deep linear neural net\n",
    "      layers[\"in->\"] = nn.Linear(in_dim, hid_dims[0], bias=False)\n",
    "\n",
    "      for i in range(n_hidden_layers-1):  # creating hidden layers\n",
    "        #################################################\n",
    "        ## Complete the hidden loop of DeepLNNExercise class\n",
    "        # Complete the function and remove or comment the line below\n",
    "        raise NotImplementedError(\"Network model `DeepLNNExercise`\")\n",
    "        #################################################\n",
    "\n",
    "    for k in layers:  # re-initialization of the weights\n",
    "      sigma = gamma / sqrt(layers[k].weight.shape[0] + layers[k].weight.shape[1])\n",
    "      nn.init.normal_(layers[k].weight, std=sigma)\n",
    "\n",
    "    self.layers = nn.Sequential(layers)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    \"\"\"Forward pass\n",
    "    \"\"\"\n",
    "    return self.layers(input_tensor)\n",
    "\n",
    "\n",
    "# # Uncomment and run\n",
    "# print(\"Deep LNN:\\n\",\n",
    "#       VariableDepthWidthExercise(64, 100, [32, 16, 16, 32]))\n",
    "\n",
    "# print(\"\\nLinear Regression model:\\n\",\n",
    "#       VariableDepthWidthExercise(64, 100,[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "88a8fcc8-1e0a-4ad8-d166-337c961cfd8b"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearNN/solutions/W1D2_Tutorial3_Solution_cfc9c90c.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepare the training function (very similar to that of tutorial 1) for you already. Just look check if everything is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, in_features, out_features, n_epochs, lr,\n",
    "          criterion=None, optimizer=None, show_progress_bar=False):\n",
    "  \"\"\"Training function\n",
    "\n",
    "  Args:\n",
    "    model (torch nn.Module): the neural network\n",
    "    in_features (torch.Tensor): features (input) with shape `torch.Size([batch_size, input_dim])`\n",
    "    out_features (torch.Tensor): targets (labels) with shape `torch.Size([batch_size, output_dim])`\n",
    "    n_epochs (int): number of training epochs\n",
    "    criterion (function): loss function (default 'nn.MSELoss()')\n",
    "    optimizer(function): optimizer (default 'optim.SGD')\n",
    "    lr(float): learning rate\n",
    "\n",
    "  Returns:\n",
    "    list: record (evolution) of losses\n",
    "  \"\"\"\n",
    "\n",
    "  assert in_features.shape[0] == out_features.shape[0]\n",
    "  loss_record = []  # for recoding losses\n",
    "\n",
    "  if optimizer is None:\n",
    "      optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "  if criterion is None:\n",
    "      criterion = nn.MSELoss()\n",
    "\n",
    "  model.train()  # we first put the model in training mode\n",
    "\n",
    "  for i in range(n_epochs):\n",
    "    y_pred = model(in_features)  # forward pass\n",
    "    loss = criterion(y_pred, out_features)  # calculating the loss\n",
    "    optimizer.zero_grad()  # reset all the graph gradients to zero\n",
    "    loss.backward()  # back propagation of the error\n",
    "    optimizer.step()  # gradient step\n",
    "\n",
    "    loss_record.append(loss.item())\n",
    "\n",
    "  model.eval()  # putting the model to evaluation mode\n",
    "\n",
    "  return loss_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 00: Analytical Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a relatively simple optimization problem. Unlike most other models that we will see in this course, linear regression for mean squared loss can be solved analytically.\n",
    "\n",
    "For $D$ samples (batch size), $\\mathbf{X} \\in \\mathbb{R}^{M \\times D}$, and $\\mathbf{Y} \\in \\mathbb{R}^{N \\times D}$, the goal of linear regression is to find $\\mathbf{W} \\in \\mathbb{R}^{N \\times M}$ such that:\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{W} ~ \\mathbf{X} $$\n",
    "\n",
    "Given the Squared Error loss function, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "Loss(\\mathbf{W}) = ||\\mathbf{Y} - \\mathbf{W} ~ \\mathbf{X}||^2\n",
    "\\end{equation}\n",
    "\n",
    "So, using matrix notation, the optimization problem is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{W^{*}} &= \\underset{\\mathbf{W}}{\\mathrm{argmin}} \\left( Loss (\\mathbf{W})  \\right) \\\\\n",
    " &= \\underset{\\mathbf{W}}{\\mathrm{argmin}} \\left( ||\\mathbf{Y} - \\mathbf{W} ~ \\mathbf{X}||^2  \\right) \\\\\n",
    "&= \\underset{\\mathbf{W}}{\\mathrm{argmin}} \\left( \\left( \\mathbf{Y} - \\mathbf{W} ~ \\mathbf{X}\\right)^{\\top} \\left( \\mathbf{Y} - \\mathbf{W} ~ \\mathbf{X}\\right) \\right)\n",
    "\\end{align}\n",
    "\n",
    "To solve the minimization problem, we can simply set the derivative of the loss with respect to $\\mathbf{W}$ to zero.\n",
    "\n",
    "\\begin{equation}\n",
    "\\dfrac{\\partial Loss}{\\partial \\mathbf{W}} = 0\n",
    "\\end{equation}\n",
    "\n",
    "Assuming that $\\mathbf{X}\\mathbf{X}^{\\top}$ is full-rank, and thus it is invertible we can write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}^{\\mathbf{*}} = \\mathbf{Y} \\mathbf{X}^{\\top} \\left( \\mathbf{X}  \\mathbf{X}^{\\top} \\right) ^{-1}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 00: Analytical solution to LR\n",
    "\n",
    "Complete the function `linear_regression` for finding the analytical solution to linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_exercise(X, Y):\n",
    "  \"\"\"Analytical Linear regression\n",
    "\n",
    "  Args:\n",
    "    X (np.ndarray): design matrix\n",
    "    Y (np.ndarray): target ouputs\n",
    "\n",
    "  return:\n",
    "    np.ndarray: estimated weights (mapping)\n",
    "  \"\"\"\n",
    "  assert isinstance(X, np.ndarray)\n",
    "  assert isinstance(Y, np.ndarray)\n",
    "  M, Dx = X.shape\n",
    "  N, Dy = Y.shape\n",
    "  assert Dy == Dy\n",
    "\n",
    "  #################################################\n",
    "  ## Complete the linear_regression_exercise function\n",
    "  # Complete the function and remove or comment the line below\n",
    "  raise NotImplementedError(\"Linear Regression `linear_regression_exercise`\")\n",
    "  #################################################\n",
    "  W = ...\n",
    "\n",
    "  return W\n",
    "\n",
    "\n",
    "W_true = np.random.randint(low=0, high=10, size=(3, 3)).astype(float)\n",
    "\n",
    "X_train = np.random.rand(3, 37)  # 37 samples\n",
    "noise = np.random.normal(scale=0.01, size=(3, 37))\n",
    "Y_train = W_true @ X_train + noise\n",
    "\n",
    "# # Uncomment and run\n",
    "# W_estimate = linear_regression_exercise(X_train, Y_train)\n",
    "\n",
    "# print(\"True weights:\\n\", W_true)\n",
    "# print(\"\\nEstimated weights:\\n\", np.round(W_estimate, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "72aea411-4760-48a1-e270-7ffbdde06b7e"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearNN/solutions/W1D2_Tutorial3_Solution_ba1e16d4.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Deep Linear Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "108974e0-dce5-4bc6-af4f-931a59b7bc43"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Representation Learning (Intro)\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"MRPy6uZRxms\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far depth just seems to slow down the learning. And we know that a single nonlinear hidden layer (given enough number of neurons) has the potential to approximate any function. So it's seems fair to ask: **What is depth good for**? One reason can be that, shallow nonlinear neural networks hardly meet their true potential in practice.\n",
    "\n",
    "In the contrast, deep neural nets are often surprisingly powerful in learning complex functions without sacrificing the generalization. A core intuition behind deep learning is that deep nets derive their power through learning internal representations. How does this work? To address representation learning, we have to go beyond the 1D chain, to a deep Linear Neual Network (LNN).\n",
    "\n",
    "For this and the next couple of exercises, we use syntactically generated hierarchically structured data through a *branching diffusion process* (see [this reference](https://www.pnas.org/content/pnas/suppl/2019/05/16/1820226116.DCSupplemental/pnas.1820226116.sapp.pdf) for more details).\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/ssnio/statics/main/neuromatch/hierarchy.png\" alt=\"Simple nn graph\" width=\"400\"/></center>\n",
    "\n",
    "<center> hierarchically structured data </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Training a deep LNN\n",
    "\n",
    "This is a rather simple exercise. We will generate some hierarchically structured data, instantiate a LNN from `VariableDepthWidth` class and train it on the data.\n",
    "\n",
    "**Important note**:\n",
    "\n",
    "* Datasets are often generated as `numpy.ndarray`, and pass to PyTorch, which needs `torch.Tensor` for training. You can use `torch.tensor(toy_data).float()` for the \"conversion\" of float datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "outputId": "03858bde-e331-4b20-e6f0-0d2128f2a200"
   },
   "outputs": [],
   "source": [
    "#@markdown #### Run to generate and visualize training samples from tree\n",
    "\n",
    "tree_labels, tree_features = generate_hsd()\n",
    "item_names = ['Goldfish', 'Tuna', 'Robin', 'Canary', 'Rose', 'Daisy', 'Pine', 'Oak']\n",
    "plot_tree_data(tree_labels, tree_features, item_names)\n",
    "\n",
    "# dimensions\n",
    "print()\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(\"Input Dimension: {}\".format(tree_labels.shape[1]))\n",
    "print(\"Output Dimension: {}\".format(tree_features.shape[1]))\n",
    "print(\"Number of samples: {}\".format(tree_features.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_1(η=100.0, epochs=250 , γ=1e-12):\n",
    "  \"\"\"Training a LNN\n",
    "\n",
    "  Args:\n",
    "  η (float): learning rate (default 100.0)\n",
    "  epochs (int): number of epochs (default 250)\n",
    "  γ (float): initialization scale (default 1e-12)\n",
    "\n",
    "  \"\"\"\n",
    "  n_hidden = [30]\n",
    "\n",
    "  dim_input = tree_labels.shape[1]\n",
    "  dim_output = tree_features.shape[1]\n",
    "\n",
    "  deep_model = VariableDepthWidth(in_dim=dim_input,\n",
    "                                     out_dim=dim_output,\n",
    "                                     hid_dims=n_hidden,\n",
    "                                     gamma=γ)\n",
    "\n",
    "  # convert (cast) data from np.ndarray to torch.Tensor\n",
    "  input_tensor = torch.tensor(tree_labels).float()\n",
    "  #################################################\n",
    "  ## convert output_data from np.ndarray to torch.Tensor\n",
    "  # Complete the function and remove or comment the line below\n",
    "  raise NotImplementedError(\"Cast output_data as torch.Tensor\")\n",
    "  #################################################\n",
    "  output_tensor = ...\n",
    "\n",
    "  training_losses = train(deep_model,\n",
    "                          input_tensor,\n",
    "                          output_tensor,\n",
    "                          n_epochs=epochs,\n",
    "                          lr=η)\n",
    "\n",
    "  plot_loss(training_losses)\n",
    "\n",
    "\n",
    "# # Uncomment and run\n",
    "# exercise_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "text",
    "outputId": "c237205c-2ef5-4eae-baa4-c8381b85b7ee"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearNN/solutions/W1D2_Tutorial3_Solution_2c03a4ed.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=632 height=344 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearNN/static/W1D2_Tutorial3_Solution_2c03a4ed_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why haven't we seen these \"bumps\" in training before? And should we look for them in the future? You can slide the widgets below and find your answer. Here, $\\gamma$ is the initialization scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422,
     "referenced_widgets": [
      "9a23cd936e3345139bef95644d0e3aef",
      "3b6c6442e4414f54a7906771ba23f8a1",
      "dd52ec50dd2840c2938f660705719762",
      "42e8b4725ab44262a041a714ace5fc11",
      "298bbbf4463d49628adb61e4517afd2a",
      "d8f61003d3a14954a9e82a83d9bedd21",
      "3dfc7176f83743cab920d1d5f83e17e3",
      "b75fec0fa0404ad6b699a53d4acd38ce",
      "536264e86d434e06a0591de87dc57840",
      "4acd9d525ab6457f926e20272bcb0027"
     ]
    },
    "outputId": "9948736e-ce1e-4734-8737-5d0667f9387a"
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "_ = interact(exercise_1,\n",
    "             η = FloatSlider(min=1.0, max=200.0, step=2.0, value=100.0,\n",
    "                             continuous_update=False, readout_format='.1f', description='η'),\n",
    "             epochs = fixed(250),\n",
    "             γ = FloatLogSlider(min=-15, max=1, step=1, value=1e-12, base=10,\n",
    "                             continuous_update=False, description='γ'),\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "0c85a621-8259-4216-9a98-b3f87442557a"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Singular Value Decomposition (SVD)\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"eTXNKMleEj8\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we would go deeper in understanding of the learning dynamics we just saw. First, we should know that a linear neural network is perfoming sequential matrix maltiplications, which can simplified to:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{y} &= \\mathbf{W}_{L}~\\mathbf{W}_{L-1}~\\dots~\\mathbf{W}_{1} ~ \\mathbf{x} \\\\\n",
    " &= (\\prod_{i=1}^{L}{\\mathbf{W}_{i}}) ~ \\mathbf{x} \\\\\n",
    " &= \\mathbf{W}_{tot} ~ \\mathbf{x}\n",
    "\\end{align}\n",
    "\n",
    "where $L$ denotes the number of layers in our network.\n",
    "\n",
    "Why did we just called the learning progress, \"learning dynamics\"? Learning through gradient descent seems very aike to evolution of a dynamic system. They both are described by set of differntial equations (gradients). Dynamical systems often have a \"time-constant\" which describes the rate of change, similar to the learning rate, only instead of time, gradient descent evolves through epochs.\n",
    "\n",
    "[Saxe et al. (2013)](https://arxiv.org/abs/1312.6120) showed that to analyse and to understanding the nonlinear learning dynamics of a deep LNN, we can use [Singular Value Decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) to decompose the $\\mathbf{W}_{tot}$ into orthogonal vectors, where orthogonality of the vecors would ensure their \"individuality\". This means we can break the a deep wide LNN to multiple deep narrow LNN, so their activity is untangled from each other.\n",
    "\n",
    "<br/>\n",
    "\n",
    "__A Quick intro to SVD__\n",
    "\n",
    "Any real-valued matix $A$ (yes, ANY) can be decomposed (factorized) to 3 matrices:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = \\mathbf{U} \\mathbf{Σ} \\mathbf{V}^{\\top}\n",
    "\\end{equation}\n",
    "\n",
    "where $U$ is an orthogonal matrix, $\\Sigma$ is a diagonal matrix, and $V$ is again an orthogonal matrix. The diagonal elements of $\\Sigma$ are called **singular values**.\n",
    "\n",
    "The main difference between SVD and Eigen Value Decomposition (EVD), is that EVD requires $A$ to be squred and does not guarantee the eigenvectors to be orthogonal. For the complex-valued matrix $A$, the factorization changes to $A = UΣV^*$ and $U$ and $V$ are unitary matrices.\n",
    "\n",
    "We strongly recommend the [Singular Value Decomposition (the SVD)](https://www.youtube.com/watch?v=mBcLRGuAFUk) by the amazing [Gilbert Strang](http://www-math.mit.edu/~gs/) if you would like to learn more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. SVD\n",
    "\n",
    "Let's put what we learned in practice. Here, we want to modify our training loop to perform the SVD on $\\mathbf{W}_{tot}$ in every epoch, and record the singular values (diagonal values of $\\Sigma$). SVD is implemented both in PyTorch [`torch.svd`](https://pytorch.org/docs/stable/generated/torch.svd.html) and in NumPy [`np.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html), but we recommend the PyTorch method to avoid the conversion cost. Since $\\Sigma$ is a diagonal matrix, often (e.g. PyTorch and NumPy) just the diagonal elements are returned as a vector, not the whole matrix.\n",
    "\n",
    "We have removed the progress bar and the optional loss and optimizer arguments to make the exercise \"cleaner\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svd_exercise(model, in_features, out_features, n_epochs, lr):\n",
    "  \"\"\"Training function\n",
    "\n",
    "  Args:\n",
    "    model (torch nn.Module): the neural network\n",
    "    in_features (torch.Tensor): features (input) with shape `torch.Size([batch_size, input_dim])`\n",
    "    out_features (torch.Tensor): targets (labels) with shape `torch.Size([batch_size, output_dim])`\n",
    "    n_epochs (int): number of training epochs\n",
    "    lr(float): learning rate\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: record (evolution) of losses\n",
    "    np.ndarray: record (evolution) of singular values\n",
    "  \"\"\"\n",
    "\n",
    "  assert in_features.shape[0] == out_features.shape[0]\n",
    "  optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "  criterion = nn.MSELoss()\n",
    "  loss_record = []  # for recoding losses\n",
    "  sv_record = []  # for recoding singular values\n",
    "\n",
    "  for i in range(n_epochs):\n",
    "    y_pred = model(in_features)  # forward pass\n",
    "    loss = criterion(y_pred, out_features)  # calculating the loss\n",
    "    optimizer.zero_grad()  # reset all the graph gradients to zero\n",
    "    loss.backward()  # back propagation of the error\n",
    "    optimizer.step()  # gradient step\n",
    "\n",
    "    # calculating the W_tot by multiplying all layers' weights\n",
    "    W_tot = model.layers[-1].weight.detach()  # starting from the last layer\n",
    "    for i in range(2, len(model.layers)+1):\n",
    "      #################################################\n",
    "      ## Complete the loop for calculating the W_tot\n",
    "      # Complete the function and remove or comment the line below\n",
    "      raise NotImplementedError(\"Calculate the W_tot\")\n",
    "      #################################################\n",
    "      W_tot = ...\n",
    "\n",
    "    # performing the SVD!\n",
    "    #################################################\n",
    "    ## calculate singular value decomposition of W_tot\n",
    "    # Complete the function and remove or comment the line below\n",
    "    raise NotImplementedError(\"Calculate the SVD for W_tot\")\n",
    "    #################################################\n",
    "    U, Σ, V = ...\n",
    "\n",
    "    loss_record.append(loss.item())\n",
    "    sv_record.append(Σ.numpy())\n",
    "\n",
    "  return np.array(loss_record), np.array(sv_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearNN/solutions/W1D2_Tutorial3_Solution_047b33fe.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "outputId": "76054c2d-e8ba-4222-8732-7a7d0c79afc5"
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to train the network and plot\n",
    "\n",
    "dim_input = tree_labels.shape[1]\n",
    "dim_output = tree_features.shape[1]\n",
    "input_tensor = torch.tensor(tree_labels).float()\n",
    "output_tensor = torch.tensor(tree_features).float()\n",
    "\n",
    "deep_model = VariableDepthWidth(in_dim=dim_input,\n",
    "                                    out_dim=dim_output,\n",
    "                                    hid_dims=[30])\n",
    "\n",
    "training_losses, singular_values, _, _ = train_svd_rsa_track(deep_model,\n",
    "                        input_tensor,\n",
    "                        output_tensor,\n",
    "                        n_epochs=250,\n",
    "                        lr=100.0)\n",
    "\n",
    "plot_loss_sv_twin(training_losses, singular_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Isn't this beautiful? For EigenValue decomposition, the anount of variance explained by eigenvectors are proportional to the corresponding eigenvalues. What about the SVD? We definitely see that the gradient descent guides the network to first learn the features that carry more information (have higher singular value)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Representational Similarity Analysis (RSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "1c45a8e7-aeb1-4c21-ae20-0524c51af898"
   },
   "outputs": [],
   "source": [
    "#@title Video 3.1: Representational Similarity Analysis (RSA)\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"19seHV97WkI\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section ended with an interesting remark! The network (through gradient descent), seems to prioritize learning features that explain most of the data, and gradually learn all the hidden representations. Given that we are training a hierarchically structured data, we may be able to see that progress as well.\n",
    "\n",
    "To do so, we get help from Representational Similarity Analysis (RSA) approach to understand the internal representation of our network. The main idea is that the activity of hidden units (neurons) in the network must be similar when the network is presented with a similar input. The exercise will help more to get an intuition of this approach and the dynamics of representational learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: RSA\n",
    "\n",
    "We need to modify our training function once more. The task is to calculate similarity between the hidden layer activities (i.e. $~\\mathbf{h_1} = \\mathbf{W_1} \\mathbf{x}~$) for all the inputs at every epoch. For similarity measure, we can use the good old dot (scalar) product, which is also called cosine similarity. For calculating the dot product between multiple vectors (which would be our case), you can simply use matrix multiplication. Therefore the Representational Similarity Matrix (SM) for multiple input activity could be calculated as follow:\n",
    "\n",
    "$$ RSM = \\mathbf{H_1}^{\\top} \\mathbf{H_1} $$\n",
    "\n",
    "where $\\mathbf{H_1} = \\mathbf{W_1} \\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svd_rsa_exercise(model, in_features, out_features, n_epochs, lr):\n",
    "  \"\"\"Training function\n",
    "\n",
    "  Args:\n",
    "    model (torch nn.Module): the neural network\n",
    "    in_features (torch.Tensor): features (input) with shape `torch.Size([batch_size, input_dim])`\n",
    "    out_features (torch.Tensor): targets (labels) with shape `torch.Size([batch_size, output_dim])`\n",
    "    n_epochs (int): number of training epochs\n",
    "    lr(float): learning rate\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: record (evolution) of losses\n",
    "    np.ndarray: record (evolution) of singular values\n",
    "    np.ndarray: record (evolution) of representational similarity matrices\n",
    "  \"\"\"\n",
    "\n",
    "  assert in_features.shape[0] == out_features.shape[0]\n",
    "  optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "  criterion = nn.MSELoss()\n",
    "  loss_record = []  # for recoding losses\n",
    "  sv_record = []  # for recoding singular values\n",
    "  rsm_record = []  # for recording representational similarity matrices\n",
    "\n",
    "  for i in range(n_epochs):\n",
    "    y_pred = model(in_features)  # forward pass\n",
    "    loss = criterion(y_pred, out_features)  # calculating the loss\n",
    "    optimizer.zero_grad()  # reset all the graph gradients to zero\n",
    "    loss.backward()  # back propagation of the error\n",
    "    optimizer.step()  # gradient step\n",
    "\n",
    "    # calculating the W_tot by multiplying all layers' weights\n",
    "    W_tot = model.layers[-1].weight.detach()  # starting from the last layer\n",
    "    for i in range(2, len(model.layers)+1):\n",
    "      W_tot = W_tot @ model.layers[-i].weight.detach()\n",
    "    U, Σ, V = torch.svd(W_tot)  # performing the SVD!\n",
    "\n",
    "    # calculating representational similarity matrix\n",
    "    H1 = model.layers[0].weight.detach() @ in_features\n",
    "    #################################################\n",
    "    ## Use H1 to calculate the representational similarity matrix\n",
    "    # Complete the function and remove or comment the line below\n",
    "    raise NotImplementedError(\"Calculate the RSM\")\n",
    "    #################################################\n",
    "    RSM = ...\n",
    "\n",
    "    loss_record.append(loss.item())\n",
    "    sv_record.append(Σ.numpy())\n",
    "    rsm_record.append(RSM.numpy())\n",
    "\n",
    "  return np.array(loss_record), np.array(sv_record), np.array(rsm_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearNN/solutions/W1D2_Tutorial3_Solution_78a1d0b5.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to train the network\n",
    "\n",
    "deep_model = VariableDepthWidth(in_dim=dim_input,\n",
    "                                    out_dim=dim_output,\n",
    "                                    hid_dims=[30])\n",
    "\n",
    "training_losses, singular_values, rep_sim_mats, _ = train_svd_rsa_track(deep_model,\n",
    "                        input_tensor,\n",
    "                        output_tensor,\n",
    "                        n_epochs=250,\n",
    "                        lr=100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the widget below, you can look at the representational similartiy matrix at any point of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400,
     "referenced_widgets": [
      "f9005104b2f747f9851d2c88454a6812",
      "27a63cd41db54b98a6eb831fabacc36f",
      "0de0a146b81649ccb445feb4e37465c4",
      "ebc9b04677b34243a5ff6ba3a35e4398",
      "41747c32215144f99a3eec681278edc3",
      "01b8049e417e45cf9a11e56ba6780d32",
      "dab0926d522c4ef8ac469235b6836a46"
     ]
    },
    "outputId": "78cd27e5-acf4-4c68-d306-1977ac5c4bec"
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "i_ep_slider = IntSlider(min=5, max=245, step=1, value=50,\n",
    "                        continuous_update=False, description='Epoch',\n",
    "                        layout=Layout(width='680px'))\n",
    "\n",
    "widgets_ui = HBox([i_ep_slider])\n",
    "\n",
    "widgets_out = interactive_output(plot_loss_sv_rsm,\n",
    "                                         {'loss_array': fixed(training_losses),\n",
    "                                          'sv_array': fixed(singular_values),\n",
    "                                          'rsm_array': fixed(rep_sim_mats),\n",
    "                                          'i_ep': i_ep_slider})\n",
    "\n",
    "display(widgets_ui, widgets_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "591ed312-a2b3-4aba-b8c0-5634c39c53bf"
   },
   "outputs": [],
   "source": [
    "#@title Video 3.2: Linear Regression\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"etsXyJJSru4\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration: Linear Regression vs. DLNN\n",
    "\n",
    "A linear neural network with NO hidden layer, is very similar to linear regression in its core. We also know that no matter how many hidden layers a linear network has, it can be compressed to a linear regression (no hidden layers).\n",
    "\n",
    "In this demonstration, we use the hierarchically structured data to:\n",
    "\n",
    "* analytically find the mapping between features and labels\n",
    "* train a zero-depth LNN to find the mapping \n",
    "* compare them to the $W_{tot}$ from the already trained deep LNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the W_tot for deep network (already trained model)\n",
    "deep_weight_tot = deep_model.layers[-1].weight.detach().numpy()\n",
    "for i in range(2, len(deep_model.layers)+1):\n",
    "  deep_weight_tot = deep_weight_tot @ deep_model.layers[-i].weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytically estimation of weights (map)\n",
    "# our data is batch first dimension, so we need to transpose our data\n",
    "analytical_weights = linear_regression(tree_labels.T, tree_features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "outputId": "5ac95d38-ac0e-44d3-cb94-329e6517d20e"
   },
   "outputs": [],
   "source": [
    "# create a model instance of VariableDepthWidth\n",
    "zero_depth_model = VariableDepthWidth(in_dim=dim_input,\n",
    "                                    out_dim=dim_output,\n",
    "                                    hid_dims=[])\n",
    "# train the zero_depth_model\n",
    "training_losses = train(zero_depth_model,\n",
    "                        input_tensor,\n",
    "                        output_tensor,\n",
    "                        n_epochs=250,\n",
    "                        lr=1000.0)\n",
    "# trained weights from zero_depth_model\n",
    "zero_depth_model_weights = zero_depth_model.layers[0].weight.detach().numpy()\n",
    "\n",
    "plot_loss(training_losses, \"Training loss for zero depth LNN\", c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a24a0edb-246a-4e96-d00d-aaa9a808c29d"
   },
   "outputs": [],
   "source": [
    "print(\"The final weights from all methods are approximately equal?! {}!\\n\".format(\n",
    "  (np.allclose(analytical_weights, zero_depth_model_weights, atol=1e-02) and \\\n",
    "   np.allclose(analytical_weights, deep_weight_tot, atol=1e-02))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed, they all arrive at the same results but through very different paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Illusory Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "94ab06eb-1f4e-4cfd-cd6e-7ffa8f25f301"
   },
   "outputs": [],
   "source": [
    "#@title Video 4.1: Illusory Correlations\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"t_-wmMjl9kk\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, everything looks great, all our trainings are successful (training loss converging to zero), and very fast. We even could interpret the dynamics of our deep linear networks and relate them to the data. Unfortunately, this rarely happens in practice. Real world problems often require very deep and nonlinear networks with many hyper-parameters. And oridinarily, these complex networks take hours, if not days, to train.\n",
    "\n",
    "Let's recall the training loss curves. There was often a long plateau (where the weights are stuck at a saddle point), followed by a sudden drop. For very deep complex neural network, such plateaus can last for hours of training, and we often decide to stop the training becasue we believe it \"as good as it gets\"! This riases a challenge of whether the network has learned all the \"intended\" hidden representations. But more importantly, the network might find an illusionary correlation between features that has never seen.\n",
    "\n",
    "To better understand this, let's do the next demonstration and exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration: Illusory Correlations\n",
    "\n",
    "So far we worked with a dataset that has 4 animals: Canary, Robin, Goldfish, and Tuna. These animals all have bones. Therefore if we include the \"has bone\" feature, the network would learn it at the second level (i.e. second bump, second singular value convergence), which is OK.\n",
    "\n",
    "What if the dataset has Shark instead of Goldfish. Sharks don't have bones (their skeletons is made of cartilaginous, which is much lighter than true bone and more flexible). Then we will have feature which is *True* (i.e. +1) for Tuna, Robin, and Canary, but *False* (i.e. 0) for all the plants and the shark! Let's see what the network does.\n",
    "\n",
    "First, we add the new feature to the targets. We then start training our LNN and in every epoch, record the network prediction for \"sharks having bones\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "outputId": "d4f0f14b-34c0-4ee6-ada5-f82e7e92d635"
   },
   "outputs": [],
   "source": [
    "item_names = ['Shark', 'Tuna', 'Robin', 'Canary', 'Rose', 'Daisy', 'Pine', 'Oak']\n",
    "has_bones = [0, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "tree_features = add_feature(tree_features, has_bones)\n",
    "plot_tree_data(tree_labels, tree_features, item_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the new feature shown in the last column of the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "outputId": "4956e801-dcaa-4ed7-eeeb-f911ec151cca"
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to train the network\n",
    "dim_input = tree_labels.shape[1]\n",
    "dim_output = tree_features.shape[1]\n",
    "input_tensor = torch.tensor(tree_labels).float()\n",
    "output_tensor = torch.tensor(tree_features).float()\n",
    "\n",
    "deep_model = VariableDepthWidth(in_dim=dim_input,\n",
    "                                out_dim=dim_output,\n",
    "                                hid_dims=[30])\n",
    "\n",
    "_, singular_values, _, ill_predictions = train_svd_rsa_track(deep_model,\n",
    "                                              input_tensor,\n",
    "                                              output_tensor,\n",
    "                                              n_epochs=250,\n",
    "                                              lr=100.0,\n",
    "                                              ill_i=0)\n",
    "\n",
    "plot_ills_sv_twin(ill_predictions, singular_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the network starts by learning the \"[alternative fact](https://en.wikipedia.org/wiki/Alternative_facts)\" that sharks have bones, and in later epochs, as it learns deeper representations, it can see (learn) beyond the illusory correlation. This is important to remember that we never presented the network with any data saying that sharks have bones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "1b645ef8-3b1a-4232-fd6e-19cb79133833"
   },
   "outputs": [],
   "source": [
    "#@title Video 4.2: Illusory Correlations Explained\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"QMuTlq-atlc\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Illusory Correlations\n",
    "\n",
    "This exercise is just for you to explore the idea of illusory correlations. Think of medical, natural or possibly social illusory correlations which can test the learning power of deep linear neural nets.\n",
    "\n",
    "**Notes**: Before you start, there are few things to important to know:\n",
    "\n",
    "* the generated data is independent of tree labels, therefore the names are just for convenience.\n",
    "\n",
    "* you can rename any node in the tree object(from the `SimpleTree` class) to help you keep track of your tree. This tree, also is not the one generating the samples and is purely for convenience.\n",
    "\n",
    "Here is our example for **Non-human Living things don't speak**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fae07f3b-fcb6-4ea1-b1e8-5e892dfdefd6"
   },
   "outputs": [],
   "source": [
    "# this is just for plotting a tree and has no connection to data!\n",
    "tree = SimpleTree()  # creates a tree\n",
    "tree.rename(\"Canary\", \"Parrot\")  # renames the Canary node to Parrot\n",
    "tree.plot()  # plots the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "outputId": "2ff2e7e4-9067-4703-90b5-41f1f563d18b"
   },
   "outputs": [],
   "source": [
    "item_names = ['Goldfish', 'Tuna', 'Robin', 'Parrot', 'Rose', 'Daisy', 'Pine', 'Oak']\n",
    "can_NOT_speak = [1, 1, 1, 0, 1, 1, 1, 1]  # creating the new feature\n",
    "ill_id = 3  # the index of your feature\n",
    "\n",
    "tree_labels, tree_features = generate_hsd()  # sampling new data from the tree\n",
    "tree_features = add_feature(tree_features, can_NOT_speak) # adding the feature\n",
    "plot_tree_data(tree_labels, tree_features, item_names)  # plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "outputId": "1027e81a-1daf-48d6-a6f9-8bbef931e0ca"
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to train the network and plot the output\n",
    "dim_input = tree_labels.shape[1]\n",
    "dim_output = tree_features.shape[1]\n",
    "input_tensor = torch.tensor(tree_labels).float()\n",
    "output_tensor = torch.tensor(tree_features).float()\n",
    "\n",
    "deep_model = VariableDepthWidth(in_dim=dim_input,\n",
    "                                out_dim=dim_output,\n",
    "                                hid_dims=[30])\n",
    "\n",
    "_, singular_values, _, ill_predictions = train_svd_rsa_track(deep_model,\n",
    "                                              input_tensor,\n",
    "                                              output_tensor,\n",
    "                                              n_epochs=250,\n",
    "                                              lr=100.0,\n",
    "                                              ill_i=ill_id)\n",
    "\n",
    "plot_ills_sv_twin(ill_predictions, singular_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Wrap up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "c0a2cecc-19a0-47df-fe84-f3490b27cec4"
   },
   "outputs": [],
   "source": [
    "#@title Video 4.3: Outro\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"Y0JfyCtikhc\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, *regression* refers to a set of methods for modeling the mapping (relationship) between one (or more) independent variable(s) (i.e. features) and one (or more) dependent variable(s) (i.e. labels). For example, if we want to examine the relative impacts of calendar date, GPS coordinates, and time of the say (the independent variables) on air temperature (the dependent variable). On the other hand, regression can be used for predictive analysis. Thus the independent variables are also called predictors. When the model contains more than one predictor, then the method is called *multiple regression*, and if it contains more than one dependent variable called *multivariate regression*. Regression problems pop up whenever we want to predict a numerical (usually continuous) value.\n",
    "\n",
    "The independent variables are collected in vector $\\mathbf{x} \\in \\mathbb{R}^M$, where $M$ denotes the number of independent variables, while the dependent variables are collected in vector $\\mathbf{y} \\in \\mathbb{R}^N$, where $N$ denotes the number of independent variables. And the mapping between them is represented by the weight matrix $\\mathbf{W} \\in \\mathbb{R}^{N \\times M}$ and a bias vector $\\mathbf{b} \\in \\mathbb{R}^{N}$ (generalizing to affine mappings).\n",
    "\n",
    "The multivariate regression model can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y} = \\mathbf{W} ~ \\mathbf{x} + \\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "or it can be written in matrix format as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{N} \\\\ \\end{bmatrix} = \\begin{bmatrix} w_{1,1} & w_{1,2} & \\dots & w_{1,M} \\\\ w_{2,1} & w_{2,2} & \\dots & w_{2,M} \\\\ \\vdots & \\ddots & \\ddots & \\vdots \\\\ w_{N,1} & w_{N,2} & \\dots & w_{N,M} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{M} \\\\ \\end{bmatrix} + \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\b_{N} \\\\ \\end{bmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vectorized regression__\n",
    "\n",
    "Linear regression can be simply extended to multi-samples ($D$) input-output mapping, which we can collect in a matrix $\\mathbf{X} \\in \\mathbb{R}^{M \\times D}$, sometimes called the design matrix. The sample dimension also shows up in the output matrix $\\mathbf{Y} \\in \\mathbb{R}^{N \\times D}$. Thus, linear regression takes the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Y} = \\mathbf{W} ~ \\mathbf{X} + \\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "where matrix $\\mathbf{W} \\in \\mathbb{R}^{N \\times M}$ and the vector $\\mathbf{b} \\in \\mathbb{R}^{N}$ (broudcasted over sample dimension) are the desired parameters to find."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D2_Tutorial3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
