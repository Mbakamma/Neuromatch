{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/W1D2_testing_for_video/tutorials/W1D2_LinearNN/student/W1D2_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DL Neuromatch Academy: Week 1, Day 2, Tutorial 2\n",
    "# Deep Linear Neural Networks\n",
    "\n",
    "__Content creators:__ Andrew Saxe, Saeed Salehi\n",
    "\n",
    "\n",
    "\n",
    "__Content reviewers:__ Name Surname, Name Surname. \n",
    "\n",
    "__Content editors:__ Name Surname, Name Surname.\n",
    "\n",
    "__Production editors:__ Name Surname, Name Surname.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Tutorial Objectives\n",
    "\n",
    "* Training landscape\n",
    "* The effect of depth\n",
    "* Choosing a learning rate\n",
    "* Initialisation matters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "outputId": "1482f9c6-f1f8-4a3c-a53e-0dce93421c78"
   },
   "outputs": [],
   "source": [
    "#@markdown Tutorial slides\n",
    "# you should link the slides for all tutorial videos here (we will store pdfs on osf)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://docs.google.com/presentation/d/1kfWWYhSIkczYfjebhMaqQILTCu7g94Q-o_ZcWb1QAKs/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "# import ipywidgets as widgets\n",
    "# from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, interact_manual\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "def gen_samples(n: int, a: float, σ: float):\n",
    "  \"\"\"generates `n` samples with `y = z * x + noise(σ)` linear relation. \"\"\"\n",
    "  assert n > 0\n",
    "  assert σ >= 0\n",
    "\n",
    "  if σ > 0:\n",
    "    x = np.random.rand(n)\n",
    "    noise = np.random.normal(scale = σ, size=(n))\n",
    "    y = a * x + noise\n",
    "  else:\n",
    "    x = np.linspace(0.0, 1.0, n, endpoint=True)\n",
    "    y = a * x\n",
    "  return x, y\n",
    "\n",
    "class ShallowNarrowLNN:\n",
    "  \"\"\"\n",
    "  Shallow and narrow (one neuron per layer) linear neural network\n",
    "  \"\"\"\n",
    "  def __init__(self, init_ws: list):\n",
    "    \"\"\"\n",
    "    init_ws: initial weights as a list\n",
    "    \"\"\"\n",
    "    assert isinstance(init_ws, list)\n",
    "    assert len(init_ws) == 2\n",
    "    self.w1 = init_ws[0]\n",
    "    self.w2 = init_ws[1]\n",
    "\n",
    "\n",
    "  def forward(self, x: np.ndarray):\n",
    "    \"\"\"\n",
    "    The forward pass through netwrok y = x * w1 * w2\n",
    "    \"\"\"\n",
    "    y = x * self.w1 * self.w2\n",
    "    return y\n",
    "\n",
    "\n",
    "  def loss(self, y_p: np.ndarray, y_t: np.ndarray):\n",
    "    \"\"\"\n",
    "    Mean squared error (L2) with 1/2 for convenience\n",
    "    \"\"\"\n",
    "    assert y_p.shape == y_t.shape\n",
    "    mse = ((y_t - y_p)**2).mean()\n",
    "    return mse\n",
    "\n",
    "  def dloss_dw(self, x: np.ndarray, y_t: np.ndarray):\n",
    "    \"\"\"\n",
    "    partial derivative of loss with respect to weights\n",
    "    \"\"\"\n",
    "    assert x.shape == y_t.shape\n",
    "    Error = y_t - self.w1 * self.w2 * x\n",
    "    dloss_dw1 = - (2 * self.w2 * x * Error).mean()\n",
    "    dloss_dw2 = - (2 * self.w1 * x * Error).mean()\n",
    "    return dloss_dw1, dloss_dw2\n",
    "\n",
    "  def train(self, x: np.ndarray, y_t: np.ndarray, η: float, n_ep: int):\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm\n",
    "    \"\"\"\n",
    "    assert x.shape == y_t.shape\n",
    "\n",
    "    loss_records = np.empty(n_ep)  # pre allocation of loss records\n",
    "    weight_records = np.empty((n_ep, 2))  # pre allocation of weight records\n",
    "\n",
    "    for i in range(n_ep):\n",
    "      y_p = self.forward(x)\n",
    "      loss_records[i] = self.loss(y_p, y_t)\n",
    "      dloss_dw1, dloss_dw2 = self.dloss_dw(x, y_t)\n",
    "      self.w1 -= η * dloss_dw1\n",
    "      self.w2 -= η * dloss_dw2\n",
    "      weight_records[i] = [self.w1, self.w2]\n",
    "\n",
    "    return loss_records, weight_records\n",
    "\n",
    "\n",
    "class DeepNarrowLNN:\n",
    "  \"\"\"\n",
    "  Deep but thin (one neuron per layer) linear neural network\n",
    "  \"\"\"\n",
    "  def __init__(self, init_ws: np.ndarray):\n",
    "    \"\"\"\n",
    "    init_ws: initial weights as a numpy array\n",
    "    \"\"\"\n",
    "    self.n = init_ws.size\n",
    "    self.W = init_ws.reshape(1, -1)\n",
    "\n",
    "\n",
    "  def forward(self, x: np.ndarray):\n",
    "    \"\"\"\n",
    "    x: input features\n",
    "    \"\"\"\n",
    "    y = np.prod(self.W) * x\n",
    "    return y\n",
    "\n",
    "\n",
    "  def loss(self, y_t, y_p):\n",
    "    \"\"\"\n",
    "    mean squared error (L2 loss)\n",
    "    \"\"\"\n",
    "    assert y_p.shape == y_t.shape\n",
    "    mse = ((y_t - y_p)**2 / 2).mean()\n",
    "    return mse\n",
    "\n",
    "\n",
    "  def dloss_dw(self, x, y_t, y_p):\n",
    "    \"\"\"\n",
    "    analytical gradient of weights\n",
    "    \"\"\"\n",
    "    E = y_t - y_p  # = y_t - x * np.prod(self.W)\n",
    "    Ex = np.multiply(x, E).mean()\n",
    "    Wp = np.prod(self.W) / (self.W + 1e-9)\n",
    "    dW = - Ex * Wp\n",
    "    return dW\n",
    "\n",
    "\n",
    "  def train(self, x, y_t, η, n_epochs):\n",
    "    \"\"\"\n",
    "    training using gradient descent\n",
    "    \"\"\"\n",
    "    loss_records = np.empty(n_epochs)\n",
    "    loss_records[:] = np.nan\n",
    "    for i in range(n_epochs):\n",
    "      y_p = self.forward(x)\n",
    "      loss_records[i] = self.loss(y_t, y_p).mean()\n",
    "      dloss_dw = self.dloss_dw(x, y_t, y_p)\n",
    "      if np.isnan(dloss_dw).any() or np.isinf(dloss_dw).any():\n",
    "        return loss_records\n",
    "      self.W -= η * dloss_dw\n",
    "    return loss_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Plotting functions\n",
    "\n",
    "def plot_x_y_(x_t_, y_t_, x_ev_, y_ev_, loss_log_, weight_log_):\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(12, 4))\n",
    "  plt.subplot(1, 3, 1)\n",
    "  plt.scatter(x_t_, y_t_, c='r', label='training data')\n",
    "  plt.plot(x_ev_, y_ev_, c='b', label='test results', linewidth=2)\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('y')\n",
    "  plt.legend()\n",
    "  plt.subplot(1, 3, 2)\n",
    "  plt.plot(loss_log_, c='r')\n",
    "  plt.xlabel('epochs')\n",
    "  plt.ylabel('mean squared error')\n",
    "  plt.subplot(1, 3, 3)\n",
    "  plt.plot(weight_log_)\n",
    "  plt.xlabel('epochs')\n",
    "  plt.ylabel('weights')\n",
    "  plt.show()\n",
    "\n",
    "def plot_vector_field(what, init_weights=None):\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  n_epochs=40\n",
    "  lr=0.15\n",
    "  x_pos = np.linspace(2.0, 0.5, 100, endpoint=True)\n",
    "  y_pos = 1. / x_pos\n",
    "  xx, yy = np.mgrid[-1.9:2.0:0.2, -1.9:2.0:0.2]\n",
    "  zz = np.empty_like(xx)\n",
    "  x, y = xx[:, 0], yy[0]\n",
    "\n",
    "  x_temp, y_temp = gen_samples(10, 1.0, 0.0)\n",
    "\n",
    "  cmap = matplotlib.cm.plasma\n",
    "  plt.figure(figsize=(7, 7))\n",
    "\n",
    "  if what == 'all' or what == 'vectors':\n",
    "    for i, a in enumerate(x):\n",
    "      for j, b in enumerate(y):\n",
    "        temp_model = ShallowNarrowLNN([a, b])\n",
    "        da, db = temp_model.dloss_dw(x_temp, y_temp)\n",
    "        zz[i, j] = temp_model.loss(temp_model.forward(x_temp), y_temp)\n",
    "        scale = min(40 * np.sqrt(da**2 + db**2), 50)\n",
    "        plt.quiver(a, b, - da, - db, scale=scale, color=cmap(np.sqrt(da**2 + db**2)))\n",
    "\n",
    "  if what == 'all' or what == 'trajectory':\n",
    "    if init_weights is None:\n",
    "      for init_weights in [[0.5, -0.5], [0.55, -0.45], [-1.8, 1.7]]:\n",
    "        temp_model = ShallowNarrowLNN(init_weights)\n",
    "        _, temp_records = temp_model.train(x_temp, y_temp, lr, n_epochs)\n",
    "        plt.scatter(temp_records[:, 0], temp_records[:, 1],\n",
    "                    c=np.arange(len(temp_records)), cmap='Greys')\n",
    "        plt.scatter(temp_records[0, 0], temp_records[0, 1], c='blue', zorder=9)\n",
    "        plt.scatter(temp_records[-1, 0], temp_records[-1, 1], c='red', marker='X', s=100, zorder=9)\n",
    "    else:\n",
    "        temp_model = ShallowNarrowLNN(init_weights)\n",
    "        _, temp_records = temp_model.train(x_temp, y_temp, lr, n_epochs)\n",
    "        plt.scatter(temp_records[:, 0], temp_records[:, 1],\n",
    "                    c=np.arange(len(temp_records)), cmap='Greys')\n",
    "        plt.scatter(temp_records[0, 0], temp_records[0, 1], c='blue', zorder=9)\n",
    "        plt.scatter(temp_records[-1, 0], temp_records[-1, 1], c='red', marker='X', s=100, zorder=9)\n",
    "\n",
    "  if what == 'all' or what == 'loss':\n",
    "    plt.contourf(x, y, np.log(zz+0.001), zorder=-1, cmap='coolwarm', levels=100)\n",
    "  plt.xlabel(\"$w_1$\")\n",
    "  plt.ylabel(\"$w_2$\")\n",
    "  plt.xlim(-1.9, 1.9)\n",
    "  plt.ylim(-1.9, 1.9)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def plot_loss_landscape():\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  x_temp, y_temp = gen_samples(10, 1.0, 0.0)\n",
    "\n",
    "  xx, yy = np.mgrid[-1.9:2.0:0.2, -1.9:2.0:0.2]\n",
    "  zz = np.empty_like(xx)\n",
    "  x, y = xx[:, 0], yy[0]\n",
    "\n",
    "  for i, a in enumerate(x):\n",
    "    for j, b in enumerate(y):\n",
    "      temp_model = ShallowNarrowLNN([a, b])\n",
    "      zz[i, j] = temp_model.loss(temp_model.forward(x_temp), y_temp)\n",
    "\n",
    "  temp_model = ShallowNarrowLNN([-1.8, 1.7])\n",
    "  loss_rec_1, w_rec_1 = temp_model.train(x_temp, y_temp, 0.02, 240)\n",
    "\n",
    "  temp_model = ShallowNarrowLNN([1.5, -1.5])\n",
    "  loss_rec_2, w_rec_2 = temp_model.train(x_temp, y_temp, 0.02, 240)\n",
    "\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  ax = plt.subplot(1, 1, 1, projection='3d')\n",
    "  ax.plot_surface(xx, yy, np.log(zz+0.5), cmap='coolwarm', alpha=0.5)\n",
    "  ax.scatter3D(w_rec_1[:, 0], w_rec_1[:, 1], np.log(loss_rec_1+0.5),\n",
    "                c='k', s=50, zorder=9)\n",
    "  ax.scatter3D(w_rec_2[:, 0], w_rec_2[:, 1], np.log(loss_rec_2+0.5),\n",
    "                c='k', s=50, zorder=9)\n",
    "  plt.axis(\"off\")\n",
    "  ax.view_init(45, 260)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def depth_widget(depth):\n",
    "  depth_lr_init_interplay(depth, 0.01, 0.9)\n",
    "\n",
    "\n",
    "def lr_widget(lr):\n",
    "  depth_lr_init_interplay(50, lr, 0.9)\n",
    "\n",
    "\n",
    "def depth_lr_interplay(depth, lr):\n",
    "  depth_lr_init_interplay(depth, lr, 0.9)\n",
    "\n",
    "\n",
    "def depth_lr_init_interplay(depth, lr, init_weights):\n",
    "  n_epochs = 500\n",
    "\n",
    "  x_train, y_train = gen_samples(100, 2.0, 0.1)\n",
    "  model = DeepNarrowLNN(np.full((1, depth), init_weights))\n",
    "\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.plot(model.train(x_train, y_train, lr, n_epochs),\n",
    "            linewidth=3.0, c='m')\n",
    "\n",
    "  plt.title(\"Training a {}-layer LNN with\"\n",
    "  \" $\\eta=${} initialized with $w_i=${}\".format(depth, lr, init_weights), pad=15)\n",
    "  plt.yscale('log')\n",
    "  plt.xlabel('epochs')\n",
    "  plt.ylabel('Log mean squared error')\n",
    "  plt.ylim(0.001, 1.0)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_init_effect():\n",
    "  depth = 15\n",
    "  n_epochs = 250\n",
    "  lr = 0.02\n",
    "\n",
    "  x_train, y_train = gen_samples(100, 2.0, 0.1)\n",
    "\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  for init_w in np.arange(0.7, 1.09, 0.05):\n",
    "      model = DeepNarrowLNN(np.full((1, depth), init_w))\n",
    "      plt.plot(model.train(x_train, y_train, lr, n_epochs),\n",
    "              linewidth=3.0, label=\"initial weights {:.2f}\".format(init_w))\n",
    "  plt.title(\"Training a {}-layer narrow LNN with $\\eta=${}\".format(depth, lr), pad=15)\n",
    "  plt.yscale('log')\n",
    "  plt.xlabel('epochs')\n",
    "  plt.ylabel('Log mean squared error')\n",
    "  plt.legend(loc='lower left', ncol=4)\n",
    "  plt.ylim(0.001, 1.0)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: A shallow Narrow Linear Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "outputId": "2e4b1db7-af4e-4407-9181-894ffb9a8a63"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Intro\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"yTBYmFWslZI\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1:\n",
    "\n",
    "To better understand the behaviour of a neural network training evolution through gradient descent, we start with the incredibly simple case of shallow narrow linear neural net, since the state of the art models are impossible to dissect and comprehend with our current mathematical tools.\n",
    "\n",
    "The model we use has two hidden layers, each with only one neuron. We also consider L2-loss as the cost function. As you may have already guessed, we can visualize the model as a neural network:\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/ssnio/statics/main/neuromatch/shallow_narrow_nn.png\" width=\"400\"/></center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "or by its computation graph:\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/ssnio/statics/main/neuromatch/shallow_narrow.png\" alt=\"Shallow Narrow Graph\" width=\"400\"/></center>\n",
    "\n",
    "or in a rare occasion, even as a reasonably compact mapping:\n",
    "\n",
    "$$ loss = (y - w_1 \\cdot w_2 \\cdot x)^2 $$\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate gradients\n",
    "\n",
    "Once again, we ask you to calculate the network gradients analytically, since you will need them for the next exercise. We understand how annoying this is.\n",
    "\n",
    "$\\frac{\\partial{loss}}{\\partial{w_1}} = ?$\n",
    "\n",
    "$\\frac{\\partial{loss}}{\\partial{w_2}} = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement the Shallow Narrow LNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we ask you to implement the `forward` pass for our model from scratch without using PyTorch.\n",
    "\n",
    "Also, although our model gets a single input feature and outputs a single prediction, we could calculate the loss and perform training for multiple samples at once. This is the common practice for neural networks, since computers are incredibly fast doing matrice (or tensors in general) operations, rather than processing samples through `for` loops. Therefore, for the `loss` function, please implement the **mean** squred error (MSE), and adjust your analytical gradients to accomodate the mean in MSE, while implementing the `dloss_dw` function.\n",
    "\n",
    "Finally, you should be able to complete the `train` function gradient descent algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNarrowExercise:\n",
    "  \"\"\"\n",
    "  Shallow and narrow (one neuron per layer) linear neural network\n",
    "  \"\"\"\n",
    "  def __init__(self, init_ws: list):\n",
    "    \"\"\"\n",
    "    init_ws: initial weights as a list\n",
    "    \"\"\"\n",
    "    assert isinstance(init_ws, list)\n",
    "    assert len(init_ws) == 2\n",
    "    self.w1 = init_ws[0]\n",
    "    self.w2 = init_ws[1]\n",
    "\n",
    "\n",
    "  def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The forward pass through netwrok y = x * w1 * w2\n",
    "    \"\"\"\n",
    "    #################################################\n",
    "    ## Implement the forward pass to calculate prediction\n",
    "    ## Note that prediction is not the loss\n",
    "    # Complete the function and remove or comment the line below\n",
    "    raise NotImplementedError(\"Forward Pass `forward`\")\n",
    "    #################################################\n",
    "    y = ...\n",
    "    return y\n",
    "\n",
    "\n",
    "  def dloss_dw(self, x: np.ndarray, y_t: np.ndarray):\n",
    "    \"\"\"\n",
    "    partial derivative of loss with respect to weights\n",
    "    \"\"\"\n",
    "    assert x.shape == y_t.shape\n",
    "    #################################################\n",
    "    ## Implement the gradient computation function\n",
    "    # Complete the function and remove or comment the line below\n",
    "    raise NotImplementedError(\"Forward Pass `dloss_dw`\")\n",
    "    #################################################\n",
    "    dloss_dw1 = ...\n",
    "    dloss_dw2 = ...\n",
    "    return dloss_dw1, dloss_dw2\n",
    "\n",
    "\n",
    "  def train(self, x: np.ndarray, y_t: np.ndarray, η: float, n_ep: int):\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm\n",
    "    \"\"\"\n",
    "    assert x.shape == y_t.shape\n",
    "\n",
    "    loss_records = np.empty(n_ep)  # pre allocation of loss records\n",
    "    weight_records = np.empty((n_ep, 2))  # pre allocation of weight records\n",
    "\n",
    "    for i in range(n_ep):\n",
    "      y_p = self.forward(x)\n",
    "      loss_records[i] = loss(y_p, y_t)\n",
    "      dloss_dw1, dloss_dw2 = self.dloss_dw(x, y_t)\n",
    "      #################################################\n",
    "      ## Implement the gradient descent step\n",
    "      # Complete the function and remove or comment the line below\n",
    "      raise NotImplementedError(\"Forward Pass `train`\")\n",
    "      #################################################\n",
    "      self.w1 -= ...\n",
    "      self.w2 -= ...\n",
    "      weight_records[i] = [self.w1, self.w2]\n",
    "\n",
    "    return loss_records, weight_records\n",
    "\n",
    "\n",
    "def loss(y_p: np.ndarray, y_t: np.ndarray):\n",
    "  \"\"\"\n",
    "  Mean squared error\n",
    "  \"\"\"\n",
    "  assert y_p.shape == y_t.shape\n",
    "  #################################################\n",
    "  ## Implement the MEAN squared error\n",
    "  # Complete the function and remove or comment the line below\n",
    "  raise NotImplementedError(\"Forward Pass `loss`\")\n",
    "  #################################################\n",
    "  mse = ...\n",
    "  return mse\n",
    "\n",
    "\n",
    "n_epochs = 211\n",
    "learning_rate = 0.02\n",
    "initial_weights = [1.4, -1.6]\n",
    "x_train, y_train = gen_samples(n=73, a=2.0, σ=0.2)\n",
    "x_eval = np.linspace(0.0, 1.0, 37, endpoint=True)\n",
    "\n",
    "## Uncomment the following code to run\n",
    "# sn_model = ShallowNarrowExercise(initial_weights)\n",
    "# loss_log, weight_log = sn_model.train(x_train, y_train, learning_rate, n_epochs)\n",
    "# y_eval = sn_model.forward(x_eval)\n",
    "# plot_x_y_(x_train, y_train, x_eval, y_eval, loss_log, weight_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "text",
    "outputId": "bcafd144-8c72-4b29-f5fb-0da1e401f593"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearNN/solutions/W1D2_Tutorial2_Solution_1c6a37b1.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=848 height=272 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearNN/static/W1D2_Tutorial2_Solution_1c6a37b1_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Training landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have already asked yourself, we can analytically find $w_1$ and $w_2$ without using gradient descent:\n",
    "\n",
    "$$w_1 \\cdot w_2 = \\frac{y}{x} $$\n",
    "\n",
    "In fact, we can plot the gradients, the loss function and all the solutions in one figure. In this example, we use the $y = 1x$ mapping:\n",
    "\n",
    "**Blue ribbon**: shows all the possible solutions: $~ w_1 w_2 = \\frac{y}{x} = \\frac{x}{x} = 1 \\Rightarrow w_1 = \\frac{1}{w_2}$\n",
    "\n",
    "**Contour background**: Loss values, red being higher loss\n",
    "\n",
    "**Vectors filed (arrows)**: the gradient vector field, the larger yellow arrows show larger gradients (stronger fields), which correspond to bigger steps by gradient descent.\n",
    "\n",
    "**Scatter circles**: the trajectory (evolution) of weights during training for three different initialization, with red crosses ( **x** ) marking the end of training. You can also try your own initializations (keep the initial values between `-2.0` and `2.0`) as shown here:\n",
    "```python\n",
    "plot_vector_field('all', [1.0, -1.0])\n",
    "```\n",
    "\n",
    "Finally, if the plot is too crowded, feel free to pass one of the following strings as argument:\n",
    "\n",
    "```python\n",
    "plot_vector_field('vectors')  # for vector field\n",
    "plot_vector_field('trajectory')  # for training trajectory\n",
    "plot_vector_field('loss')  # for loss contour\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: The saddle point\n",
    "\n",
    "Discuss with your pod, everything you see and infer in the next two plots. Can you find the saddle point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "outputId": "0733ef1a-78a4-42ef-da15-a2b10216ba26"
   },
   "outputs": [],
   "source": [
    "plot_vector_field('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we also visualize the loss landscape in a 3-D plot, with two training trajectories for different initial conditions.\n",
    "Note: the trajectories from the 3D plot and the previous plot are independent and different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "outputId": "94423f74-ed12-48a3-df4c-f5eb400e7b91"
   },
   "outputs": [],
   "source": [
    "plot_loss_landscape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Depth, Learning rate, and initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course of this summer school, we will see how powerful and versatile deep learning algorithms can be. But we will also realize how challenging the process of actually training deep networks can be. Successful deep learning models are often developed by a team of clever people, spending many many hours \"designing\" the architecture of their model, \"tuning\" learning hyperparameters, and finding effective initializations.\n",
    "\n",
    "In this section, we look at three basic (but often not simple) challenges when approaching neural networks: depth, learning rate, and initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: The effect of depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1989, George Cybenko published the paper *Approximation by superpositions of a sigmoidal function* (a highly recommended paper), mathematically proving that:\n",
    "\n",
    "> arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity.\n",
    "\n",
    "So if a shallow neural net can approximate any function, Why might depth be useful? What makes a network or learning system \"deep\"? Sometimes depth seems like magic. Depth can change the functions a network can represent, the way a network learns, and how a network generalizes to unseen data. \n",
    "\n",
    "But first, lets look at the challenges that depth poses in training a neural network. Imagine a single input, single output linear network with 50 hidden layers but only one neuron per layer. The output of the network is easy to calculate:\n",
    "\n",
    "$$ y_p = x \\cdot w_1 \\cdot w_2 \\cdot \\cdot \\cdot w_{50} $$\n",
    "\n",
    "If the initial value for all the weights is $w_i = 2$, the prediction for $x=1$ would be $y_p = 2^{50} \\approx 1.1256 \\times 10^{15}$. Also, for weights initialized to $w_i = 0.5$, the output is $y_p = 0.5^{50} \\approx 8.88 \\times 10^{-16}$. Similarly, if you recall the chain rule, as the graph gets deeper, the number of elements in the chain multiplication increases, leading to exploding or vanishing gradients. To avoid such numerical vulnerablities which could impair our training algorithm, we need to understand their effect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Depth widget\n",
    "\n",
    "Use the widget to explore the impact of depth on the training curve (loss evolution) of a deep but narrow neural network and discuss your findings with your pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390,
     "referenced_widgets": [
      "741a651093c647e6aed5d50a01413667",
      "d9c67bd204954ed5ab15a197cdecb4fa",
      "3442f44c45ee4fd9a7ba97752489272e",
      "dc3b1c2c0420492aa3bd1da81a4f19b0",
      "1fe34a70ec3c4a3f8eb17ec76df14022",
      "4eadf6927132421a8a0765d4da552adb",
      "b34b39ba4e864450b803197f531a6373"
     ]
    },
    "outputId": "f118824f-91e5-408b-c10f-7a7c0cebc51e"
   },
   "outputs": [],
   "source": [
    "# #@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "_ = interact(depth_widget,\n",
    "    depth = IntSlider(min=10, max=51, step=5, value=25, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "Which networks trained the fastest? Did all networks ‘work’ eventually? What is the shape of their learning trajectory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Choosing a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is the basic, but not the only, hyperparameter of most of optimization algorithms. Sometimes the only option is to try all the possibilities, but sometimes knowing some basic trade offs will help guide our seach for the optimums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Learning rate widget\n",
    "\n",
    "Use the widget to explore the impact of learning rate $\\eta$ on the training curve (loss evolution) of a deep but narrow neural network and discuss your findings with your pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390,
     "referenced_widgets": [
      "91d69e12d3ad4c4b95fee520cc55285c",
      "48ae2d2f9ebb4a20b11d1e209c982721",
      "353b00122f9849fc973c2d43b9b859e4",
      "8e87476f86e645ebbf206143f4ab881f",
      "522360a236e2409aa1f3710bea6622a0",
      "dbb03ffd73b344eb953cb87f83743ebd",
      "66d89190007d46e087617a610c2b8951"
     ]
    },
    "outputId": "9f8d18b9-528f-44d2-fc4d-19f0f74b9cd3"
   },
   "outputs": [],
   "source": [
    "# #@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "_ = interact(lr_widget,\n",
    "    lr = FloatSlider(min=0.005, max=0.045, step=0.005, value=0.005,\n",
    "                     continuous_update=False, readout_format='.3f', description='η'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "Can we say that larger learning rates always lead to faster learning? Why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Depth-Learning-Rate-Interplay\n",
    "\n",
    "Use the widgets to explore the interplay between learning rate $\\eta$ and depth while searching for an smooth and fast training regime for our deep but narrow neural network and discuss your findings with your pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422,
     "referenced_widgets": [
      "7a54a754dd2347e0a100acd2f03d0cd8",
      "e21b0bbab3174f4f990b10a8b0b0dffc",
      "977b9ef49b29415ca9367e23cc667cf2",
      "bdfff50896124d429e39eb255849fc64",
      "a3b3a23b4d3247d7938e27cf0bc43863",
      "4ea14ad35667476aa7d505b6f19c8421",
      "0afa07a356d24082830711595b76ea84",
      "2c0bfaa1b24e4f65a90b7848242b3632",
      "5b3d75b125134af68b062269fdeeba5d",
      "b3c0412058e745c5a6a5504ccc8e8d3d"
     ]
    },
    "outputId": "6c7d0194-cb7a-402e-9d7c-816eea215775"
   },
   "outputs": [],
   "source": [
    "# #@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "_ = interact(depth_lr_interplay,\n",
    "             depth = IntSlider(min=10, max=51, step=5, value=25, continuous_update=False),\n",
    "             lr = FloatSlider(min=0.005, max=0.1, step=0.005, value=0.005,\n",
    "                     continuous_update=False, readout_format='.3f', description='η'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Can you explain the relationship between the depth and optimal learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3: Initialisation matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve seen, even in the simplest of cases, that depth can slow learning. Why? Based on the chain rule, gradients are multiplied by current weight at each layer, so the product can vanish or explode. Therefore, weight initialisation is a fundamentally important hyperparameter.\n",
    "\n",
    "The figure below shows the effect of initialization on the speed of learning for the deep but narrow LNN. We have excluded the initializations that could lead to nummerical errors such as `nan` or `inf` which are the consequence of smaller or larger initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "outputId": "76a512ad-28b2-4f22-803a-23ab9453901e"
   },
   "outputs": [],
   "source": [
    "# #@markdown Make sure you execute this cell to see the figure!\n",
    "\n",
    "plot_init_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Hyperparameter interaction\n",
    "\n",
    "Finally, lets put everything we learned together and find best initial weight and learning rate for a given depth. By now you should have learned the interactions and know how to find the optimal values quickly. If you get `numerical overflow` warnings, don't be discouraged! They are often caused by \"exploding\" or \"vanishing\" gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454,
     "referenced_widgets": [
      "9b9c1becc4a049a09c47fb45cc309c1f",
      "382737f14e9346a9988fde975dbb49a0",
      "e2e82d4d314a4ba79f394ccd97afc5fe",
      "b80c9527703843d8871b1665d24c3dc5",
      "9ee79363d2db42d08d4ff047d52dc8e2",
      "af1d47fe396e4401b8ac861a93e27c03",
      "5a59e498068346beb81757348090ce01",
      "beb48a8f90344fa8851774510a4aaa95",
      "7878b4d7d17e4d9f8f6e180de0f3948a",
      "8446d25d5c9946ea84b065ca54f66026",
      "515faf91caba45e3a380a140c404a8a2",
      "dd83c17547054ff09cb21ddb49fd57f5",
      "57240056a7e54f26915b0585f1cd3e52"
     ]
    },
    "outputId": "bbbd711c-6cd7-44c8-fda9-32dd14c8810c"
   },
   "outputs": [],
   "source": [
    "# #@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "_ = interact(depth_lr_init_interplay,\n",
    "             depth = IntSlider(min=10, max=51, step=5, value=25, continuous_update=False),\n",
    "             lr = FloatSlider(min=0.001, max=0.1, step=0.005, value=0.005,\n",
    "                     continuous_update=False, readout_format='.3f', description='η'),\n",
    "             init_weights = FloatSlider(min=0.1, max=3.0, step=0.1, value=0.9,\n",
    "                     continuous_update=False, readout_format='.3f', description='initial weights'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Did you experience any surprising behaviour or difficulty finding the optimal parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4: Initialization of deep high dimensional Networks (Optional)\n",
    "\n",
    "**PyTorch:**  \n",
    "the learnable parameters (weights and biases) of a linear layer with shape ($n_{in}$, $n_{out}$) are initialized with values sampled from uniform distribution:\n",
    "$$\\mathcal{U}(-\\sqrt{\\frac{1}{n_{in}}}, ~~\\sqrt{\\frac{1}{n_{in}}})$$\n",
    "for more details, see [torch.nn.linear](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py) and [torch.nn.init](https://pytorch.org/docs/stable/nn.init.html).\n",
    "\n",
    "\n",
    "**Xavier:**  \n",
    "the learnable parameters (weights and biases) of a layer with shape ($n_{in}$, $n_{out}$) are initialized with values sampled from either normal distributions:\n",
    "\n",
    "$$\\mathcal{N}\\left(\\mu=0, ~~\\sigma=\\sqrt{\\frac{2}{n_{in} + n_{out}}} \\right)$$\n",
    "\n",
    "or uniform distribution:\n",
    "\n",
    "$$\\mathcal{U}\\left(- \\sqrt{\\frac{6}{n_{in} + n_{out}}},~~ \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right)$$\n",
    "\n",
    "which is named after by *Xavier Glorot*, [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a.html).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D2_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
