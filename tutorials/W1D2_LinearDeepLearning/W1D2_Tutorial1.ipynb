{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W1D2_Tutorial1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/test_jupyterbook/tutorials/W1D2_LinearDeepLearning/W1D2_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "pM2YdhC4WPik"
      },
      "source": [
        "\n",
        "# DL Neuromatch Academy: Week 1, Day 2, Tutorial 1\n",
        "# Gradients and AutoGrad\n",
        "\n",
        "__Content creators:__ Saeed Salehi, Vladimir Haltakov, Andrew Saxe\n",
        "\n",
        "\n",
        "\n",
        "__Content reviewers:__ Polina Turishcheva, Atnafu Lambebo, Yu-Fang Yang\n",
        "\n",
        "__Content editors:__ Anoop Kulkarni\n",
        "\n",
        "__Production editors:__ Khalid Almubarak, Spiros Chavlis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JhL-bEAlWPil"
      },
      "source": [
        "---\n",
        "#Tutorial Objectives\n",
        "\n",
        "Day 2 Tutorial 1 will continue on buiding PyTorch skillset and motivate its core functionality, Autograd. In this notebook, we will cover the key concepts and ideas of:\n",
        "\n",
        "* Gradient descent\n",
        "* PyTorch Autograd\n",
        "* PyTorch nn module\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "88Nt6I1GWPim"
      },
      "source": [
        "#@markdown Tutorial slides\n",
        "# you should link the slides for all tutorial videos here (we will store pdfs on osf)\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML('<iframe src=\"https://docs.google.com/presentation/d/1kfWWYhSIkczYfjebhMaqQILTCu7g94Q-o_ZcWb1QAKs/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "XQM4hmGfWPim"
      },
      "source": [
        "---\n",
        "# Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "vjC_WT1qWPin"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "import random\n",
        "\n",
        "from tqdm.notebook import tqdm, trange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "CsEc0aKkWPin"
      },
      "source": [
        "#@title Figure settings\n",
        "\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "wovKT0sTWPin"
      },
      "source": [
        "#@title Plotting functions\n",
        "\n",
        "def ex3_plot(epochs, losses, values, gradients):\n",
        "  f, (plot1, plot2, plot3) = plt.subplots(3, 1, sharex=True, figsize=(10, 7))\n",
        "  plot1.set_title(\"Cross Entropy Loss\")\n",
        "  plot1.plot(np.linspace(1, epochs, epochs), losses, color='r')\n",
        "\n",
        "  plot2.set_title(\"First Parameter value\")\n",
        "  plot2.plot(np.linspace(1, epochs, epochs), values, color='c')\n",
        "\n",
        "  plot3.set_title(\"First Parameter gradient\")\n",
        "  plot3.plot(np.linspace(1, epochs, epochs), gradients, color='m')\n",
        "  plot3.set_xlabel(\"Epoch\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "DXO5c1WGWPio"
      },
      "source": [
        "#@title Helper functions\n",
        "\n",
        "seed = 1943  # McCulloch & Pitts (1943)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "_jwANe3gWPio"
      },
      "source": [
        "---\n",
        "# Section 1: Gradient Descent Algorithm\n",
        "\n",
        "Since the goal of most learning algorithms is **minimizing the risk (cost) function**, optimization is the soul of learning! The gradient descent algorithm, along with its variations such as stochastic gradient descent, is one of the most powerful and popular optimization methods used for deep learning.\n",
        "\n",
        "## 1.1: Gradient Descent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "_h37z2S6WPip"
      },
      "source": [
        "#@title Video 1.1: Introduction\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"PFQeUDxQFls\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "TRemg8yoWPip"
      },
      "source": [
        "#@title Video 1.2: Gradient Descent\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"Z3dyRLR8GbM\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ervW55miWPip"
      },
      "source": [
        "\n",
        "Gradient Descent (introduced by Augustin-Louis Cauchy in 1847) is an **iterative method** to **minimize** a **continuous** and (ideally) **differentiable function** of **many variables**.\n",
        "\n",
        "### Definition\n",
        "Let $f(\\mathbf{w}): \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be a differentiable function. Gradient Descent is an iterative algorithm for minimizing the function $f$, starting with an initial value for variables $\\mathbf{w}$, taking steps of size $\\eta$ in the direction of the negative gradient at the current point to update the variables $\\mathbf{w}$.\n",
        "\n",
        "$$ \\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla f (\\mathbf{w}^{(t)}) $$\n",
        "\n",
        "where $\\eta > 0$ and $\\nabla f (\\mathbf{w})= \\left( \\frac{\\delta f(\\mathbf{w})}{\\delta w_1}, ..., \\frac{\\delta f(\\mathbf{w})}{\\delta w_d} \\right)$. Since negative gradients always point locally in the direction of steepest descent, the algorithm makes small steps at each point **towards** the minimum.\n",
        "  \n",
        "<br/>\n",
        "\n",
        "### Vanilla Algorithm\n",
        "\n",
        "---\n",
        "> *inputs*: initial guess $\\mathbf{w}^{(0)}$, step size $\\eta > 0$, number of steps $T$\n",
        "\n",
        "> *For* *t = 1, 2, ..., T* *do* \\\n",
        "$\\qquad$ $\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla f (\\mathbf{w}^{(t)})$\\\n",
        "*end*\n",
        "\n",
        "> *return*: $\\mathbf{w}^{(t+1)}$\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "To be able to use this algorithm, we need to calculate the gradient of the loss with respect to the learnable parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kIns1032WPiq"
      },
      "source": [
        "## 1.2: Calculating Gradients\n",
        "\n",
        "To minimize the empirical risk (loss) function using gradient descent, we need to calculate the vector of partial derivatives:\n",
        "\n",
        "$$\\dfrac{\\partial Loss}{\\partial \\mathbf{w}} = \\left[ \\dfrac{\\partial Loss}{\\partial w_1}, \\dfrac{\\partial Loss}{\\partial w_2} , ..., \\dfrac{\\partial Loss}{\\partial w_d} \\right]^{\\top} $$\n",
        "\n",
        "Although PyTorch and other deep learning frameworks (e.g. JAX and TensorFlow) provide us with incredibly powerful and efficient algorithms for automatic differentiation, calculating few derivatives with hand would be fun.\n",
        "\n",
        "### Exercise 1.2\n",
        "1.  Given $L(w_1, w_2) = w_1^2 - 2w_1 w_2$ find $\\dfrac{\\partial L}{\\partial w_1}$ and $\\dfrac{\\partial L}{\\partial w_1}$.\n",
        "\n",
        "<br/>\n",
        "\n",
        "2.  Given $f(x) = sin(x)$ and $g(x) = \\ln(x)$, find the derivative of their composite function $\\dfrac{d (f \\circ g)(x)}{d x}$ (*hint: chain rule*).\n",
        "\n",
        "  **Chain rule**: For a composite function $F(x) = f(g(x)) \\equiv (f \\circ g)(x)$:\n",
        "$$F'(x) = f'(g(x)) \\cdot g'(x)$$\n",
        "or differently denoted:\n",
        "$$ \\frac{dF}{dx} = \\frac{df}{dg} ~ \\frac{dg}{dx} $$\n",
        "\n",
        "<br/>\n",
        "\n",
        "3.  Given $f(x, y, z) = \\tanh \\left( \\ln \\left[1 + z \\frac{2x}{sin(y)} \\right] \\right)$, how easy is it to derive $\\dfrac{\\partial f}{\\partial x}$, $\\dfrac{\\partial f}{\\partial y}$ and $\\dfrac{\\partial f}{\\partial z}$? (*hint: you don't have to actually calculate them!*)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dY98Vgh-WPiq"
      },
      "source": [
        "## 1.3: Computational Graphs and Backprop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "P-BaEN1tWPir"
      },
      "source": [
        "#@title Video 1.3: Computational Graph\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"7c8iCHcVgVs\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "c75FUOkJWPir"
      },
      "source": [
        "\n",
        "The last function in *Exercise 1.2* is an example of how overwhelming the derivation of gradients can get, as the number of variables and nested functions increases. This function is still extremely simple compared to the loss functions of modern neural networks. So how do PyTorch and similar frameworks approach such beasts?\n",
        "\n",
        "### 1.3.1: Computational Graphs\n",
        "\n",
        "Let’s look at the function again:\n",
        "\n",
        "$$f(x, y, z) = \\tanh \\left(\\ln \\left[1 + z \\frac{2x}{sin(y)} \\right] \\right)$$\n",
        "\n",
        "we can build a so-called computational graph (shown below) to break the original function to smaller and more approachable expressions. If this \"reverse engineering\" approach seems unintuitive and arbitrary, it's because it is! Usually, the graph is built first.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/ssnio/statics/main/neuromatch/comput_graph.png\" alt=\"Computation Graph\" width=\"800\"/></center>\n",
        "\n",
        "Starting from $x$, $y$, and $z$ and following the arrows and expressions, you would see that our graph returns the same function as $f$. It does so by calculating intermediate variables $a,b,c,d,$ and $e$. This is called the **forward pass**.\n",
        "\n",
        "Now, let’s start from $f$, and work our way against the arrows while calculating the gradient of each expression as we go. This is called **backward pass**, which later inspires **backpropagation of errors**.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/ssnio/statics/main/neuromatch/comput_graph_full.png\" alt=\"Computation Graph full\" width=\"1200\"/></center>\n",
        "\n",
        "Because we've split the computation into simple operations on intermediate variables, I hope you can appreciate how easy it now is to calculate the partial derivatives. \n",
        "\n",
        "Now we can use chain rule and simply calculate any gradient:\n",
        "\n",
        "$$ \\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial e}~\\dfrac{\\partial e}{\\partial d}~\\dfrac{\\partial d}{\\partial c}~\\dfrac{\\partial c}{\\partial a}~\\dfrac{\\partial a}{\\partial x} = \\left( 1-\\tanh^2(e) \\right) \\cdot \\frac{1}{d}\\cdot z \\cdot \\frac{1}{b} \\cdot 2$$\n",
        "\n",
        "Conveniently, the values for $e$, $b$, and $d$ are available to us from when we did the forward pass through the graph. That is, the partial derivatives have simple expressions in terms of the intermediate variables $a,b,c,d,e$ that we calculated and stored during the forward pass.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "w0i_jWJiWPis"
      },
      "source": [
        "### Exercise 1.3\n",
        "For the function above, calculate the $\\dfrac{\\partial f}{\\partial y}$ and $\\dfrac{\\partial f}{\\partial z}$ using the computational graph and chain rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "pe66nTjxWPis"
      },
      "source": [
        "For more: [Calculus on Computational Graphs: Backpropagation](https://colah.github.io/posts/2015-08-Backprop/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "qAZuAdclWPis"
      },
      "source": [
        "---\n",
        "# Section 2: PyTorch AutoGrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "D8Sx57UfWPis"
      },
      "source": [
        "#@title Video 2.1: Automatic Differentiation\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"h8B8Nlcz7yY\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "L1R638eaWPit"
      },
      "source": [
        "Deep learning frameworks such as PyTorch, JAX, and TensorFlow come with a very efficient and sophisticated set of algorithms, commonly known as Automatic differentiation. AutoGrad is PyTorch's automatic differentiation engine. Here we start by covering the essentials of AutoGrad, but you will learn more in the coming days.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "z2xslNr6WPit"
      },
      "source": [
        "## Section 2.1: Forward Propagation\n",
        "\n",
        "Everything starts with the forward propagation (pass). PyTorch plans the computational graph, as we declare the variables and operations, and it builds the graph when we call the backward pass. PyTorch rebuilds the graph every time we iterate or change it (or simply put, PyTorch uses a dynamic graph).\n",
        "\n",
        "Before we start our first example, let's recall gradient descent algorithm. In gradient descent algorithm, it is only required to have the gradient of our cost function with respect to variables which are accessible to us for updating (changing). These variables are often called \"learnable parameters\" or simply parameter in PyTorch. In the case of neural networks, weights and biases are often the learnable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "adhx6M2XWPit"
      },
      "source": [
        "### Exercise 2.1\n",
        "\n",
        "In PyTorch, we can set the optional argument `requires_grad` in tensors to `True`, so PyTorch would track every operation on them while configuring the computational graph. For this exercise, use the provided tensors to build the following graph.\n",
        "\n",
        "<br/>\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/ssnio/statics/main/neuromatch/simple_graph.png\" alt=\"Simple nn graph\" width=\"600\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "p24iKiUdWPit"
      },
      "source": [
        "class SimpleGraph:\n",
        "  def __init__(self, w=None, b=None):\n",
        "    \"\"\"Initializing the SimpleGraph\n",
        "\n",
        "    Args:\n",
        "      w (float): initial value for weight\n",
        "      b (float): initial value for bias\n",
        "    \"\"\"\n",
        "    if w is None:\n",
        "      self.w = torch.randn(1, requires_grad=True)\n",
        "    else:\n",
        "      self.w = torch.tensor([w], requires_grad=True)\n",
        "    if b is None:\n",
        "      self.b = torch.randn(1, requires_grad=True)\n",
        "    else:\n",
        "      self.b = torch.tensor([b], requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward pass\n",
        "\n",
        "    Args:\n",
        "      x (torch.Tensor): 1D tensor of features\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: model predictions\n",
        "    \"\"\"\n",
        "    assert isinstance(x, torch.Tensor)\n",
        "    #################################################\n",
        "    ## Implement the the forward pass to calculate prediction\n",
        "    ## Note that prediction is not the loss, but the value after `tanh`\n",
        "    # Complete the function and remove or comment the line below\n",
        "    raise NotImplementedError(\"Forward Pass `forward`\")\n",
        "    #################################################\n",
        "    prediction = ...\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def sq_loss(y_true, y_prediction):\n",
        "  \"\"\"L2 loss function\n",
        "\n",
        "  Args:\n",
        "    y_true (torch.Tensor): 1D tensor of target labels\n",
        "    y_true (torch.Tensor): 1D tensor of predictions\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor: L2-loss (squared error)\n",
        "  \"\"\"\n",
        "  assert isinstance(y_true, torch.Tensor)\n",
        "  assert isinstance(y_prediction, torch.Tensor)\n",
        "  #################################################\n",
        "  ## Implement the L2-loss (squred error) given true label and prediction\n",
        "  # Complete the function and remove or comment the line below\n",
        "  raise NotImplementedError(\"Loss function `sq_loss`\")\n",
        "  #################################################\n",
        "  loss = ...\n",
        "  return loss\n",
        "\n",
        "\n",
        "# # Uncomment to run\n",
        "# feature = torch.tensor([1])  # input tensor\n",
        "# target = torch.tensor([7])  # target tensor\n",
        "\n",
        "# simple_graph = SimpleGraph(-0.5, 0.5)\n",
        "# print(\"initial weight = {} \\ninitial bias = {}\".format(simple_graph.w.item(),\n",
        "#                                                         simple_graph.b.item()))\n",
        "\n",
        "# prediction = simple_graph.forward(feature)\n",
        "# square_loss = sq_loss(target, prediction)\n",
        "\n",
        "# print(\"for x={} and y={}, prediction={} and L2 Loss = {}\".format(feature.item(),\n",
        "#                                                                  target.item(),\n",
        "#                                                                  prediction.item(),\n",
        "#                                                                  square_loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "DJFh2E6VWPiu"
      },
      "source": [
        "# to_remove solution\n",
        "class SimpleGraph:\n",
        "  def __init__(self, w=None, b=None):\n",
        "    \"\"\"Initializing the SimpleGraph\n",
        "\n",
        "    Args:\n",
        "      w (float): initial value for weight\n",
        "      b (float): initial value for bias\n",
        "    \"\"\"\n",
        "    if w is None:\n",
        "      self.w = torch.randn(1, requires_grad=True)\n",
        "    else:\n",
        "      self.w = torch.tensor([w], requires_grad=True)\n",
        "    if b is None:\n",
        "      self.b = torch.randn(1, requires_grad=True)\n",
        "    else:\n",
        "      self.b = torch.tensor([b], requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward pass\n",
        "\n",
        "    Args:\n",
        "      x (torch.Tensor): 1D tensor of features\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: model predictions\n",
        "    \"\"\"\n",
        "    assert isinstance(x, torch.Tensor)\n",
        "    prediction = torch.tanh(x * self.w + self.b)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def sq_loss(y_true, y_prediction):\n",
        "  \"\"\"L2 loss function\n",
        "\n",
        "  Args:\n",
        "    y_true (torch.Tensor): 1D tensor of target labels\n",
        "    y_true (torch.Tensor): 1D tensor of predictions\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor: L2-loss (squared error)\n",
        "  \"\"\"\n",
        "  assert isinstance(y_true, torch.Tensor)\n",
        "  assert isinstance(y_prediction, torch.Tensor)\n",
        "  loss = (y_true - y_prediction)**2\n",
        "  return loss\n",
        "\n",
        "\n",
        "feature = torch.tensor([1])  # input tensor\n",
        "target = torch.tensor([7])  # target tensor\n",
        "\n",
        "simple_graph = SimpleGraph(-0.5, 0.5)\n",
        "print(\"initial weight = {} \\ninitial bias = {}\".format(simple_graph.w.item(),\n",
        "                                                        simple_graph.b.item()))\n",
        "\n",
        "prediction = simple_graph.forward(feature)\n",
        "square_loss = sq_loss(target, prediction)\n",
        "\n",
        "print(\"for x={} and y={}, prediction={} and L2 Loss = {}\".format(feature.item(),\n",
        "                                                                 target.item(),\n",
        "                                                                 prediction.item(),\n",
        "                                                                 square_loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "X0JAqFzmWPiu"
      },
      "source": [
        "It is important to appreciate the fact that PyTorch can follow our operations as we arbitrary go through classes and functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jsWUIrEFWPiv"
      },
      "source": [
        "## 2.2 Backward Propagation\n",
        "\n",
        "Here is where all the magic lies. We can first look at the operations that PyTorch kept track of. Tensor property `grad_fn` keeps reference to backward propagation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "MjWWtt5gWPiv"
      },
      "source": [
        "print('Gradient function for prediction =', prediction.grad_fn)\n",
        "print('Gradient function for loss =', square_loss.grad_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "RMalcYrKWPiw"
      },
      "source": [
        "Now let's kick off backward pass to calculate the gradients by calling the `.backward()` on the tensor we wish to initiate the backpropagation from. Often, `.backward()` is called on the loss, which is the last on the graph. Before doing that, let's calculate the loss gradients:\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{w}} = - 2 x (y_t - y_p)(1 - y_p^2)$$\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{b}} = - 2 (y_t - y_p)(1 - y_p^2)$$\n",
        "\n",
        "We can then compare it to PyTorch gradients, which can be obtained by calling `.grad` on tensors.\n",
        "\n",
        "**Important Notes**\n",
        "* Always keep in mind that PyTorch is tracking all the operations for tensors that require grad. To stop this tracking, we use `.detach()`.\n",
        "\n",
        "* PyTorch builds the graph only when `.backward()` is called and then it is set free. If you try calling `.backward()` after it is already called, you get the following error:\n",
        "\n",
        "    *`Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.`*\n",
        "\n",
        "* Learnable parameters are \"contagious\". If you recall from our computational graph, we need all the intermediate gradients to be able to use the chain rule. Therefore, we need to `.detach()` any tensor that was on the path of gradient flow (e.g. prediction tensor).\n",
        "\n",
        "* `.backward()` accumulates gradients in the leaves. For most of training methods, we call `.zero_grad()` on the loss or optimizer to zero `.grad` attributes (see [autograd.backward](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward) for more)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "NJ_1a517WPiw"
      },
      "source": [
        "# analytical gradients (remember detaching)\n",
        "ana_dloss_dw = - 2 * feature * (target - prediction.detach())*(1 - prediction.detach()**2)\n",
        "ana_dloss_db = - 2 * (target - prediction.detach())*(1 - prediction.detach()**2)\n",
        "\n",
        "square_loss.backward()  # first we should call the backward to build the graph\n",
        "autograd_dloss_dw = simple_graph.w.grad\n",
        "autograd_dloss_db = simple_graph.b.grad\n",
        "\n",
        "print(ana_dloss_dw == autograd_dloss_dw)\n",
        "print(ana_dloss_db == autograd_dloss_db)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "fDa3NeAqWPiw"
      },
      "source": [
        "References and more:\n",
        "* [A GENTLE INTRODUCTION TO TORCH.AUTOGRAD](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
        "\n",
        "* [AUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRAD](https://pytorch.org/docs/stable/autograd.html)\n",
        "\n",
        "* [AUTOGRAD MECHANICS](https://pytorch.org/docs/stable/notes/autograd.html)\n",
        "\n",
        "* [AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "RImXlGJRWPix"
      },
      "source": [
        "---\n",
        "# Section 3: PyTorch's Neural Net module (`nn.Module`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "dLQcf_ElWPix"
      },
      "source": [
        "#@title Video 3.1: Putting it together\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"rUChBWj9ihw\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hWgW03FPWPix"
      },
      "source": [
        "In this section we will focus on training the simple neural network model from yesterday."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "ud4LHt5PWPix"
      },
      "source": [
        "#@title Generate the sample dataset\n",
        "import sklearn.datasets\n",
        "\n",
        "# Create a dataset of 256 points with a little noise\n",
        "X_orig, y_orig = sklearn.datasets.make_moons(256, noise=0.1)\n",
        "\n",
        "# Plot the dataset\n",
        "plt.figure(figsize=(9, 7))\n",
        "plt.scatter(X_orig[:,0], X_orig[:,1], s=40, c=y_orig)\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$x_2$\")\n",
        "plt.show()\n",
        "\n",
        "# Select the appropriate device (GPU or CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Convert the 2D points to a float tensor\n",
        "X = torch.from_numpy(X_orig).type(torch.FloatTensor)\n",
        "X = X.to(device)\n",
        "\n",
        "# Convert the labels to a long interger tensor\n",
        "y = torch.from_numpy(y_orig).type(torch.LongTensor)\n",
        "y = y.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "c0PJZqo_WPiy"
      },
      "source": [
        "Let's define the same simple neural network model as in Day 1. This time we will not define a `train` method, but instead implement it outside of the class so we can better inspect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "u78InRRXWPiy"
      },
      "source": [
        "# Simple neural network with a single hidden layer\n",
        "class NaiveNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializing the NaiveNet\n",
        "    \"\"\"\n",
        "    super(NaiveNet, self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(2, 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16, 2),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward pass\n",
        "\n",
        "    Args:\n",
        "      x (torch.Tensor): 2D tensor of features\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: model predictions\n",
        "    \"\"\"\n",
        "    return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "84VGj48mWPiy"
      },
      "source": [
        "PyTorch provides us with ready to use neural network building blocks, such as linear or recurrent layers, activation functions and loss functions, packed in the [`torch.nn`](https://pytorch.org/docs/stable/nn.html) module. If we build a neural network using the `torch.nn` layers, the weights and biases are already in `requires_grad` mode. \n",
        "\n",
        "Now let's prepare the training! We need 3 things for that:\n",
        "\n",
        "*   **Model parameters** - Model parameters refer to all the learnable parameters' of the model which are accessible by calling `.parameters()` on the model. Please note that not all the `requires_grad` are seen as model parameters. To create a custom model parameter, you can use [`nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html) (*A kind of Tensor that is to be considered a module parameter*). When we create a new instace of our model, layer parameters are initialized using a uniform distribution (more on that in the coming tutorials and days).\n",
        "\n",
        "*   **Loss function** - we need to define the loss that we are going to be optimizing. The cross entropy loss is suitable for classification problems.\n",
        "\n",
        "*   **Optimizer** - the optimizer will perform the adaptation of the model parameters according to the chosen loss function. The optimizer takes the parameters of the model (often by calling `.parameters()` on the model) as its input to be adapted.\n",
        "\n",
        "You will learn more details about choosing the right loss function and optimizer later in the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "F4k4KiNuWPiy"
      },
      "source": [
        "# Create an instance of our network\n",
        "naive_model = NaiveNet().to(device)\n",
        "\n",
        "# Create a cross entropy loss function\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Stochstic Gradient Descent optimizer with a learning rate of 1e-1\n",
        "learning_rate = 1e-1\n",
        "sgd_optimizer = torch.optim.SGD(naive_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "DpA2IDAMWPiy"
      },
      "source": [
        "The training process in PyTorch is interactive - you can perform training iterations as you wish and inspect the results after each iteration. We encourage leaving the loss function outside the explicit forward pass function, and rather calculate it on the output (prediction).\n",
        "\n",
        "Let's perform one training iteration. You can run the cell multiple times and see how the parameters are being updated and the loss is reducing. We pick the parameters of the first neuron in the first layer. Please make sure you go through all the commands and discuss their purpose with the pod."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "5qJRu1I3WPiy"
      },
      "source": [
        "# Reset all gradients to 0\n",
        "sgd_optimizer.zero_grad()\n",
        "\n",
        "# Forward pass (Compute the output of the model on the data)\n",
        "y_logits = naive_model(X)\n",
        "\n",
        "# Compute the loss\n",
        "loss = cross_entropy_loss(y_logits, y)\n",
        "print('Loss:', loss.item())\n",
        "\n",
        "# Perform backpropagation to build the graph and compute the gradients\n",
        "loss.backward()\n",
        "\n",
        "# `.parameters()` returns a generator\n",
        "print('Gradients:', next(naive_model.parameters()).grad[0].detach().numpy())\n",
        "\n",
        "# Print model's first learnable parameters\n",
        "print('Parameters before:', next(naive_model.parameters())[0].detach().numpy())\n",
        "\n",
        "# Optimizer takes a step in the steepest direction (negative of gradient)\n",
        "# and \"updates\" the weights and biases of the network\n",
        "sgd_optimizer.step()\n",
        "\n",
        "# Print model's first learnable parameters\n",
        "print('Parameters after: ', next(naive_model.parameters())[0].detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JPhApwShWPiz"
      },
      "source": [
        "## Exercise 3\n",
        "Following everything we learned so far, we ask you to complete the `train` function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "RrKI4TSGWPiz"
      },
      "source": [
        "def train(features, labels, model, loss_fun, optimizer, n_epochs):\n",
        "\n",
        "  \"\"\"Training function\n",
        "\n",
        "  Args:\n",
        "    features (torch.Tensor): features (input) with shape torch.Size([n_samples, 2])\n",
        "    labels (torch.Tensor): labels (targets) with shape torch.Size([n_samples, 2])\n",
        "    model (torch nn.Module): the neural network\n",
        "    loss_fun (function): loss function\n",
        "    optimizer(function): optimizer\n",
        "    n_epochs (int): number of training epochs\n",
        "\n",
        "  Returns:\n",
        "    list: record (evolution) of losses\n",
        "    list: record (evolution) of value of the first parameter\n",
        "    list: record (evolution) of gradient of the first parameter\n",
        "  \"\"\"\n",
        "  loss_record = []  # keeping recods of loss\n",
        "  par_values = []  # keeping recods of first parameter\n",
        "  par_grads = []  # keeping recods of gradient of first parameter\n",
        "\n",
        "  # we use `tqdm` methods for progress bar\n",
        "  epoch_range = trange(n_epochs, desc='loss: ', leave=True)\n",
        "  for i in epoch_range:\n",
        "\n",
        "    if loss_record:\n",
        "      epoch_range.set_description(\"loss: {:.4f}\".format(loss_record[-1]))\n",
        "      epoch_range.refresh()  # to show immediately the update\n",
        "      time.sleep(0.01)\n",
        "\n",
        "    #################################################\n",
        "    ## Implement the missing parts of the training loop\n",
        "    # Complete the function and remove or comment the line below\n",
        "    raise NotImplementedError(\"Training setup `train`\")\n",
        "    #################################################\n",
        "    ...  # Initialize gradients to 0\n",
        "    predictions = ...  # Compute model prediction (output)\n",
        "    loss = ...  # Compute the loss\n",
        "    ...  # Compute gradients (backward pass)\n",
        "    ...  # update parameters (optimizer takes a step)\n",
        "\n",
        "    loss_record.append(loss.item())\n",
        "    par_values.append(next(model.parameters())[0][0].item())\n",
        "    par_grads.append(next(model.parameters()).grad[0][0].item())\n",
        "\n",
        "  return loss_record, par_values, par_grads\n",
        "\n",
        "# # Uncomment to run\n",
        "# epochs = 5000\n",
        "# losses, values, gradients = train(X, y,\n",
        "#                                   naive_model,\n",
        "#                                   cross_entropy_loss,\n",
        "#                                   sgd_optimizer,\n",
        "#                                   epochs)\n",
        "\n",
        "# ex3_plot(epochs, losses, values, gradients)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "d5msL7rWWPiz"
      },
      "source": [
        "# to_remove solution\n",
        "def train(features, labels, model, loss_fun, optimizer, n_epochs):\n",
        "\n",
        "  \"\"\"Training function\n",
        "\n",
        "  Args:\n",
        "    features (torch.Tensor): features (input) with shape torch.Size([n_samples, 2])\n",
        "    labels (torch.Tensor): labels (targets) with shape torch.Size([n_samples, 2])\n",
        "    model (torch nn.Module): the neural network\n",
        "    loss_fun (function): loss function\n",
        "    optimizer(function): optimizer\n",
        "    n_epochs (int): number of training epochs\n",
        "\n",
        "  Returns:\n",
        "    list: record (evolution) of losses\n",
        "    list: record (evolution) of value of the first parameter\n",
        "    list: record (evolution) of gradient of the first parameter\n",
        "  \"\"\"\n",
        "  loss_record = []  # keeping recods of loss\n",
        "  par_values = []  # keeping recods of first parameter\n",
        "  par_grads = []  # keeping recods of gradient of first parameter\n",
        "\n",
        "  # we use `tqdm` methods for progress bar\n",
        "  epoch_range = trange(n_epochs, desc='loss: ', leave=True)\n",
        "  for i in epoch_range:\n",
        "\n",
        "    if loss_record:\n",
        "      epoch_range.set_description(\"loss: {:.4f}\".format(loss_record[-1]))\n",
        "      epoch_range.refresh()  # to show immediately the update\n",
        "      time.sleep(0.01)\n",
        "\n",
        "    optimizer.zero_grad()  # Initialize gradients to 0\n",
        "    predictions = model(features)  # Compute model prediction (output)\n",
        "    loss = loss_fun(predictions, labels)  # Compute the loss\n",
        "    loss.backward()  # Compute gradients (backward pass)\n",
        "    optimizer.step()  # update parameters (optimizer takes a step)\n",
        "\n",
        "    loss_record.append(loss.item())\n",
        "    par_values.append(next(model.parameters())[0][0].item())\n",
        "    par_grads.append(next(model.parameters()).grad[0][0].item())\n",
        "\n",
        "  return loss_record, par_values, par_grads\n",
        "\n",
        "\n",
        "epochs = 5000\n",
        "losses, values, gradients = train(X, y,\n",
        "                                  naive_model,\n",
        "                                  cross_entropy_loss,\n",
        "                                  sgd_optimizer,\n",
        "                                  epochs)\n",
        "\n",
        "with plt.xkcd():\n",
        "  ex3_plot(epochs, losses, values, gradients)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "yIjTgYZzWPiz"
      },
      "source": [
        "#@title Video 3.2: Wrap-up\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"zFmWs6doqhM\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}