{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D4_ContinualLearning/W3D4_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Neuromatch Academy: Week 3, Day 4, Tutorial 1\n",
    "# Introduction to Continual Learning\n",
    "\n",
    "__Content creators:__ Keiland Cooper, Diganta Misra, [Vincenzo Lomonaco, Gido van de Ven, Andrea Cossu\n",
    "\n",
    "\n",
    "__Content reviewers:__ Arush Tagade, Jeremy Forest\n",
    "\n",
    "__Content editors:__ Anoop Kulkarni, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Deepak Raya, Spiros Chavlis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "outputId": "54112369-08d3-4b7c-b862-1492c198f9f8"
   },
   "outputs": [],
   "source": [
    "#@markdown Tutorial slides\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://docs.google.com/presentation/d/1PLmrRvwJn2nCeQ26fJGhVAAhOC623bNzRAKzcMpa-iA/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "4f34fcd2-becb-4fa8-bc84-375408a4a4a5"
   },
   "outputs": [],
   "source": [
    "#@title Video 0: Introduction\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"ARVxFIfw4JU\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Tutorial Objectives\n",
    "\n",
    "\n",
    "In this tutorial we'll dive head-first into the exciting field of continual learning (CL). CL has gained increasing attention in recent years, and for good reason. CL is positioned as a problem accross sub-disciplines, from academia and industry, and may promise to be a major pathway towards strong artificial intelligence. As datasets get bigger and AI gets smarter, we're expecting more and more cognitive capabilities from our machines. \n",
    "\n",
    "We have a few specific objectives for this tutorial:\n",
    "*   Introduce major CL concepts\n",
    "*   Introduce the most common strategies to aid CL\n",
    "*   Utilize benchmarks and evaluation metrics \n",
    "*   Explore present day applications of CL \n",
    "\n",
    "\n",
    "_Use a line (---) separator from title block to objectives. You should briefly introduce your content here in a few sentences. In this tutorial, you will learn what a waxed notebook should look like. **You should make sure the notebook runs from start to finish when done waxing (do restart and run all and make sure there are no errors)**_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load in some useful packages and functions. We'll primarly be using PyTorch as our neural network framework of choice. Be sure to run all the cells below so the code runs properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch # should work natively now\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8cf458f8-b663-4572-f7fa-483e8d876b07"
   },
   "outputs": [],
   "source": [
    "#@title Configure PyTorch\n",
    "\n",
    "# PyTorch Configuration\n",
    "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  print('For quicker runtime, add GPU at Runtime -> Change runtime type -> Hardware accelerator -> GPU')\n",
    "\n",
    "else:\n",
    "  # switch to False to use CPU\n",
    "  use_cuda = True\n",
    "\n",
    "  use_cuda = use_cuda and torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\");\n",
    "\n",
    "# set the seed\n",
    "torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "46eec637-b51c-4a6f-e652-af845ddffbe4"
   },
   "outputs": [],
   "source": [
    "#@title Data-loader Helper functions (MNIST and core50)\n",
    "\n",
    "# TODO:\n",
    "# We need a more permenate solution for this...\n",
    "# https://github.com/pytorch/vision/issues/1938\n",
    "\n",
    "# we should also probably supress most of this output\n",
    "# unless we change the source of the data\n",
    "\n",
    "print('Downloading and unpacking MNIST data. Please wait a moment...\\n')\n",
    "\n",
    "# The MNIST repo on LeCun's website nor AWS seem to be availible\n",
    "# This is one popular private repo, but beware\n",
    "# PyTorch is reported to be hosting one soon...\n",
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "mnist_train = MNIST('./', download=False,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                    ]), train=True)\n",
    "mnist_test = MNIST('./', download=False,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                    ]), train=False)\n",
    "\n",
    "def load_mnist(verbose=False, asnumpy=True):\n",
    "    '''\n",
    "    Helper function to maintain compatability with\n",
    "    previous MNIST dataloaders in CLAI COLAB\n",
    "\n",
    "    Much of this can likely now be fixed with the toTensor call on inport\n",
    "    Or by using proper PyTorch functions... lol\n",
    "\n",
    "    - KWC\n",
    "    '''\n",
    "\n",
    "    x_traint, t_traint = mnist_train.data, mnist_train.targets\n",
    "    x_testt, t_testt = mnist_test.data, mnist_test.targets\n",
    "\n",
    "    if asnumpy:\n",
    "      # Fix dimensions and convert back to np array for code compatability\n",
    "      # We aren't using torch dataloaders for ease of use\n",
    "      x_traint = torch.unsqueeze(x_traint, 1)\n",
    "      x_testt = torch.unsqueeze(x_testt, 1)\n",
    "      x_train, x_test = x_traint.numpy().copy(), x_testt.numpy()\n",
    "      t_train, t_test = t_traint.numpy().copy(), t_testt.numpy()\n",
    "    else:\n",
    "      x_train, t_train = x_traint, t_traint\n",
    "      x_test, t_test = x_testt, t_testt\n",
    "\n",
    "    if verbose:\n",
    "      print(\"x_train dim and type: \", x_train.shape, x_train.dtype)\n",
    "      print(\"t_train dim and type: \", t_train.shape, t_train.dtype)\n",
    "      print(\"x_test dim and type: \", x_test.shape, x_test.dtype)\n",
    "      print(\"t_test dim and type: \", t_test.shape, t_test.dtype)\n",
    "      print()\n",
    "\n",
    "\n",
    "    return x_train, t_train, x_test, t_test\n",
    "\n",
    "print('\\nDownloading core50 in the background...')\n",
    "# this lines of code will download core50 in background. We suggest to run it at the start of the notebook\n",
    "!wget http://raw.githubusercontent.com/ContinualAI/colab/master/scripts/download_and_extract_core50_mini.sh\n",
    "!nohup sh download_and_extract_core50_mini.sh &\n",
    "\n",
    "print('Core50 will be silently unpacked in the backgroud. Please continue.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Plotting and Utils Helper functions\n",
    "\n",
    "# If any helper functions you want to hide for clarity, add here\n",
    "# If helper code depends on libraries that aren't used elsewhere,\n",
    "# import those libaries here, rather than in the main import cell\n",
    "\n",
    "\n",
    "def plot_mnist(data, nPlots=10):\n",
    "  \"\"\" Plot MNIST-like data \"\"\"\n",
    "  f, axarr = plt.subplots(1,nPlots)\n",
    "  for ii in range(nPlots):\n",
    "    axarr[ii].imshow(data[ii,0], cmap=\"gray\")\n",
    "  np.vectorize(lambda ax:ax.axis('off'))(axarr);\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def permute_mnist(mnist, seed, verbose=False):\n",
    "    \"\"\" Given the training set, permute pixels of each img the same way. \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    if verbose: print(\"starting permutation...\")\n",
    "    h = w = 28\n",
    "    perm_inds = list(range(h*w))\n",
    "    np.random.shuffle(perm_inds)\n",
    "    # print(perm_inds)\n",
    "    perm_mnist = []\n",
    "    for set in mnist:\n",
    "        num_img = set.shape[0]\n",
    "        flat_set = set.reshape(num_img, w * h)\n",
    "        perm_mnist.append(flat_set[:, perm_inds].reshape(num_img, 1, w, h))\n",
    "    if verbose: print(\"done.\")\n",
    "    return perm_mnist\n",
    "\n",
    "\n",
    "def multi_task_barplot(accs, tasks, t=None):\n",
    "  ''' Plot n task accuracy\n",
    "      used for S1 intro to CF code '''\n",
    "  nTasks = len(accs)\n",
    "  plt.bar(range(nTasks), accs, color='k')\n",
    "  plt.ylabel('Testing Accuracy (%)', size=18)\n",
    "  plt.xticks(range(nTasks),\n",
    "            [f'{TN}\\nTask {ii+1}' for ii,TN in enumerate(tasks.keys())],\n",
    "            size=18)\n",
    "  plt.title(t)\n",
    "\n",
    "\n",
    "def plot_task(axs, data, samples_num):\n",
    "  for sample in range(samples_num):\n",
    "    axs[sample].imshow(data[sample][0], cmap=\"gray\")\n",
    "    # np.vectorize(lambda ax:ax.axis('off'))(axs[sample]);\n",
    "    axs[sample].xaxis.set_ticks([])\n",
    "    axs[sample].yaxis.set_ticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: The sequential learning problem: catastrophic forgetting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "5647c299-5ff3-4699-fcf2-c0f0851eb25c"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Catastrophic forgetting\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"WIbgFxzaFP4\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll explore catastrophic forgetting first hand, a key barrier preventing continual learning in neural networks. To do so, we'll build a simple network model and try our best to teach it the trusty MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: A brief example of catastrophic forgetting \n",
    "\n",
    "Let's define a simple CNN that can perform fairly well on MNIST. We'll also load in some training and testing functions we wrote to load the data into the model and train / test it. We don't need to get into the details how they work for now (pretty standard) but feel free to double click the cell if you're curious!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title [RUN ME!] Model Training and Testing Functions\n",
    "def train(model, device, x_train, t_train, optimizer, epoch):\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "  model.train()\n",
    "\n",
    "  for start in range(0, len(t_train)-1, 256):\n",
    "    end = start + 256\n",
    "    x = torch.from_numpy(x_train[start:end]).type(torch.cuda.FloatTensor)\n",
    "    y = torch.from_numpy(t_train[start:end]).long()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(x)\n",
    "    loss = F.cross_entropy(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(loss.item())\n",
    "  print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, x_test, t_test):\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  correct, test_loss = 0, 0\n",
    "  for start in range(0, len(t_test)-1, 256):\n",
    "    end = start + 256\n",
    "    with torch.no_grad():\n",
    "      x = torch.from_numpy(x_test[start:end]).type(torch.cuda.FloatTensor)\n",
    "      y = torch.from_numpy(t_test[start:end]).long()\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      output = model(x)\n",
    "      test_loss += F.cross_entropy(output, y).item() # sum up batch loss\n",
    "      pred = output.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "      correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "  test_loss /= len(t_train)\n",
    "  print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(t_test),\n",
    "      100. * correct / len(t_test)))\n",
    "  return 100. * correct / len(t_test)\n",
    "\n",
    "\n",
    "class simpNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(simpNet,self).__init__()\n",
    "    self.linear1 = nn.Linear(28*28, 320)\n",
    "    self.out = nn.Linear(320, 10)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, img):\n",
    "    x = img.view(-1, 28*28)\n",
    "    x = self.relu(self.linear1(x))\n",
    "    x = self.out(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a simple multilayer CNN. Nothing too fancy\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "    self.conv2_drop = nn.Dropout2d()\n",
    "    self.fc1 = nn.Linear(320, 50)\n",
    "    self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" run the network forward\n",
    "    (uses the functional library (F) imported from pytorch)\"\"\"\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "    x = x.view(-1, 320)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load in our dataset, MNIST. We'll also run a function we defined in the helper function cell above that permutes (scrambles) the images. This allows us to create aditional datasets with similar statictics to MNIST on the fly. We'll call the normal MNIST Task 1, and the permuted MNIST Task 2. We'll see why in a second!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "outputId": "271a1f0a-6d75-4a3d-91e2-a18241fde5bf"
   },
   "outputs": [],
   "source": [
    "# Load in MNIST and create an additional permuted dataset\n",
    "x_train, t_train, x_test, t_test = load_mnist(verbose=True)\n",
    "x_train2, x_test2 = permute_mnist([x_train, x_test], 0, verbose=False)\n",
    "\n",
    "# Plot the data to see what we're working with\n",
    "print('Task 1: MNIST Training data:')\n",
    "plot_mnist(x_train, nPlots=10)\n",
    "print('\\nTask 2: Permuted MNIST data:')\n",
    "plot_mnist(x_train2, nPlots=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have our data. Now, let's initialize and train our model on the standard MNIST dataset (Task 1) and make sure everything is working properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3eee92e0-8729-4abc-998d-a512d3e7d8a5"
   },
   "outputs": [],
   "source": [
    "# Define a new model and set params\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the model on MNIST\n",
    "nEpochs = 3\n",
    "print(f'Training model on {nEpochs} epochs...')\n",
    "for epoch in range(1, nEpochs+1):\n",
    "  train(model, device, x_train, t_train, optimizer, epoch)\n",
    "  test(model, device, x_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay great! It seems we get decent accuracy on standard MNIST which means the model is learning our dataset. Now, a reasonable assumption is that, like humans, once the network learns something, it can aggregate its knowledge and learn something else. \n",
    "\n",
    "First, let's get a baseline for how the model performs on the dataset it was just trained on (Task 1) as well as to see how well it performs on a new dataset (Task 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "outputId": "41705858-e396-49ef-fb39-b835a9a1ed92"
   },
   "outputs": [],
   "source": [
    "# test the model's accuracy on both the regular and permuted dataset\n",
    "\n",
    "# Let's define a dictionary that holds each of the task\n",
    "# datasets and labels\n",
    "tasks = {'MNIST':(x_test, t_test),\n",
    "         'Perm MNIST':(x_test2, t_test)}\n",
    "t1_accs = []\n",
    "for ti, task in enumerate(tasks.keys()):\n",
    "  print(f\"Testing on task {ti+1}\")\n",
    "  t1_accs.append(test(model, device, tasks[task][0], tasks[task][1]))\n",
    "\n",
    "# And then let's plot the testing accuracy on both datasets\n",
    "\n",
    "multi_task_barplot(t1_accs, tasks,\n",
    "                   t='Accuracy after training on Task 1 \\nbut before Training on Task 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, the model does great on the Task 1 dataset it was trained on, but not so well on the new one. No worries! We havn't taught it the permuted MNIST dataset yet! So let's train the *same* task 1-trained-model on the new data, and see if we can get comparable performance between the two types of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "outputId": "e1bb5e56-1c93-481d-a7a4-2d7c95a6d98e"
   },
   "outputs": [],
   "source": [
    "# Train the previously trained model on Task 2, the permuted MNIST dataset\n",
    "for epoch in range(1, 3):\n",
    "  train(model, device, x_train2, t_train, optimizer, epoch)\n",
    "  test(model, device, x_test2, t_test)\n",
    "\n",
    "# Same data as before, stored in a dict\n",
    "tasks = {'MNIST':(x_test, t_test),\n",
    "         'Perm MNIST':(x_test2, t_test)}\n",
    "# Test the model on both datasets, same as before\n",
    "t12_accs = []\n",
    "for ti, task in enumerate(tasks.keys()):\n",
    "  print(f\"Testing on task {ti+1}\")\n",
    "  t12_accs.append(test(model, device, tasks[task][0], tasks[task][1]))\n",
    "\n",
    "# And then let's plot each of the testing accuracies after the new training\n",
    "multi_task_barplot(t12_accs, tasks,\n",
    "                   t='Accuracy after training on Task 1 \\nand *AFTER* Training on Task 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey! Training did the trick, task 2 (permuted MNIST) has great accuracy now that we trained the model on it. But something is wrong. We just saw that Task 1 (standard MNIST) had high accuracy before we trained on the new task. What gives? Try to incorperate what you learned in the lecture to help explain the problem we're seeing. You might also take a few seconds and think of what possible soultions you might like to try. In the next section, we'll look into exactly that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Continual Learning strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "020b1485-cabf-4fc9-d2fd-4ea3e83cdc86"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Strategies\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"IJL3FNxrOaE\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: Split MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will again use the MNIST dataset, but we will now create 5 tasks by splitting the dataset up in such a way that each task contains 2 classes. This problem is called Split MNIST, and it is popular toy problem in the continual learning literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "outputId": "715e41b3-6daa-402d-d263-94b3eb2ffbfd"
   },
   "outputs": [],
   "source": [
    "# Specify which classes should be part of which task\n",
    "task_classes_arr = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9)]\n",
    "tasks_num = len(task_classes_arr) # 5\n",
    "\n",
    "# Divide the data over the different tasks\n",
    "task_data_with_overlap = []\n",
    "for task_id, task_classes in enumerate(task_classes_arr):\n",
    "  train_mask = np.isin(t_train, task_classes)\n",
    "  test_mask = np.isin(t_test, task_classes)\n",
    "  x_train_task, t_train_task = x_train[train_mask], t_train[train_mask]\n",
    "  x_test_task, t_test_task = x_test[test_mask], t_test[test_mask]\n",
    "  # Convert the original class labels (i.e., the digits 0 to 9) to\n",
    "  # \"within-task labels\" so that within each task one of the digits is labelled\n",
    "  # as '0' and the other as '1'.\n",
    "  task_data_with_overlap.append((x_train_task, t_train_task - (task_id * 2),\n",
    "                                 x_test_task, t_test_task - (task_id * 2)))\n",
    "\n",
    "# Display tasks\n",
    "n_tasks, samples = 5, 5\n",
    "_, axs = plt.subplots(n_tasks, samples, figsize=(5, 5))\n",
    "for task in range(n_tasks):\n",
    "  axs[task, 0].set_ylabel(f'Task {task}', rotation=0)\n",
    "  axs[task, 0].yaxis.set_label_coords(-0.5,1)\n",
    "  plot_task(axs[task], task_data_with_overlap[task][0], samples)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Naive strategy (\"fine-tuning\")\n",
    "First, let's see what happens if we simply sequentially train a deep neural network on these tasks in the standard way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining our network. As is common in the continual learning literature, we will use a \"multi-headed layout\". This means that we have a separate output layer for each task to be learned, but the hidden layers of the network are shared between all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base network that is shared between all tasks\n",
    "class FBaseNet(nn.Module):\n",
    "  def __init__(self, hsize=512):\n",
    "    super(FBaseNet, self).__init__()\n",
    "    self.l1 = nn.Linear(784, hsize)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = F.relu(self.l1(x))\n",
    "    return x\n",
    "\n",
    "## Output layer, which will be separate for each task\n",
    "class FHeadNet(nn.Module):\n",
    "  def __init__(self, base_net, input_size=512):\n",
    "    super(FHeadNet, self).__init__()\n",
    "\n",
    "    self.base_net = base_net\n",
    "    self.output_layer = nn.Linear(input_size, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.base_net.forward(x)\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2581a00a-a3a3-4a4e-9a55-9b05e950584e"
   },
   "outputs": [],
   "source": [
    "# Define the base network (a new head is defined when we encounter a new task)\n",
    "base = FBaseNet().to(device)\n",
    "heads = []\n",
    "\n",
    "# Define a list to store test accuracies for each task\n",
    "accs_naive = []\n",
    "\n",
    "# Set the number of epochs to train each task for\n",
    "epochs = 3\n",
    "\n",
    "# Loop through all tasks\n",
    "for task_id in range(tasks_num):\n",
    "  # Collect the training data for the new task\n",
    "  x_train, t_train, _, _ = task_data_with_overlap[task_id]\n",
    "\n",
    "  # Define a new head for this task\n",
    "  model = FHeadNet(base).to(device)\n",
    "  heads.append(model)\n",
    "\n",
    "  # Set the optimizer\n",
    "  optimizer = optim.SGD(heads[task_id].parameters(), lr=0.01)\n",
    "\n",
    "  # Train the model (with the new head) on the current task\n",
    "  train(heads[task_id], device, x_train, t_train, optimizer, epochs)\n",
    "\n",
    "  # Test the model on all tasks seen so far\n",
    "  accs_subset = []\n",
    "  for i in range(0, task_id + 1):\n",
    "    _, _, x_test, t_test = task_data_with_overlap[i]\n",
    "    test_acc = test(heads[i], device, x_test, t_test)\n",
    "    accs_subset.append(test_acc)\n",
    "  # For unseen tasks, we don't test\n",
    "  if task_id < (tasks_num - 1):\n",
    "    accs_subset.extend([np.nan] * (4 - task_id))\n",
    "  # Collect all test accuracies\n",
    "  accs_naive.append(accs_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, whenever this network is trained on a new task, its performance on previously learned tasks drops substantially.\n",
    "\n",
    "Now, let's see whether we can use a continual learning strategy to prevent such forgetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3: Elastic Weight Consolidation (EWC)\n",
    "\n",
    "EWC is a popular CL strategy which involves computing the importance of weights of the network relative to the task using the Fisher score and then penalizing the network for changes to the most important weights of the previous task. \n",
    "\n",
    "It was introduced in the paper \"[Overcoming catastrophic forgetting in neural networks\n",
    "](https://arxiv.org/abs/1612.00796)\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EWC, we need to define a new function to compute the fisher information matrix for each weight at the end of every task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_task_update(task_id, x_mem, t_mem, model, shared_model, fisher_dict,\n",
    "                   optpar_dict):\n",
    "\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # accumulating gradients\n",
    "  for start in range(0, len(t_mem)-1, 256):\n",
    "    end = start + 256\n",
    "    x = torch.from_numpy(x_train[start:end]).type(torch.cuda.FloatTensor)\n",
    "    y = torch.from_numpy(t_train[start:end]).long()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    output = model(x)\n",
    "    loss = F.cross_entropy(output, y)\n",
    "    loss.backward()\n",
    "\n",
    "  fisher_dict[task_id] = {}\n",
    "  optpar_dict[task_id] = {}\n",
    "\n",
    "  # gradients accumulated can be used to calculate fisher\n",
    "  for name, param in shared_model.named_parameters():\n",
    "    optpar_dict[task_id][name] = param.data.clone()\n",
    "    fisher_dict[task_id][name] = param.grad.data.clone().pow(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to modify our train function to add the new regularization loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ewc(model, shared_model, device, task_id, x_train, t_train, optimizer,\n",
    "              epoch, ewc_lambda, fisher_dict, optpar_dict):\n",
    "    model.train()\n",
    "\n",
    "    for start in range(0, len(t_train)-1, 256):\n",
    "      end = start + 256\n",
    "      x = torch.from_numpy(x_train[start:end]).type(torch.cuda.FloatTensor)\n",
    "      y = torch.from_numpy(t_train[start:end]).long()\n",
    "      x, y = x.to(device), y.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      output = model(x)\n",
    "      loss = F.cross_entropy(output, y)\n",
    "\n",
    "      ### magic here! :-)\n",
    "      for task in range(task_id):\n",
    "        for name, param in shared_model.named_parameters():\n",
    "          fisher = fisher_dict[task][name]\n",
    "          optpar = optpar_dict[task][name]\n",
    "          loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train with EWC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "de2bbde1-56c4-4618-8b30-1f111e39f2b2"
   },
   "outputs": [],
   "source": [
    "# Define the base network (a new head is defined when we encounter a new task)\n",
    "base = FBaseNet().to(device)\n",
    "heads = []\n",
    "\n",
    "# Define a list to store test accuracies for each task\n",
    "accs_ewc = []\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 2\n",
    "\n",
    "# Set EWC hyperparameter\n",
    "ewc_lambda = 0.4\n",
    "\n",
    "# Define dictionaries to store values needed by EWC\n",
    "fisher_dict = {}\n",
    "optpar_dict = {}\n",
    "\n",
    "# Loop through all tasks\n",
    "for task_id in range(tasks_num):\n",
    "  # Collect the training data for the new task\n",
    "  x_train, t_train, _, _ = task_data_with_overlap[task_id]\n",
    "\n",
    "  # Define a new head for this task\n",
    "  model = FHeadNet(base).to(device)\n",
    "  heads.append(model)\n",
    "\n",
    "  # Set the optimizer\n",
    "  optimizer = optim.SGD(heads[task_id].parameters(), lr=0.01)\n",
    "\n",
    "  # Train the model (with the new head) on the current task\n",
    "  for epoch in range(1, epochs+1):\n",
    "      train_ewc(heads[task_id], heads[task_id].base_net, device, task_id,\n",
    "                x_train, t_train, optimizer, epoch, ewc_lambda, fisher_dict,\n",
    "                optpar_dict)\n",
    "  on_task_update(task_id, x_train, t_train, heads[task_id],\n",
    "                  heads[task_id].base_net, fisher_dict, optpar_dict)\n",
    "\n",
    "  # Test the model on all tasks seen so far\n",
    "  accs_subset = []\n",
    "  for i in range(0, task_id + 1):\n",
    "    _, _, x_test, t_test = task_data_with_overlap[i]\n",
    "    test_acc = test(heads[i], device, x_test, t_test)\n",
    "    accs_subset.append(test_acc)\n",
    "  # For unseen tasks, we don't test\n",
    "  if task_id < (tasks_num - 1):\n",
    "    accs_subset.extend([np.nan] * (4 - task_id))\n",
    "  # Collect all test accuracies\n",
    "  accs_ewc.append(accs_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "outputId": "9daafa63-eed9-485d-ed06-6107904cff36"
   },
   "outputs": [],
   "source": [
    "#@title Plot Naive vs EWC results\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "accs_fine_grid = np.array(accs_naive)\n",
    "nan_mask = np.isnan(accs_naive)\n",
    "\n",
    "sns.heatmap(accs_naive, vmin=0, vmax=100, mask=nan_mask, annot=True,fmt='.0f',\n",
    "            yticklabels=range(1, 6), xticklabels=range(1, 6), ax=axes[0], cbar=False)\n",
    "sns.heatmap(accs_ewc, vmin=0, vmax=100, mask=nan_mask, annot=True,fmt='.0f',\n",
    "            yticklabels=range(1, 6), xticklabels=range(1, 6), ax=axes[1], cbar=False)\n",
    "\n",
    "axes[0].set_ylabel('Tested on Task')\n",
    "\n",
    "axes[0].set_xlabel('Naive')\n",
    "axes[1].set_xlabel('EWC')\n",
    "\n",
    "axes[2].plot(range(1, 6), np.nanmean(accs_naive, axis=1))\n",
    "axes[2].plot(range(1, 6), np.nanmean(accs_ewc, axis=1))\n",
    "\n",
    "axes[2].legend(['Naive', 'EWC'])\n",
    "axes[2].set_ylabel('Accumulated Accuracy for Seen Tasks')\n",
    "axes[2].set_xlabel('Task Number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Continual learning benchmarks\n",
    "\n",
    "In this section, we will introduce different ways in which a continual learning problem could be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "63314845-e78d-44d1-d521-52d272d47b24"
   },
   "outputs": [],
   "source": [
    "#@title Video 3: Benchmarks and different types of continual learning\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"2MiMLmtXp-Q\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As introduced in the above video, continual learning research certainly does not only use the MNIST dataset.\n",
    "But to make things not more complicated than necessary (and to make sure the examples run in an acceptable amount of time), we continue with the Split MNIST example for now.\n",
    "At the end of this notebook we will take a sneak peak at the CORe50 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Task-incremental Split MNIST *versus* class-incremental Split MNIST\n",
    "\n",
    "First, let's identify according to which scenario the Split MNIST problem in the previous section was performed.\n",
    "\n",
    "Recall that the Split MNIST problem consists of five tasks, whereby each task contains two digits. In the previous section, the model was set-up in such a way that it had a separate output layer for each of these tasks (this is typically called a 'multi-headed output layer'). At test time, the model then used the output layer of the task to which the example to be classified belonged. This means that it was assumed that the model always knows which task it must performed, so this was an example of **task-incremental learning**.\n",
    "\n",
    "In the continual learning literature, a multi-headed output layer is probably the most common way to use task identity information, but it is certainly not the only way (for example, see [this paper](https://doi.org/10.1073/pnas.1803839115))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reorganize the above Split MNIST problem to set it up as a **class-incremental learning** problem. That is, task information is no longer provided to the model; the model must be able to decide itself to which task a test sample belongs.\n",
    "This means that, after all tasks have been learned, the model must now choose between all ten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d25f894f-3147-4caa-f3ce-c7f91003b59f"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "x_train, t_train, x_test, t_test = load_mnist(verbose=True)\n",
    "\n",
    "# Define which classes are part of each task\n",
    "classes_per_task = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9)]\n",
    "\n",
    "# Divde the MNIST dataset in tasks\n",
    "task_data = []\n",
    "for _, classes_in_this_task in enumerate(classes_per_task):\n",
    "\n",
    "  # Which data-points belong to the classes in the current task?\n",
    "  train_mask = np.isin(t_train, classes_in_this_task)\n",
    "  test_mask = np.isin(t_test, classes_in_this_task)\n",
    "  x_train_task, t_train_task = x_train[train_mask], t_train[train_mask]\n",
    "  x_test_task, t_test_task = x_test[test_mask], t_test[test_mask]\n",
    "\n",
    "  # Add the data for the current task\n",
    "  task_data.append((x_train_task, t_train_task, x_test_task, t_test_task))\n",
    "\n",
    "# In contrast to the task-incremental version of Split MNIST explored in the\n",
    "# last section, now task identity information will not be provided to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: EWC on the class-incremental version of Split MNIST\n",
    "\n",
    "Let's now try the EWC method on this class-incremental version of Split MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "62bc0141-405b-4bd1-f27c-45e2f7846c50"
   },
   "outputs": [],
   "source": [
    "# Define the model and the optimzer\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set 'lambda', the hyperparameter of EWC\n",
    "ewc_lambda = 0.4\n",
    "\n",
    "# Define dictionaries to store values needed by EWC\n",
    "fisher_dict = {}\n",
    "optpar_dict = {}\n",
    "\n",
    "# Prepare list to store average accuracies after each task\n",
    "ewc_accs = []\n",
    "\n",
    "# Loop through all tasks\n",
    "for id, task in enumerate(task_data):\n",
    "\n",
    "  # Collect training data\n",
    "  x_train, t_train, _, _ = task\n",
    "\n",
    "  # Training with EWC\n",
    "  print(\"Training on task: \", id)\n",
    "  for epoch in range(1, 2):\n",
    "    train_ewc(model, model, device, id, x_train, t_train, optimizer, epoch,\n",
    "              ewc_lambda, fisher_dict, optpar_dict)\n",
    "  on_task_update(id, x_train, t_train, model, model, fisher_dict,\n",
    "                 optpar_dict)\n",
    "\n",
    "  # Evaluate performance after training on this task\n",
    "  avg_acc = 0\n",
    "  for id_test, task in enumerate(task_data):\n",
    "    print(\"Testing on task: \", id_test)\n",
    "    _, _, x_test, t_test = task\n",
    "    acc = test(model, device, x_test, t_test)\n",
    "    avg_acc = avg_acc + acc\n",
    "\n",
    "  print(\"Avg acc: \", avg_acc / len(task_data))\n",
    "  ewc_accs.append(avg_acc / len(task_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model only correctly predicts the classes from the last task it has seen, all earlier seen classes seem to be forgotten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder whether the reason that EWC performed so badly in the above example is because we chose an unsuitable value for the hyperparameter lambda.\n",
    "Although we don't have time to demonstrate this, there are no values of lambda that would lead to good performance.\n",
    "\n",
    "In general, parameter regularization based methods, such as EWC, have been found not to work well on class-incremental learning problems, as for example illustrated in [this paper](https://arxiv.org/abs/1904.07734)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2: Replay\n",
    "\n",
    "As dicsussed in the lecture of the previous section, another popular continual learning strategy is replay. Let's see whether replay works better on the class-incremental learning version of Split MNIST than EWC did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One implementation of replay is to simply store all data from previously seen tasks, and to then, whenever a new task must be learned, mix in that stored data with the training data of the new task.\n",
    "\n",
    "To achieve this form of replay, let's define the following function for shuffling multiple datasets (e.g., the data from previous tasks with the data from the current task) together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(dataset, seed, in_place=False):\n",
    "  \"\"\" Shuffle two (or more) list in unison. \"\"\"\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  rng_state = np.random.get_state()\n",
    "  new_dataset = []\n",
    "  for x in dataset:\n",
    "    if in_place:\n",
    "      np.random.shuffle(x)\n",
    "    else:\n",
    "      new_dataset.append(np.random.permutation(x))\n",
    "    np.random.set_state(rng_state)\n",
    "\n",
    "  if not in_place:\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this form of replay is somewhat extreme, as it stores all the training data from previous tasks. In practice, replay is often implemented in ways that store less data, for example either by using relatively small memory buffers (see [this paper](https://arxiv.org/abs/1902.10486)) or by learning a generative model to then generate the data to be replayed (see [this paper](https://arxiv.org/abs/1705.08690) or [this paper](https://www.nature.com/articles/s41467-020-17866-2))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Test replay on the class-incremental version of Split MNIST\n",
    "\n",
    "Let's try whether this replay strategy works better than EWC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6897a1ac-c7d2-4980-e181-4fc8b16130e6"
   },
   "outputs": [],
   "source": [
    "# Define the model and the optimizer\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare list to store average accuracies after each task\n",
    "rehe_accs = []\n",
    "\n",
    "# Loop through all tasks\n",
    "for id, task in enumerate(task_data):\n",
    "\n",
    "  # Collect training data\n",
    "  x_train, t_train, _, _ = task\n",
    "\n",
    "  # Add replay\n",
    "  for i in range(id):\n",
    "    past_x_train, past_t_train, _, _ = task_data[i]\n",
    "    x_train = np.concatenate((x_train, past_x_train))\n",
    "    t_train = np.concatenate((t_train, past_t_train))\n",
    "  x_train, t_train = shuffle_in_unison([x_train, t_train], 0)\n",
    "\n",
    "  # Training\n",
    "  print(\"Training on task: \", id)\n",
    "  for epoch in range(1, 3):\n",
    "    train(model, device, x_train, t_train, optimizer, epoch)\n",
    "\n",
    "  # Evaluate performance after training on this task\n",
    "  avg_acc = 0\n",
    "  for id_test, task in enumerate(task_data):\n",
    "    print(\"Testing on task: \", id_test)\n",
    "    _, _, x_test, t_test = task\n",
    "    acc = test(model, device, x_test, t_test)\n",
    "    avg_acc = avg_acc + acc\n",
    "\n",
    "  print(\"Avg acc: \", avg_acc / len(task_data))\n",
    "  rehe_accs.append(avg_acc/len(task_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's compare the performance of EWC and Replay on the class-incremental version of Split MNIST in a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "outputId": "481e0733-0d4e-4531-eeb9-b779db0a2a29"
   },
   "outputs": [],
   "source": [
    "#@title Plot EWC vs replay\n",
    "plt.plot([1, 2, 3, 4, 5], rehe_accs, '-o', label=\"Rehearsal\")\n",
    "plt.plot([1, 2, 3, 4, 5], ewc_accs, '-o', label=\"EWC\")\n",
    "plt.xlabel('Tasks Encountered', fontsize=14)\n",
    "plt.ylabel('Average Accuracy', fontsize=14)\n",
    "plt.title('CL Strategies on Class-incremental version of Split MNIST',\n",
    "          fontsize=14);\n",
    "plt.xticks([1, 2, 3, 4, 5])\n",
    "plt.legend(prop={'size': 16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: Identify the scenario of the permuted MNIST example from Section 1\n",
    "\n",
    "What type of 'scenario' was the permuted MNIST problem that was introduced in Section 1? Was it task-incremental, domain-incremental or class-incremental? Try to motivate your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "'''\n",
    "The Permuted MNIST problem in Section 1 is an example of domain-incremental\n",
    "learning.\n",
    "\n",
    "Recall that this problem consisted of two tasks: normal MNIST (task 1) and MNIST\n",
    "with permuted input images (task 2).\n",
    "After learning both task, when the model is evaluated, the model is not told to\n",
    "which task an image belongs (i.e., the model is not told whether the image be\n",
    "classified is permuted or not), but the model also does not need to identify to\n",
    "which task an image belongs (i.e., the model does not need to predict whether\n",
    "the image to be classified has permuted pixels or not; it only needs to predict\n",
    "the original digit displayed in the image).\n",
    "\n",
    "Another way to motivate that this problem is an example of domain-incremental\n",
    "learning, is to say that in both task 1 (normal MNIST) and task 2 (MNIST with\n",
    "permuted input images), the 'type of problem' is the same (i.e., identify the\n",
    "digit displayed in the original image), but the 'context' is changing (i.e.,\n",
    "the order in which the image pixels are presented).\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Evaluation of continual learning algorithms\n",
    "\n",
    "Understanding how your CL algorithm is performing is key to gain insights on its behavior and to decide how to improve it. \n",
    "\n",
    "Here, we will focus on how to build some of the most important CL metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "ed0505c5-55f1-404c-cbe9-02eb906bc859"
   },
   "outputs": [],
   "source": [
    "#@title Video 4: Continual Learning Evaluation\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"tR-5zraPOto\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already trained the model on T tasks and recorded all the accuracy values in a single TxT matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Accuracy\n",
    "\n",
    "The Average Accuracy (ACC) metric computes the average accuracy over all tasks after training on all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACC(result_matrix):\n",
    "  \"\"\"\n",
    "  Average Accuracy metric\n",
    "\n",
    "  :param result_matrix: TxT matrix containing accuracy values in each (i, j) entry.\n",
    "    (i, j) -> test accuracy on task j after training on task i\n",
    "  \"\"\"\n",
    "\n",
    "  final_accs = result_matrix[-1, :]  # take accuracies after final training\n",
    "  acc = np.mean(final_accs)  # compute average\n",
    "  return acc, final_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Transfer\n",
    "\n",
    "The Backward Transfer (BWT) metric of task i computes the accuracy on task i after training on last task **minus** the accuracy on task i after training on task i.\n",
    "\n",
    "To get the average BWT you have to average across all tasks.\n",
    "\n",
    "**Negative BWT expresses the amount of forgetting suffered by the algorithm.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BWT(result_matrix):\n",
    "  \"\"\"\n",
    "  Backward Transfer metric\n",
    "\n",
    "  :param result_matrix: TxT matrix containing accuracy values in each (i, j) entry.\n",
    "    (i, j) -> test accuracy on task j after training on task i\n",
    "  \"\"\"\n",
    "\n",
    "  final_accs = result_matrix[-1, :]  # take accuracies after final training\n",
    "  # accuracies on task i right after training on task i, for all i\n",
    "  training_accs = np.diag(result_matrix)\n",
    "  task_bwt = final_accs - training_accs  # BWT for each task\n",
    "  average_bwt = np.mean(task_bwt)  # compute average\n",
    "  return average_bwt, task_bwt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 4.1: evaluate your CL algorithm on a benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should replace the `...` with your code. This is the only cell you have to modify :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "  \"\"\"Train a model with a CL algorithm of your choice on a chosen CL benchmark.\n",
    "     The benchmark will have T tasks.\n",
    "\n",
    "  Returns:\n",
    "    result_matrix: TxT matrix of accuracies. Each (i,j) element is the accuracy\n",
    "        on task j after training on task i\n",
    "  \"\"\"\n",
    "\n",
    "  model = Net().to(device)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  #################################################\n",
    "  # Fill in missing code below (...),\n",
    "  # then remove or comment the line below to test your function\n",
    "  raise NotImplementedError(\"You should train a model on Split MNIST with replay\"\n",
    "                             \"and multi-head. You should return the matrix of accuracies\"\n",
    "                             \"of all tasks after training on each task.\")\n",
    "  #################################################\n",
    "  benchmark = ...  # define your CL benchmark\n",
    "\n",
    "  # train the model on the benchmark with the strategy and get the result\n",
    "  result_matrix = train_multihead(past_examples_percentage=..., epochs=...)\n",
    "\n",
    "  return np.array(result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "def train_model():\n",
    "  \"\"\"Train a model with a CL algorithm of your choice on a chosen CL benchmark.\n",
    "     The benchmark will have T tasks.\n",
    "\n",
    "  Returns:\n",
    "    result_matrix: TxT matrix of accuracies. Each (i,j) element is the accuracy\n",
    "        on task j after training on task i\n",
    "  \"\"\"\n",
    "\n",
    "  model = Net().to(device)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  benchmark = None  # define your CL benchmark\n",
    "\n",
    "  # train the model on the benchmark with the strategy and get the result\n",
    "  result_matrix = train_multihead(past_examples_percentage=0.5, epochs=10)\n",
    "\n",
    "  return np.array(result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "5a04768e-d151-4a28-ff2b-5975b7a723ab"
   },
   "outputs": [],
   "source": [
    "#to_remove explanation\n",
    "\"\"\"\n",
    "As we discussed, the number of metrics you can evaluate is very large. To keep things compact,\n",
    "we only focus on 2 performance metrics. You can have fun and implement the forward transfer :)\n",
    "\n",
    "Result matrix have nan values in correspondence of future tasks since we do not evaluate our\n",
    "model on future tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You **don't** need to modify the next cell, just execute it to see metrics in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here compute the result matrix containing all the accuracy values on a strategy\n",
    "# and CL benchmark of your choice.\n",
    "\n",
    "# Give an Error -- Uncomment when `train_multihead` added in the notebook\n",
    "\n",
    "# result_matrix = train_model()\n",
    "\n",
    "#if result_matrix is None or n_tasks is None:\n",
    "#  raise ValueError(\"You should fill the values of `result_matrix`, `n_tasks` and `random_acc` first.\")\n",
    "\n",
    "#print(\"Result matrix shape: \", result_matrix.shape)\n",
    "#print(\"Result matrix values: \", result_matrix)\n",
    "\n",
    "### print Average Accuracy metric\n",
    "#acc, final_accs = ACC(result_matrix)\n",
    "#print(\"ACC: \", acc)\n",
    "#print(\"Accuracies for each task: \",  final_accs)\n",
    "#print()\n",
    "\n",
    "### print Backward Transfer metric\n",
    "#bwt, bwt_task = BWT(result_matrix)\n",
    "#print(\"BWT: \", bwt)\n",
    "#print(\"BWT for each task: \", bwt_task)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Continual Learning Applications\n",
    "\n",
    "Continual Learning with deep architectures may help us develop sustainable AI systems that can efficiently improve their skills and knowledge over time, adapting to ever-changing environments and learning objectives. In this section we will discuss about intriguing real-world applications that would highly benefit from recent advances in Continual Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "e249df16-76fe-49f4-bbee-e88749ec2a7b"
   },
   "outputs": [],
   "source": [
    "#@title Video 5: Continual Learning Applications\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"vNcJ4Ygaxio\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CORe50** is an interesting rea-world video dataset composed of 50 domestic objects belonging to 10 different categories and specifically designed for Continual Learning. You can find more information about the dataset and benchmark in its [official website](https://vlomonaco.github.io/core50). \n",
    "\n",
    "Here we will use the [Avalanche library](https://avalanche.continualai.org) to automatically download and use this dataset. Avalanche allows you to explore more challenging datasets and tasks to bring your continual learning algorithms into the real-world!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "88057825-e7cb-454f-a2f5-6d8d60dedd67"
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/ContinualAI/avalanche.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751,
     "referenced_widgets": [
      "46e3b0bb41ed4395aebef5ff89262c73",
      "f2081199f4204543b69b658676c1035b",
      "c3d499b8969e462099de054dcc85915a",
      "287d1e8bbd4b419e8f182bfe4cdb321b",
      "7e79ce2dcff344ffb865ac5e26413979",
      "47dbcdef75ef47e4b1f639230c587ad0",
      "064aa45f7e7a4c02bffd3a8f3c9682e0",
      "f2faaba506f74393b9eaad9799e91563",
      "6642723d31024e50bdb95b290e5503a9",
      "13809bd673d74b398bbd20aa194aa6c4",
      "e4815a2025f84780bf14dd665d29698d",
      "5a4d2617333a43e995f16e10f42fb70d",
      "5ff590ac1e7f41849a0847b05604596e",
      "8dceaab7945641f29d0706c5c1ac69a3",
      "5185fd1bf00b4cb8a26682c42458d232",
      "ea2d05730e5746fc9d30a0c61357e5be",
      "6573c54f74af4225b20117628701bb57",
      "692ff1b4d57a42239ba7129bf3219c27",
      "e813053fe8db48d59e7311216a4f219f",
      "ada3c51760b14d68aa81613f8f115d21",
      "e8ebf7d9e44a4120a5167432f747f83d",
      "0f5f611f587a4f7783e14ab6b8fc7a85",
      "a107bdc25ebe4234af6d49a1c12be163",
      "4f6e622a3dc445afb6d8160364608e3f",
      "52f1db6ce8764facbafe7bbe53e826a2",
      "ce21a21999f2456ab65719aed69391ff",
      "e1a39fd3e2254588ab6616bd7886bb90",
      "0a0e8cbc2ef44dea92a16062cf412c2d",
      "c5fce28dfc044991b7f156b0b6222981",
      "28414b77e5444653a0c4511d500e270e",
      "c41f3764b92d45fca417490665eb5ef0",
      "dc48ca11011a471d9367372c4eee4002",
      "ab416799aa534673b32fa1a954ed67b3",
      "0f687f938f434c0a97e33520e1213d5e",
      "a94a136229d140679ee01d588ce4fa5c",
      "c5783cfa03a54c578f63f7255462c32c",
      "d4e78223ff894a6fb2fdd95b93d0cd7f",
      "a58d1170267f44f99633b977f23bb984",
      "426f885c622640ea8d608b50f0eac2e0",
      "3cf513dc371f48f2b66087f1418d80aa",
      "c5eec3703f2a492abd88f7a5c9bf27c0",
      "6a9712a8e16c4abbb0723a68028f942d",
      "30601b5e8a1e4ec29ec34510c658c5fd",
      "f5e397395bb949769886e4d44eb4c067",
      "417b8fa7f19a4ce389f2ad68172ac68d",
      "27dd5360b7364004bdcaca5206ddfca5",
      "1fbd127177964bab8695b21a9e87c335",
      "858b8ae8e49344d68d8e2198db93d121",
      "1fa7f95eaa11460fb2162a31b38a428c",
      "0124bb72cc784a07a1d8ed900c7073ed",
      "710a4d2f1ead45d5b817f43c8f7d636f",
      "2130b9911b544f9983fdf6e02210b8fb",
      "5e2f7864bb45433daa0c1e95699d36d6",
      "ae4e71966fdc4e4ca4ffd56eaa3df57e",
      "691df8e1204a41d8adc298644fb23125",
      "498fe658bcee476592c4f6de9a6a58e9"
     ]
    },
    "outputId": "8ef3cfdc-80ab-445c-853e-9d03dbf4a755"
   },
   "outputs": [],
   "source": [
    "from avalanche.benchmarks.classic import CORe50\n",
    "# the scenario \"New Instances\" (NI) corresponds to the previously introduced\n",
    "# Domain-Incremental setting and its based on the idea of encounterning images\n",
    "# of the same classes for every incremental batch of data (or experience if you\n",
    "# will). The \"mini\" option downloads data 32x32 instead of the original 128x128.\n",
    "benchmark = CORe50(scenario=\"ni\", mini=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "161e9ca4-607e-4606-fa93-9cb394465909"
   },
   "outputs": [],
   "source": [
    "for exp in benchmark.train_stream:\n",
    "  print(exp.classes_in_this_experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Explore the challenging CORe50 scenarios!\n",
    "\n",
    "CORe50 offers and number of interesting preset scenarios already implemented and available to you through Avalanche. \n",
    "\n",
    "In this exercise try to explore the different scenarios offered (like the challenging NICv2-391) and possibly even apply what you've previously learned (like a replay approach) to get the best accuracy you can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bonus\n",
    "\n",
    "Add extra text that you want students to have for reference later but that will make reading times too long during tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should probably hint at Avalanche at least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Free up resources when done :)\n",
    "\n",
    "#import os, signal\n",
    "#os.kill(os.getpid(), signal.SIGKILL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W3D4_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
