{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "human_rl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/ReinforcementLearning/human_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using RL to Model Cognitive Tasks\n",
        "\n",
        "## Background\n",
        "\n",
        "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests).\n",
        "\n",
        "- Despite an extensive body of research that explains human performance using descriptive what-models, we still need a more sophisticated approach to gain a better understanding of the underlying processes (i.e., a how-model).\n",
        "\n",
        "- Interestingly, many of such tests can be thought of as a continuous stream of stimuli and corresponding actions, that is in consonant with the RL formulation. In fact, RL itself is in part motivated by how the brain enables goal-directed behaviors using reward systems, making it a good choice to explain human performance.\n",
        "\n",
        "- This project aims to use behavioral data to train an agent and then use the agent to investigate data produced by human subjects. Having a computational agent that mimics humans in such tests, we will be able to compare its mechanics with human data.\n",
        "\n",
        "- In another conception, we could fit an agent that learns many cognitive tasks that require abstract-level constructs such as executive functions. This is a multi-task control problem.\n",
        "\n",
        "- One example would be the N-back task.\n",
        "\n",
        "  - In the N-back, participants view a sequence of stimuli, one by one, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedback is given at both timestep and trajectory levels.\n",
        "\n",
        "  - The agent is rewarded when its response matches the stimulus that was shown N steps back in the episode. A simpler version of the N-back uses two-choice action schema, that is match vs non-match. Once the present stimulus matches the one presented N step back, then the agent is expected to respond to it as being a `match`.\n",
        "\n",
        "\n",
        "- Given a trained RL agent, we then find correlates of its fitted parameters with the brain mechanisms. The most straightforward composition could be the correlation of model parameters with the brain activities.\n",
        "\n",
        "\n",
        "## Datasets\n",
        "\n",
        "- HCP WM task ([NMA-CN HCP notebooks](https://github.com/NeuromatchAcademy/course-content/tree/master/projects/fMRI))\n",
        "\n",
        "\n",
        "Any dataset that used cognitive tests would work.\n",
        "Question: limit to behavioral data vs fMRI?\n",
        "Question: Which stimuli and actions to use?\n",
        "classic tests can be modeled using 1) bounded symbolic stimuli/actions (e.g., A, B, C), but more sophisticated one would require texts or images (e.g., face vs neutral images in social stroop dataset)\n",
        "The HCP dataset from NMA-CN contains behavioral and imaging data for 7 cognitive tests including various versions of N-back.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### N-back\n",
        "\n",
        "In the N-back task, participants view a sequence of stimuli, one per time, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedbacks are given at both timestep and trajectory levels.\n",
        "\n",
        "In a typical neuro setup, both accuracy and response time are measured, but here, for the sake of brevity, we focus only on accuracy of responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "kRcujCSkYcwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cognitive Tests Environment\n",
        "\n",
        "First we develop an environment in that agents perform a cognitive test, here the N-back."
      ],
      "metadata": {
        "id": "534dirBzKFAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First install and import all the required packages:"
      ],
      "metadata": {
        "id": "jmyVjQTIKFAT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Install dependencies\n",
        "\n",
        "%%capture\n",
        "\n",
        "# Install required packages\n",
        "\n",
        "!pip install dm-env\n",
        "!pip install dm-sonnet\n",
        "!pip install dm-acme==0.2.0 dm-acme[tf]==0.2.0 dm-acme[reverb]==0.2.0\n",
        "\n",
        "# Imports\n",
        "\n",
        "from acme.tf import networks\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "import dm_env\n",
        "from dm_env import specs\n",
        "import acme\n",
        "import sonnet as snt\n",
        "from acme import EnvironmentLoop\n",
        "from acme.testing import fakes\n",
        "import acme.tf.networks as networks\n",
        "from acme.tf import utils as tf2_utils\n",
        "from acme.agents.tf.d4pg import D4PG\n",
        "from acme.agents.tf.ddpg import DDPG\n",
        "from acme.agents.tf.dqn import DQN\n",
        "from acme.agents.tf.dmpo import DistributionalMPO\n",
        "from acme.agents.tf.r2d2 import R2D2\n",
        "from acme.utils.loggers import TerminalLogger\n",
        "from acme.tf import networks\n",
        "from acme import specs\n",
        "from acme import wrappers"
      ],
      "outputs": [],
      "metadata": {
        "id": "TRVmFkrvKFAU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title\n",
        "\n",
        "class InMemoryLogger(acme.utils.loggers.Logger):\n",
        "  \"\"\"A simple logger that keeps all data in memory.\n",
        "  \n",
        "  Reference:\n",
        "    https://github.com/deepmind/acme/blob/master/acme/utils/loggers/dataframe.py\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self._data = []\n",
        "\n",
        "  def write(self, data: acme.utils.loggers.LoggingData):\n",
        "    self._data.append(data)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human dataset\n",
        "\n",
        "We need a dataset of human perfoming a N-back test, with the following features:\n",
        "\n",
        "- `participant_id`: following the BIDS format, it contains a unique identifier for each participant.\n",
        "- `trial_index`: same as `time_step`.\n",
        "- `stimulus`: same as `observation`.\n",
        "- `response`: same as `action`, recorded response by the human subject.\n",
        "- `expected_response`: correct response.\n",
        "- `is_correct`: same as `reward`, whether the human subject responded correctly.\n",
        "- `response_time`: won't be used here.\n",
        "\n",
        "Here we generate a mock dataset with those features, but remember to **replace this with real human data.**"
      ],
      "metadata": {
        "id": "8yJ_vFcbKFAU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def generate_mock_nback_dataset(N=2,\n",
        "                                n_participants=10,\n",
        "                                n_trials=32,\n",
        "                                stimulus_choices=list('ABCDEF'),\n",
        "                                response_choices=['match', 'non-match']):\n",
        "  \"\"\"Generate a mock dataset for the N-back task.\"\"\"\n",
        "\n",
        "  n_rows = n_participants * n_trials\n",
        "\n",
        "  participant_ids = sorted([f'sub-{pid}' for pid in range(1,n_participants+1)] * n_trials)\n",
        "  trial_indices = list(range(1,n_trials+1)) * n_participants\n",
        "  stimulus_sequence = np.random.choice(stimulus_choices, n_rows)\n",
        "\n",
        "  responses = np.random.choice(response_choices, n_rows)\n",
        "  is_corrects = np.random.choice([True, False], n_rows)\n",
        "  response_times = np.random.exponential(size=n_rows)\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      'participant_id': participant_ids,\n",
        "      'trial_index': trial_indices,\n",
        "      'stimulus': stimulus_sequence,\n",
        "      'response': responses,\n",
        "      'is_correct': is_corrects,\n",
        "      #TODO: is_match\n",
        "      'response_time': response_times\n",
        "  })\n",
        "\n",
        "  # mark matchig stimuli\n",
        "  _nback_stim = df['stimulus'].shift(N)\n",
        "  df['expected_response'] = (df['stimulus'] == _nback_stim).map({True: 'match', False: 'non-match'})\n",
        "\n",
        "  # we don't care about burn-in trials (trial < N)\n",
        "  df.loc[df['trial_index'] <= N, 'is_correct'] = True\n",
        "  df.loc[df['trial_index'] <= N, ['response','response_time','expected_response']] = None\n",
        "\n",
        "  return df\n",
        "\n",
        "# ========\n",
        "# now generate the actual data with the provided function and plot some of its features\n",
        "mock_nback_data = generate_mock_nback_dataset()\n",
        "\n",
        "sns.displot(data=mock_nback_data, x='response_time')\n",
        "plt.suptitle('response time distribution of the mock N-back dataset', y=1.01)\n",
        "plt.show()\n",
        "\n",
        "sns.displot(data=mock_nback_data, x='is_correct')\n",
        "plt.suptitle('Accuracy distribution of the mock N-back dataset', y=1.06)\n",
        "plt.show()\n",
        "\n",
        "mock_nback_data"
      ],
      "outputs": [],
      "metadata": {
        "id": "_r-fAUndkGH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell implments N-back envinronment, that we later use to train a RL agent on human data. It is capable of performing two kinds of simulation:\n",
        "- rewards the agent once the action was correct (i.e., a normative model of the environment).\n",
        "- receives human data (or mock data if you prefer), and returns what participants performed as the observation. This is more useful for preference-based RL.\n"
      ],
      "metadata": {
        "id": "e2UBDVArobhL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class NBack(dm_env.Environment):\n",
        "\n",
        "  ACTIONS = ['match', 'non-match']\n",
        "\n",
        "  def __init__(self,\n",
        "               N=2,\n",
        "               episode_steps=32,\n",
        "               stimuli_choices=list('ABCDEF'),\n",
        "               human_data=None,\n",
        "               seed=1,\n",
        "               ):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      N\n",
        "      episode_steps\n",
        "      stimuli_choices\n",
        "      human_data\n",
        "      seed\n",
        "\n",
        "    \"\"\"\n",
        "    self.N = N\n",
        "    self.episode_steps = episode_steps\n",
        "    self.stimuli_choices = stimuli_choices\n",
        "    self.stimuli = np.empty(shape=episode_steps)  # will be filled in the `reset()`\n",
        "\n",
        "    self._reset_next_step = True\n",
        "\n",
        "    # whether mimic humans or reward the agent once it responds optimally.\n",
        "    if human_data is None:\n",
        "      self._imitate_human = False\n",
        "      self.human_data = None\n",
        "      self.human_subject_data = None\n",
        "    else:\n",
        "      self._imitate_human = True\n",
        "      self.human_data = human_data\n",
        "      self.human_subject_data = None\n",
        "\n",
        "    self._action_history = []\n",
        "\n",
        "  def reset(self):\n",
        "    self._reset_next_step = False\n",
        "    self._current_step = 0\n",
        "    self._action_history.clear()\n",
        "\n",
        "    # generate a random sequence instead of relying on human data\n",
        "    if self.human_data is None:\n",
        "      # self.stimuli = np.random.choice(self.stimuli_choices, self.episode_steps)\n",
        "      # FIXME This is a fix for acme & reverb issue with string observation. Agent should be able to handle strings\n",
        "      self.stimuli = np.random.choice(len(self.stimuli_choices), self.episode_steps).astype(np.float32)\n",
        "    else:\n",
        "      # randomly choose a subject from the human data and follow her trials and responses.\n",
        "      self.human_subject_data = self.human_data.query('participant_id == participant_id.sample().iloc[0]',\n",
        "                                                engine='python').sort_values('trial_index')\n",
        "      self.stimuli = self.human_subject_data['stimulus'].values\n",
        "      # FIXME should we always use one specific human subject or randomly select one in each episode?\n",
        "\n",
        "    return dm_env.restart(self._observation())\n",
        "\n",
        "\n",
        "  def _episode_return(self):\n",
        "    if self._imitate_human:\n",
        "      return np.mean(self.human_subject_data['response'] == self._action_history)\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  def step(self, action: int):\n",
        "    if self._reset_next_step:\n",
        "      return self.reset()\n",
        "\n",
        "    if self._imitate_human:\n",
        "      # if it was the same action as the human subject, then reward the agent\n",
        "      human_action = self.human_subject_data['response'].iloc[self._current_step]\n",
        "      agent_action = NBack.ACTIONS[action]\n",
        "      step_reward = (agent_action == human_action)\n",
        "    else:\n",
        "      # assume the agent is rationale and doesn't want to reproduce human, reward once the response it correct\n",
        "      expected_action = 'match' if (self.stimuli[self._current_step] == self.stimuli[self._current_step - self.N]) else 'non-match'\n",
        "      agent_action = NBack.ACTIONS[action]\n",
        "      step_reward = 1. if (agent_action == expected_action) else -1.\n",
        "\n",
        "    self._action_history.append(agent_action)\n",
        "\n",
        "    self._current_step += 1\n",
        "    \n",
        "    # Check for termination.\n",
        "    if self._current_step == self.stimuli.shape[0]:\n",
        "      self._reset_next_step = True\n",
        "      # we are using the mean of total time step rewards as the episode return\n",
        "      return dm_env.termination(reward=self._episode_return(),\n",
        "                                observation=self._observation())\n",
        "    else:\n",
        "      return dm_env.transition(reward=step_reward,\n",
        "                               observation=self._observation())\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return dm_env.specs.BoundedArray(\n",
        "        shape=self.stimuli.shape,\n",
        "        dtype=self.stimuli.dtype,\n",
        "        name='nback_stimuli', minimum=0, maximum=1)\n",
        "\n",
        "  def action_spec(self):\n",
        "    return dm_env.specs.DiscreteArray(\n",
        "        dtype=int,\n",
        "        num_values=len(NBack.ACTIONS),\n",
        "        name='action')\n",
        "\n",
        "  def _observation(self):\n",
        "\n",
        "    # agents observe only the current stimulus\n",
        "    obs = self.stimuli[self._current_step - 1]\n",
        "\n",
        "    # TODO uncomment to observe all the previrous stimuli instead of only the current stimulus\n",
        "    # obs = self.stimuli[:self.current_step]\n",
        "    # obs = ''.join(obs)\n",
        "\n",
        "    return obs \n",
        "\n",
        "  def plot_state(self):\n",
        "    \"\"\"Display current state of the environment.\n",
        "     \n",
        "     Note: `M` mean `match`, and `.` is a `non-match`.\n",
        "    \"\"\"\n",
        "    from IPython.display import HTML\n",
        "    stimuli = self.stimuli[:self._current_step - 1]\n",
        "    actions = ['M' if a=='match' else '.' for a in self._action_history[:self._current_step - 1]]\n",
        "    return HTML(\n",
        "        f'<b>Environment ({self.N}-back):</b><br />'\n",
        "        f'<pre><b>Stimuli:</b> {\"\".join(map(str,map(int,stimuli)))}</pre>'\n",
        "        f'<pre><b>Actions:</b> {\"\".join(actions)}</pre>'\n",
        "    )\n",
        "\n",
        "  @staticmethod\n",
        "  def create_environment():\n",
        "    \"\"\"Utility function to create a N-back environment and its spec.\"\"\"\n",
        "\n",
        "    # Make sure the environment outputs single-precision floats.\n",
        "    environment = wrappers.SinglePrecisionWrapper(NBack())\n",
        "\n",
        "    # Grab the spec of the environment.\n",
        "    environment_spec = specs.make_environment_spec(environment)\n",
        "\n",
        "    return environment, environment_spec\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZrZlYu0Dc10D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a random agent (see W3D2):"
      ],
      "metadata": {
        "id": "XH10PYIMoTST"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class RandomAgent(acme.Actor):\n",
        "\n",
        "  def __init__(self, environment_spec):\n",
        "    \"\"\"Gets the number of available actions from the environment spec.\"\"\"\n",
        "    self._num_actions = environment_spec.actions.num_values\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    \"\"\"Selects an action uniformly at random.\"\"\"\n",
        "    action = np.random.randint(self._num_actions)\n",
        "    return action\n",
        "\n",
        "  def observe_first(self, timestep):\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def update(self):\n",
        "    \"\"\"Does not update as the RandomAgent does not learn from data.\"\"\"\n",
        "    pass"
      ],
      "outputs": [],
      "metadata": {
        "id": "wY55a7NwnEDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init the environment and the agent:"
      ],
      "metadata": {
        "id": "iF-Zj7OpoQX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "environment, environment_spec = NBack.create_environment()\n",
        "agent = RandomAgent(environment_spec)\n",
        "\n",
        "print('actions:\\n', environment_spec.actions)\n",
        "print('observations:\\n', environment_spec.observations)\n",
        "print('rewards:\\n', environment_spec.rewards)\n",
        "\n",
        "# DEBUG\n",
        "# timestep = environment.step(NBack.ACTIONS[0])  # pytype: dm_env.TimeStep\n",
        "# timestep"
      ],
      "outputs": [],
      "metadata": {
        "id": "zxAnYNZmnIom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the loop (see W3D2):"
      ],
      "metadata": {
        "id": "XedKwwRUoO2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# fitting parameters\n",
        "n_episodes = 1_000\n",
        "n_total_steps = 0\n",
        "log_loss = False\n",
        "n_steps = n_episodes * 32\n",
        "all_returns = []\n",
        "\n",
        "\n",
        "# main loop\n",
        "for episode in range(n_episodes):\n",
        "  episode_steps = 0\n",
        "  episode_return = 0\n",
        "  episode_loss = 0\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  timestep = environment.reset()\n",
        "\n",
        "  # Make the first observation.\n",
        "  agent.observe_first(timestep)\n",
        "\n",
        "  # Run an episode\n",
        "  while not timestep.last():\n",
        "    \n",
        "    # DEBUG\n",
        "    # print(timestep)\n",
        "\n",
        "    # Generate an action from the agent's policy and step the environment.\n",
        "    action = agent.select_action(timestep.observation)\n",
        "    timestep = environment.step(action)\n",
        "\n",
        "    # Have the agent observe the timestep and let the agent update itself.\n",
        "    agent.observe(action, next_timestep=timestep)\n",
        "    agent.update()\n",
        "\n",
        "    # Book-keeping.\n",
        "    episode_steps += 1\n",
        "    n_total_steps += 1\n",
        "    episode_return += timestep.reward\n",
        "\n",
        "    if log_loss:\n",
        "      episode_loss += agent.last_loss\n",
        "\n",
        "    if n_steps is not None and n_total_steps >= n_steps:\n",
        "      break\n",
        "\n",
        "  # Collect the results and combine with counts.\n",
        "  steps_per_second = episode_steps / (time.time() - start_time)\n",
        "  result = {\n",
        "      'episode': episode,\n",
        "      'episode_length': episode_steps,\n",
        "      'episode_return': episode_return,\n",
        "  }\n",
        "  if log_loss:\n",
        "    result['loss_avg'] = episode_loss/episode_steps\n",
        "\n",
        "  all_returns.append(episode_return)\n",
        "\n",
        "  from IPython.display import display\n",
        "  display(environment.plot_state())\n",
        "  # Log the given results.\n",
        "  print(result)\n",
        "\n",
        "  if n_steps is not None and n_total_steps >= n_steps:\n",
        "    break\n",
        "\n",
        "print('\\n', 'All episode returns:', all_returns)"
      ],
      "outputs": [],
      "metadata": {
        "id": "tSXSsY4Zmkzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can simplify the environment loop using [DeepMind Acme](https://github.com/deepmind/acme)."
      ],
      "metadata": {
        "id": "UmirxCZ_KFAZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def make_networks_d4pg(action_spec,\n",
        "                       policy_layer_sizes=(256, 256, 256),\n",
        "                       critic_layer_sizes=(512, 512, 256),\n",
        "                       vmin=-150.,\n",
        "                       vmax=150.,\n",
        "                       num_atoms=51,\n",
        "                      ):\n",
        "  \"\"\"Networks for D4PG agent.\"\"\"\n",
        "  action_size = np.prod(action_spec.shape, dtype=int)\n",
        "\n",
        "  policy_network = snt.Sequential([\n",
        "      tf2_utils.batch_concat,\n",
        "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes + (action_size,)),\n",
        "      networks.TanhToSpec(spec=action_spec)\n",
        "      ])\n",
        "  critic_network = snt.Sequential([\n",
        "      networks.CriticMultiplexer(\n",
        "          action_network=networks.ClipToSpec(action_spec),\n",
        "          critic_network=networks.LayerNormMLP(\n",
        "              layer_sizes=critic_layer_sizes,\n",
        "              activate_final=True),\n",
        "      ),\n",
        "      networks.DiscreteValuedHead(vmin=vmin, \n",
        "                                  vmax=vmax, \n",
        "                                  num_atoms=num_atoms)\n",
        "      ])\n",
        "\n",
        "  return policy_network, critic_network\n",
        "\n",
        "\n",
        "def make_networks_dqn(action_spec):\n",
        "  network = snt.Sequential([\n",
        "      snt.Flatten(),\n",
        "      snt.nets.MLP([50, 50, action_spec.num_values]),\n",
        "  ])\n",
        "  return network\n",
        "\n",
        "\n",
        "policy_optimizer = snt.optimizers.Adam(1e-4)\n",
        "critic_optimizer = snt.optimizers.Adam(1e-4)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Zgu3MDN3KFAa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# init the N-back environment\n",
        "env, env_spec = NBack.create_environment()\n",
        "\n",
        "# DEBUG fake testing environment.\n",
        "# Uncomment this to debug your agent without using the N-back environment.\n",
        "# env = fakes.DiscreteEnvironment(\n",
        "#     num_actions=2,\n",
        "#     num_observations=1000,\n",
        "#     obs_dtype=np.float32,\n",
        "#     episode_length=32)\n",
        "# env_spec = specs.make_environment_spec(env)"
      ],
      "outputs": [],
      "metadata": {
        "id": "e9YAyYdyKFAa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# DQN agent\n",
        "# agent = DQN(\n",
        "#     environment_spec=env_spec,\n",
        "#     network=make_networks_dqn(env_spec.actions))\n",
        "\n",
        "# D4PG agent\n",
        "# policy_network, critic_network = make_networks_d4pg(env_spec.actions)\n",
        "# agent = D4PG(environment_spec=env_spec,\n",
        "#              policy_network=policy_network,\n",
        "#              critic_network=critic_network,\n",
        "#              observation_network=tf2_utils.batch_concat, # Identity Op.\n",
        "#              policy_optimizer=policy_optimizer,\n",
        "#              critic_optimizer=critic_optimizer,\n",
        "#              logger=InMemoryLogger())\n",
        "\n",
        "# random agent\n",
        "agent = RandomAgent(env_spec)"
      ],
      "outputs": [],
      "metadata": {
        "id": "OzVvipa5KFAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we run the environment loop with the initiated agent and print the training log."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# training loop\n",
        "loop = EnvironmentLoop(env, agent, logger=InMemoryLogger())\n",
        "loop.run(n_episodes)\n",
        "\n",
        "# print logs\n",
        "logs = pd.DataFrame(loop._logger._data)\n",
        "logs.tail()"
      ],
      "outputs": [],
      "metadata": {
        "id": "AvKTEi4fKFAb"
      }
    }
  ]
}